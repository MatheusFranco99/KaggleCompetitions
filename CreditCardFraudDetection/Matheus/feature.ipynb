{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "df = None\n",
    "with open('data/balancing.pk','rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature extraction\n",
    "    - features interactions (e.g. feature area, price -> price per unit area). Common text-relted methods as: bag of words (BoW), TF-IDF, BM25, stemming, tokenization, etc.  \n",
    "- Feature transformation\n",
    "    - common with signals with methods as: Wavelet transform, Fourier transform, binning / discretization (dividing countinuous numerical features into bins (intervals)), enconding (categorical features into numerical (e.g. one-hot encoding, label encoding, target encoding)),\n",
    "- Feature Selection\n",
    "    - dimensionality reduction (PCA, LDA, t-SNE), redundant, irrelevant, noise feature removal, randomized methods, correlation-based (pearson correlation coefficient, spearman rank correlation), filter methods (chi-square test, ANOVA, information gain, correlation coefficient)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Since we don't have knowledge about the features, I found it difficult to combine features. Thus, I'll jump this step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation\n",
    "\n",
    "The same applies for feature transformation. I didn't find any method that I was able to apply here and I'll jump."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Though the dataset already passed through a feature selection method (PCA), we're still going to apply common methods in order to check the result and as an opportunity to learn new methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is an useful method when dealing with high dimensional data. The main objective is to reduce the number of dimensions retaining as much information as possible. The origins of PCA goes to Pearson who tried to find the closest line, plane or hyperplane regarding a set of points, in a least square metric.\n",
    "\n",
    "Note that the hype-plane that closest fits the set of points also produces the maximum variance of the points in the plane. This idea is important. The more the variability, the more the information contained in the data.\n",
    "\n",
    "On the mathematics: consider a matrix NxC where N is the number of observations and C is the number of columns. Each row of the matrix is apoint in the K-space. These points have an average point. We can, with a translation, shift the set of points in a way that the new mean is 0. PCA then proceeds by scaling the data to unit variance. Then, with Linear Algebra, we can compute the line that best fits the matrix (the set of points). The projection of the points in this line is the PC1, the first principal component. The ortoghonal line (regarding the PC1 line) that best fits the points is calculated and the projection in this line is PC2. Note that the lines from PC1 and PC2 compose a plane. With PC3, PC4, etc, we'll have the best fit hyperplane.\n",
    "\n",
    "https://www.sartorius.com/en/knowledge/science-snippets/what-is-principal-component-analysis-pca-and-how-it-is-used-507186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.147705</td>\n",
       "      <td>0.169151</td>\n",
       "      <td>0.335411</td>\n",
       "      <td>0.603089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.043453</td>\n",
       "      <td>0.584180</td>\n",
       "      <td>0.265906</td>\n",
       "      <td>0.670798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.258784</td>\n",
       "      <td>-0.209164</td>\n",
       "      <td>-0.050699</td>\n",
       "      <td>0.109833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.474737</td>\n",
       "      <td>0.177471</td>\n",
       "      <td>-0.196969</td>\n",
       "      <td>1.155678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.003145</td>\n",
       "      <td>-0.046066</td>\n",
       "      <td>0.844102</td>\n",
       "      <td>0.247561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3       PC4  Class\n",
       "0 -2.147705  0.169151  0.335411  0.603089      0\n",
       "1 -2.043453  0.584180  0.265906  0.670798      0\n",
       "2 -2.258784 -0.209164 -0.050699  0.109833      0\n",
       "3 -2.474737  0.177471 -0.196969  1.155678      0\n",
       "4 -2.003145 -0.046066  0.844102  0.247561      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca(df, class_col_name, n_components):\n",
    "    # Separate the target class column from the feature columns\n",
    "    X = df.drop(columns=[class_col_name])\n",
    "    y = df[class_col_name]\n",
    "    \n",
    "    # Standardize the feature columns by subtracting the mean and scaling to unit variance\n",
    "    X_std = (X - X.mean()) / X.std()\n",
    "    \n",
    "    # Create a PCA object and fit it to the standardized feature columns\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_std)\n",
    "    \n",
    "    # Transform the standardized feature columns to the new PCA space\n",
    "    X_pca = pca.transform(X_std)\n",
    "    \n",
    "    # Create a new DataFrame with the transformed feature columns and the target class column\n",
    "    pca_columns = [f\"PC{i}\" for i in range(1, n_components+1)]\n",
    "    df_pca = pd.DataFrame(X_pca, columns=pca_columns, index=X.index)\n",
    "    df_pca[class_col_name] = y\n",
    "    \n",
    "    return df_pca\n",
    "\n",
    "df_pca = apply_pca(df, 'Class', 4)\n",
    "df_pca.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA is another dimensionality reduction method. LDA chooses new axes such that the separability between classes is optimized (thus it's a supervised method). It's a good method to increase model performance.\n",
    "\n",
    "It's built in three steps:\n",
    "- Calculate separability between classes (between-class variance) defined as the distance between the mean of different classes.\n",
    "$$\n",
    "S_b = \\sum^{g}_{i = 1}N_i(\\overline{x_i}-\\overline{x})(\\overline{x}_i-\\overline{x})^T\n",
    "$$\n",
    "Where $g$ is the number of classes, $N_i$ is the sample size of the class $i$, $\\overline{x}_i$ is the mean of the class $i$ and $\\overline{x}$ is the overall mean.\n",
    "- Calculate the within-class variance.\n",
    "$$\n",
    "S_w = \\sum_{i = 1}^{g}\\sum_{j=1}^{N_i}(x_{i,j}-\\overline{x_i})(x_{i,j}-\\overline{x_i})^T\n",
    "$$\n",
    "- Construct a lower-dimensional space that maximizes the between-class variance (\"separability\") and minimizes the within-class variance.\n",
    "\n",
    "https://medium.com/analytics-vidhya/linear-discriminant-analysis-explained-in-under-4-minutes-e558e962c877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA1</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.728391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.518905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.755254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.318189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.839148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LDA1  Class\n",
       "0 -0.728391      0\n",
       "1 -1.518905      0\n",
       "2 -1.755254      0\n",
       "3 -1.318189      0\n",
       "4 -0.839148      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def apply_lda(df, class_col_name, n_components):\n",
    "    # Separate the target class column from the feature columns\n",
    "    X = df.drop(columns=[class_col_name])\n",
    "    y = df[class_col_name]\n",
    "    \n",
    "    # Create an LDA object and fit it to the feature columns and target class column\n",
    "    lda = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "    lda.fit(X, y)\n",
    "    \n",
    "    # Transform the feature columns to the new LDA space\n",
    "    X_lda = lda.transform(X)\n",
    "    \n",
    "    # Create a new DataFrame with the transformed feature columns and the target class column\n",
    "    lda_columns = [f\"LDA{i}\" for i in range(1, n_components+1)]\n",
    "    df_lda = pd.DataFrame(X_lda, columns=lda_columns, index=X.index)\n",
    "    df_lda[class_col_name] = y\n",
    "    \n",
    "    return df_lda\n",
    "\n",
    "df_lda = apply_lda(df, 'Class', 1)\n",
    "df_lda.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Unlike PCA, t-SNE can separate data that can't be separated by a line or plane. I.e., it's a nonlinear method, useful when PCA can't handle the nonlinear class separation. It's most used to understand high dimensional data and project into 2D or 3D.\n",
    "\n",
    "The method mesures the dimilarity between pairs of points and then constructs a probability of each point being chosen as a neighbor of another point, based on their similarity. Then, it constructs a similar probability distribution in a lower-dimensional space and tries to map the points to the new space minimizing the divergence between the two distributions, using gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def apply_tsne(df, class_col_name, n_components):\n",
    "    # Separate the target class column from the feature columns\n",
    "    X = df.drop(columns=[class_col_name])\n",
    "    y = df[class_col_name]\n",
    "    \n",
    "    # Create a t-SNE object and fit_transform it to the feature columns and target class column\n",
    "    tsne = TSNE(n_components=n_components, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    \n",
    "    # Create a new DataFrame with the transformed feature columns and the target class column\n",
    "    tsne_columns = [f\"t-SNE{i}\" for i in range(1, n_components+1)]\n",
    "    df_tsne = pd.DataFrame(X_tsne, columns=tsne_columns, index=X.index)\n",
    "    df_tsne[class_col_name] = y\n",
    "    \n",
    "    return df_tsne\n",
    "\n",
    "\n",
    "df_tsne = apply_tsne(df, 'Class', 1)\n",
    "df_tsne.head()\n",
    "# I wasn't able to run this on my pc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundant\n",
    "\n",
    "Redundant features are those features that are highly correlated with other features and thus do not bring much \"new\" information to distinguish the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/4b_rm7gs2fj181km0x332skh0000gn/T/ipykernel_45392/578981726.py:11: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V18       V19       V20       V21       V22  \\\n",
       "0  1.468177 -0.470401  0.025791  0.403993  0.251412 -0.018307  0.277838   \n",
       "1  0.635558  0.463917 -0.183361 -0.145783 -0.069083 -0.225775 -0.638672   \n",
       "2  2.345865 -2.890083 -0.121359 -2.261857  0.524980  0.247998  0.771679   \n",
       "3 -0.631418 -1.059647  1.965775 -1.232622 -0.208038 -0.108300  0.005274   \n",
       "4  0.175121 -0.451449 -0.038195  0.803487  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Amount  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def drop_redundant(df, class_col_name):\n",
    "    # Separate the target class column from the feature columns\n",
    "    X = df.drop(columns=[class_col_name])\n",
    "    y = df[class_col_name]\n",
    "    \n",
    "    # Drop redundant features using correlation\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    X = X.drop(columns=to_drop)\n",
    "    \n",
    "    # Create a new DataFrame with the feature columns and the target class column\n",
    "    df_new = X.copy()\n",
    "    df_new[class_col_name] = y\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "df_red = drop_redundant(df, 'Class')\n",
    "df_red.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrelevant\n",
    "\n",
    "Irrelevant features, not to be confused with redundant, doesn't contribute to the model because it has low correlation with target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/4b_rm7gs2fj181km0x332skh0000gn/T/ipykernel_45392/3741278378.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[class_col] = y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V2        V3        V4       V10       V11       V12       V14  \\\n",
       "0 -0.072781  2.536347  1.378155  0.090794 -0.551600 -0.617801 -0.311169   \n",
       "1  0.266151  0.166480  0.448154 -0.166974  1.612727  1.065235 -0.143772   \n",
       "2 -1.340163  1.773209  0.379780  0.207643  0.624501  0.066084 -0.165946   \n",
       "3 -0.185226  1.792993 -0.863291 -0.054952 -0.226487  0.178228 -0.287924   \n",
       "4  0.877737  1.548718  0.403034  0.753074 -0.822843  0.538196 -1.119670   \n",
       "\n",
       "        V16       V17  Class  \n",
       "0 -0.470401  0.207971      0  \n",
       "1  0.463917 -0.114805      0  \n",
       "2 -2.890083  1.109969      0  \n",
       "3 -1.059647 -0.684093      0  \n",
       "4 -0.451449 -0.237033      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "def drop_irrelevant_features(data, class_col):\n",
    "    \"\"\"\n",
    "    Drops irrelevant features using a random forest classifier and returns a new dataset without these features.\n",
    "    Parameters:\n",
    "        - data: pandas DataFrame containing the dataset\n",
    "        - class_col: name of the column containing the class labels\n",
    "    Returns:\n",
    "        - new_data: pandas DataFrame containing the new dataset with only relevant features\n",
    "    \"\"\"\n",
    "    # Separate the features and labels\n",
    "    X = data.drop(columns=[class_col])\n",
    "    y = data[class_col]\n",
    "\n",
    "    # Use a random forest classifier to estimate feature importance\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # Use a threshold to select important features\n",
    "    selector = SelectFromModel(rf, prefit=True)\n",
    "    important_features = selector.get_support(indices=True)\n",
    "\n",
    "    # Drop irrelevant features from the dataset\n",
    "    new_data = data.iloc[:, important_features]\n",
    "    new_data[class_col] = y\n",
    "\n",
    "    return new_data\n",
    "\n",
    "\n",
    "df_irr = drop_irrelevant_features(df, 'Class')\n",
    "df_irr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy features\n",
    "\n",
    "Noisy features are those with random or inconsistent relationship with the target variable of features that may have errors.\n",
    "\n",
    "mRMR is method that tries to find the subset of features that maximizes relevance and minimizes redundancy between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymrmr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpymrmr\u001b[39;00m \u001b[39mimport\u001b[39;00m mRMR\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_noisy_features\u001b[39m(data, class_col):\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m    Removes noisy features using the mRMR algorithm and returns a new dataset without these features.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m    Parameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m        - new_data: pandas DataFrame containing the new dataset without noisy features\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymrmr'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymrmr import mRMR\n",
    "\n",
    "def remove_noisy_features(data, class_col):\n",
    "    \"\"\"\n",
    "    Removes noisy features using the mRMR algorithm and returns a new dataset without these features.\n",
    "    Parameters:\n",
    "        - data: pandas DataFrame containing the dataset\n",
    "        - class_col: name of the column containing the class labels\n",
    "    Returns:\n",
    "        - new_data: pandas DataFrame containing the new dataset without noisy features\n",
    "    \"\"\"\n",
    "    # Separate the features and labels\n",
    "    X = data.drop(columns=[class_col])\n",
    "    y = data[class_col]\n",
    "\n",
    "    # Use the mRMR algorithm to select features\n",
    "    selected_features = mRMR.mRMR(X.values, \"MIQ\", len(X.columns))\n",
    "    selected_features = [int(f) for f in selected_features]\n",
    "\n",
    "    # Remove noisy features from the dataset\n",
    "    new_data = data.iloc[:, selected_features]\n",
    "    new_data[class_col] = y\n",
    "\n",
    "    return new_data\n",
    "\n",
    "df_noisy = remove_noisy_features(df, 'Class')\n",
    "df_noisy.head()\n",
    "# I could not install the mrmr library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation methods: Pearson correlation\n",
    "\n",
    "This method is similar to the redundancy method but uses the Pearson correlation.\n",
    "\n",
    "The Pearson correlation is defined by:\n",
    "$$\n",
    "r = \\frac{\\sum(x_i-\\overline{x})(y_i-\\overline{y})}{\\sqrt{\\sum(x_i - \\overline{x})^2\\sum(y_i - \\overline{y})^2}}\n",
    "$$\n",
    "i.e. is the covariance divided by the product of their standard deviations. It ranges from -1 to 1. 0 means no correlation, +1 indicates positive correlation and -1 indicates negative. This model assumes that the relationship between the variables is linear and the data is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V7</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V2        V3        V4        V7        V9       V10       V11  \\\n",
       "0 -0.072781  2.536347  1.378155  0.239599  0.363787  0.090794 -0.551600   \n",
       "1  0.266151  0.166480  0.448154 -0.078803 -0.255425 -0.166974  1.612727   \n",
       "2 -1.340163  1.773209  0.379780  0.791461 -1.514654  0.207643  0.624501   \n",
       "3 -0.185226  1.792993 -0.863291  0.237609 -1.387024 -0.054952 -0.226487   \n",
       "4  0.877737  1.548718  0.403034  0.592941  0.817739  0.753074 -0.822843   \n",
       "\n",
       "        V12       V14       V16       V17  Class  \n",
       "0 -0.617801 -0.311169 -0.470401  0.207971      0  \n",
       "1  1.065235 -0.143772  0.463917 -0.114805      0  \n",
       "2  0.066084 -0.165946 -2.890083  1.109969      0  \n",
       "3  0.178228 -0.287924 -1.059647 -0.684093      0  \n",
       "4  0.538196 -1.119670 -0.451449 -0.237033      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_pearson_correlated_features(data, class_col, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Removes features that are highly correlated with the class label and returns a new dataset without these features.\n",
    "    Parameters:\n",
    "        - data: pandas DataFrame containing the dataset\n",
    "        - class_col: name of the column containing the class labels\n",
    "        - threshold: Pearson correlation threshold below which features will be removed (default=0.5)\n",
    "    Returns:\n",
    "        - new_data: pandas DataFrame containing the new dataset without Pearson correlated features\n",
    "    \"\"\"\n",
    "    # Calculate the Pearson correlation coefficients between features and class label\n",
    "    pearson_coeffs = data.drop(class_col, axis=1).apply(lambda x: x.corr(data[class_col]))\n",
    "\n",
    "    # Find the features with correlation coefficient below the threshold\n",
    "    pearson_corr_features = pearson_coeffs[abs(pearson_coeffs) < threshold].index.tolist()\n",
    "\n",
    "    # Remove Pearson correlated features from the dataset\n",
    "    new_data = data.drop(pearson_corr_features, axis=1)\n",
    "    new_data[class_col] = data[class_col]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "df_pearson = remove_pearson_correlated_features(df, 'Class')\n",
    "df_pearson.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation methods: Spearman rank correlation\n",
    "\n",
    "The correlation metric used in this method is the Spearman rank correlation. Instead of comparing the data, it ranks them (from lowest to highest) and calculates the correlation by finding the difference between the ranks and calculating the Pearson correlation on the ranks.\n",
    "$$\n",
    "p = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n",
    "$$\n",
    "where $n$ is the number of observations and $d_i$ is the difference between the two ranks of each observation.\n",
    "\n",
    "\n",
    "The values is similar to the Pearson's. Nevertheless, it doesn't assume that the relationship is linear and is more robust to outliers than Pearson's. It's more used when dealing with ordered, ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V7</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V7        V9       V10  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155  0.239599  0.363787  0.090794   \n",
       "1  1.191857  0.266151  0.166480  0.448154 -0.078803 -0.255425 -0.166974   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780  0.791461 -1.514654  0.207643   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291  0.237609 -1.387024 -0.054952   \n",
       "4 -1.158233  0.877737  1.548718  0.403034  0.592941  0.817739  0.753074   \n",
       "\n",
       "        V11       V12       V14       V16       V17  Class  \n",
       "0 -0.551600 -0.617801 -0.311169 -0.470401  0.207971      0  \n",
       "1  1.612727  1.065235 -0.143772  0.463917 -0.114805      0  \n",
       "2  0.624501  0.066084 -0.165946 -2.890083  1.109969      0  \n",
       "3 -0.226487  0.178228 -0.287924 -1.059647 -0.684093      0  \n",
       "4 -0.822843  0.538196 -1.119670 -0.451449 -0.237033      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_spearman_correlated_features(data, class_col, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Removes features that are highly correlated with the class label and returns a new dataset without these features.\n",
    "    Parameters:\n",
    "        - data: pandas DataFrame containing the dataset\n",
    "        - class_col: name of the column containing the class labels\n",
    "        - threshold: Spearman rank correlation threshold below which features will be removed (default=0.5)\n",
    "    Returns:\n",
    "        - new_data: pandas DataFrame containing the new dataset without Spearman rank correlated features\n",
    "    \"\"\"\n",
    "    # Calculate the Spearman rank correlation coefficients between features and class label\n",
    "    spearman_coeffs = data.drop(class_col, axis=1).apply(lambda x: x.corr(data[class_col], method='spearman'))\n",
    "\n",
    "    # Find the features with correlation coefficient below the threshold\n",
    "    spearman_corr_features = spearman_coeffs[abs(spearman_coeffs) < threshold].index.tolist()\n",
    "\n",
    "    # Remove Spearman rank correlated features from the dataset\n",
    "    new_data = data.drop(spearman_corr_features, axis=1)\n",
    "    new_data[class_col] = data[class_col]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "df_spearman = remove_spearman_correlated_features(df, 'Class')\n",
    "df_spearman.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter methods: ANOVA\n",
    "\n",
    "ANOVA is an statistical test to compare the mean of two or more groups. The ANOVA F-test can measure the difference between the mean of a feature and the mean of the target variable. It works by calculating the ratio of: between-grop variability by within-group variability.\n",
    "The between-group variability is:\n",
    "$$\n",
    "\\sum_{i=1}^{k}n_i(\\overline{Y}_i-\\overline{Y})^2/(K-1)\n",
    "$$\n",
    "where $\\overline{Y}_i$ is the mean of sample $i$, $n_i$ is the number of observations of the $i$-th group and $K$ is the number of groups.The within-group variability is:\n",
    "$$\n",
    "\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(\\overline{Y_{i,j}}-\\overline{Y_i})^2/(N-K)\n",
    "$$\n",
    "The value reached is then compared to the F-distribution to determine the significance of the difference in means.\n",
    "The filter method relies on ranking the features by its F-score. The higher, the more significant the difference between means and the more relevant to the target variable. Then, the filter method selects the top K features.\n",
    "\n",
    "https://en.wikipedia.org/wiki/F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V2        V3        V4        V9       V10       V11       V12  \\\n",
       "0 -0.072781  2.536347  1.378155  0.363787  0.090794 -0.551600 -0.617801   \n",
       "1  0.266151  0.166480  0.448154 -0.255425 -0.166974  1.612727  1.065235   \n",
       "2 -1.340163  1.773209  0.379780 -1.514654  0.207643  0.624501  0.066084   \n",
       "3 -0.185226  1.792993 -0.863291 -1.387024 -0.054952 -0.226487  0.178228   \n",
       "4  0.877737  1.548718  0.403034  0.817739  0.753074 -0.822843  0.538196   \n",
       "\n",
       "        V14       V16       V17  Class  \n",
       "0 -0.311169 -0.470401  0.207971      0  \n",
       "1 -0.143772  0.463917 -0.114805      0  \n",
       "2 -0.165946 -2.890083  1.109969      0  \n",
       "3 -0.287924 -1.059647 -0.684093      0  \n",
       "4 -1.119670 -0.451449 -0.237033      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def select_features_anova(df, class_col, k=10):\n",
    "    X = df.drop(class_col, axis=1)\n",
    "    y = df[class_col]\n",
    "    \n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    return df[selected_features + [class_col]]\n",
    "\n",
    "df_anova = select_features_anova(df, 'Class')\n",
    "df_anova.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter methods: Information gain\n",
    "\n",
    "It uses two important concepts.\n",
    "- Entropy: measure of randomness or uncertainty of a dataset. Entropy of a dataset with $C$ classes:\n",
    "$$E = -\\sum_{i=1}^{C}p_ilog_2p_i$$\n",
    "where $p_i$ is the probability of randomly picking an element of the class $i$.\n",
    "- Information gain: when we transform a dataset, it will have a new entropy. The difference is the information gain:\n",
    "$$Gain = E_{old}-E_{new}$$\n",
    "Note that the more entropy removed, the better.\n",
    "\n",
    "In the filter method, it can be applied in the following way: you measure the entropy of the target variable and then the information gain of each featue (based on the difference between the entropy of the target variable before and after the feature is added). Then, it's just a task of ranking the information gain of each feature and selecting the top K.\n",
    "\n",
    "https://victorzhou.com/blog/information-gain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V14</th>\n",
       "      <th>V10</th>\n",
       "      <th>V12</th>\n",
       "      <th>V17</th>\n",
       "      <th>V4</th>\n",
       "      <th>V11</th>\n",
       "      <th>Amount</th>\n",
       "      <th>V3</th>\n",
       "      <th>V16</th>\n",
       "      <th>V7</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.311169</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>149.62</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.143772</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.165946</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>378.66</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>123.50</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>69.99</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        V14       V10       V12       V17        V4       V11  Amount  \\\n",
       "0 -0.311169  0.090794 -0.617801  0.207971  1.378155 -0.551600  149.62   \n",
       "1 -0.143772 -0.166974  1.065235 -0.114805  0.448154  1.612727    2.69   \n",
       "2 -0.165946  0.207643  0.066084  1.109969  0.379780  0.624501  378.66   \n",
       "3 -0.287924 -0.054952  0.178228 -0.684093 -0.863291 -0.226487  123.50   \n",
       "4 -1.119670  0.753074  0.538196 -0.237033  0.403034 -0.822843   69.99   \n",
       "\n",
       "         V3       V16        V7  Class  \n",
       "0  2.536347 -0.470401  0.239599      0  \n",
       "1  0.166480  0.463917 -0.078803      0  \n",
       "2  1.773209 -2.890083  0.791461      0  \n",
       "3  1.792993 -1.059647  0.237609      0  \n",
       "4  1.548718 -0.451449  0.592941      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def select_features_info_gain(df, class_col, k=10):\n",
    "    X = df.drop(class_col, axis=1)\n",
    "    y = df[class_col]\n",
    "    \n",
    "    info_gains = mutual_info_classif(X, y)\n",
    "    selected_features = X.columns[np.argsort(info_gains, kind='heapsort')[::-1][:k]].tolist()\n",
    "    \n",
    "    return df[selected_features + [class_col]]\n",
    "\n",
    "df_info_gain = select_features_info_gain(df, 'Class')\n",
    "df_info_gain.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter methods: chi-square test\n",
    "The statistical chi-square test is a statistical method used to determine the significance of association between two categorical variables. In our context, it can measure the dependence between each feature and the class variable. Features high high chi-square and low p-value (which measures the statistical significance of the association) are more important. Usually a threshold for the p-value is 0.05 or 0.01.\n",
    "\n",
    "Considering categorical variables, the chi-square formula is:\n",
    "$$X^2 = \\sum\\frac{(O-E)^2}{E}$$\n",
    "where $O$ is the observed frequency and $E$ is the expected frequency. I.e. the chi-square test compare the observed frequencies of the categories with the expected frequencies. The expected frequencies are calculated as there is no association between the variables. If the observed frequencies differ significantly, it indicates that there may be an association.\n",
    "\n",
    "If the variable is numeric and \"continuous\", the method discretizes it to make it categorical.\n",
    "\n",
    "https://www.scribbr.com/statistics/chi-square-tests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V3        V4       V10       V11       V12       V14       V16  \\\n",
       "0   0.0  2.536347  1.378155  0.090794 -0.551600 -0.617801 -0.311169 -0.470401   \n",
       "1   0.0  0.166480  0.448154 -0.166974  1.612727  1.065235 -0.143772  0.463917   \n",
       "2   1.0  1.773209  0.379780  0.207643  0.624501  0.066084 -0.165946 -2.890083   \n",
       "3   1.0  1.792993 -0.863291 -0.054952 -0.226487  0.178228 -0.287924 -1.059647   \n",
       "4   2.0  1.548718  0.403034  0.753074 -0.822843  0.538196 -1.119670 -0.451449   \n",
       "\n",
       "        V17  Amount  Class  \n",
       "0  0.207971  149.62      0  \n",
       "1 -0.114805    2.69      0  \n",
       "2  1.109969  378.66      0  \n",
       "3 -0.684093  123.50      0  \n",
       "4 -0.237033   69.99      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def select_features_chi2(df_o, class_col, k=10):\n",
    "    df = df_o.copy()\n",
    "    X = df.drop(class_col, axis=1)\n",
    "    y = df[class_col]\n",
    "\n",
    "    for col in X.columns:\n",
    "        if min(X[col]) < 0:\n",
    "            X[col] = X[col] - min(X[col])\n",
    "    \n",
    "    chi2_selector = SelectKBest(chi2, k=k)\n",
    "    chi2_selector.fit(X, y)\n",
    "    selected_features = X.columns[chi2_selector.get_support()].tolist()\n",
    "    \n",
    "    return df[selected_features + [class_col]]\n",
    "\n",
    "df_chi2 = select_features_chi2(df, 'Class')\n",
    "df_chi2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection method comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def applyLogisticRegression(df, method_name = \"\",class_weights = None):\n",
    "    # Split data into train and test sets\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(random_state=42)\n",
    "    if class_weights != None:\n",
    "        clf = LogisticRegression(random_state=42,class_weight={0:class_weights[0], 1:class_weights[1]})\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "    precision_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "    recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "    f1_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "    f1_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    matthews_corr = matthews_corrcoef(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    g_mean = (recall_0*recall_1)**0.5\n",
    "    classification_error = 1 - accuracy\n",
    "    sensitivity_0 = recall_0\n",
    "    sensitivity_1 = recall_1\n",
    "    specificity_0 = 1 - recall_0\n",
    "    specificity_1 = 1 - recall_1\n",
    "    \n",
    "    # Return dictionary with performance metrics\n",
    "    return {\n",
    "        'method_name': method_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_0': precision_0,\n",
    "        'precision_1': precision_1,\n",
    "        'recall_0': recall_0,\n",
    "        'recall_1': recall_1,\n",
    "        'f1_0': f1_0,\n",
    "        'f1_1': f1_1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cohen_kappa': cohen_kappa,\n",
    "        'matthews_corr': matthews_corr,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'g_mean': g_mean,\n",
    "        'classification_error': classification_error,\n",
    "        'sensitivity_0': sensitivity_0,\n",
    "        'sensitivity_1': sensitivity_1,\n",
    "        'specificity_0': specificity_0,\n",
    "        'specificity_1': specificity_1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "df_ans = applyLogisticRegression(df,method_name=\"df\")\n",
    "df_pca_ans = applyLogisticRegression(df_pca,method_name=\"pca\")\n",
    "df_lda_ans = applyLogisticRegression(df_lda,method_name=\"lda\")\n",
    "df_red_ans = applyLogisticRegression(df_red,method_name=\"red\")\n",
    "df_irr_ans = applyLogisticRegression(df_irr,method_name=\"irr\")\n",
    "df_pearson_ans = applyLogisticRegression(df_pearson,method_name=\"pearson\")\n",
    "df_spearman_ans = applyLogisticRegression(df_spearman,method_name=\"spearman\")\n",
    "df_anova_ans = applyLogisticRegression(df_anova,method_name=\"anova\")\n",
    "df_info_gain_ans = applyLogisticRegression(df_info_gain,method_name=\"info\")\n",
    "df_chi2_ans = applyLogisticRegression(df_chi2,method_name=\"chi2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== accuracy ======\n",
      "score: 0.9769621722385382 (spearman)\t0.976839069342103 (pearson)\t0.9761444172836466 (info)\t0.9752387316884441 (irr)\t0.975221145560382 (anova)\t0.9739021859557181 (df)\t0.9655751543182738 (chi2)\t0.955498302938642 (lda)\t0.9537660693245168 (red)\t0.9485517823540791 (pca)\t\n",
      "improvement(from next/worst): 0.012602%, 2.995133% (spearman)\t0.071163%, 2.982155% (pearson)\t0.092868%, 2.908922% (info)\t0.001803%, 2.813441% (irr)\t0.13543%, 2.811587% (anova)\t0.862391%, 2.672538% (df)\t1.054617%, 1.79467% (chi2)\t0.18162%, 0.732329% (lda)\t0.54971%, 0.54971% (red)\t\n",
      "====== precision_0 ======\n",
      "score: 0.9661454721952573 (df)\t0.9647468919568651 (spearman)\t0.9644193470872954 (pearson)\t0.9638767276161044 (info)\t0.9628574370945041 (irr)\t0.9625544615595732 (anova)\t0.9598586523230108 (chi2)\t0.9478344059836493 (red)\t0.9429903498397354 (lda)\t0.9192599792425166 (pca)\t\n",
      "improvement(from next/worst): 0.144969%, 5.100352% (df)\t0.033963%, 4.94821% (spearman)\t0.056296%, 4.912578% (pearson)\t0.105861%, 4.853551% (info)\t0.031476%, 4.742669% (irr)\t0.280855%, 4.70971% (anova)\t1.268602%, 4.416452% (chi2)\t0.513691%, 3.108416% (red)\t2.581465%, 2.581465% (lda)\t\n",
      "====== precision_1 ======\n",
      "score: 0.9898853310255301 (pearson)\t0.989781942692377 (spearman)\t0.989023269227303 (info)\t0.9885436963267663 (anova)\t0.9882446271455358 (irr)\t0.9820839226779821 (pca)\t0.9818872907669796 (df)\t0.9714102951367295 (chi2)\t0.9686738286539677 (lda)\t0.9598300021338644 (red)\t\n",
      "improvement(from next/worst): 0.010446%, 3.131318% (pearson)\t0.076709%, 3.120546% (spearman)\t0.048513%, 3.041504% (info)\t0.030263%, 2.99154% (anova)\t0.627309%, 2.960381% (irr)\t0.020026%, 2.318527% (pca)\t1.078535%, 2.298041% (df)\t0.282496%, 1.206494% (chi2)\t0.921395%, 0.921395% (lda)\t\n",
      "====== recall_0 ======\n",
      "score: 0.9901145374449339 (pearson)\t0.9900088105726872 (spearman)\t0.9892687224669604 (info)\t0.9888105726872247 (anova)\t0.988511013215859 (irr)\t0.9832599118942731 (pca)\t0.9821145374449339 (df)\t0.971647577092511 (chi2)\t0.9694273127753305 (lda)\t0.960193832599119 (red)\t\n",
      "improvement(from next/worst): 0.010679%, 3.116111% (pearson)\t0.074812%, 3.1051% (spearman)\t0.046333%, 3.028023% (info)\t0.030304%, 2.980309% (anova)\t0.53405%, 2.949111% (irr)\t0.116623%, 2.402232% (pca)\t1.077238%, 2.282946% (df)\t0.229028%, 1.192858% (chi2)\t0.961627%, 0.961627% (lda)\t\n",
      "====== recall_1 ======\n",
      "score: 0.9657224094355518 (df)\t0.9639672844706543 (spearman)\t0.9636162594776748 (pearson)\t0.9630721707385566 (info)\t0.9620190957596181 (irr)\t0.9616856220162876 (anova)\t0.9595268183094636 (chi2)\t0.947363802302724 (red)\t0.9416245436675091 (lda)\t0.9139813254703735 (pca)\t\n",
      "improvement(from next/worst): 0.182073%, 5.661066% (df)\t0.036428%, 5.469035% (spearman)\t0.056495%, 5.430629% (pearson)\t0.109465%, 5.371099% (info)\t0.034676%, 5.255881% (irr)\t0.224986%, 5.219395% (anova)\t1.28388%, 4.983197% (chi2)\t0.609506%, 3.652424% (red)\t3.024484%, 3.024484% (lda)\t\n",
      "====== f1_0 ======\n",
      "score: 0.9772146174316872 (spearman)\t0.9770980419434494 (pearson)\t0.9764076698986913 (info)\t0.9755155983723437 (irr)\t0.9755058758083583 (anova)\t0.9740645589751656 (df)\t0.9657171379283168 (chi2)\t0.9560261010852282 (lda)\t0.9539740896358543 (red)\t0.9501834807707044 (pca)\t\n",
      "improvement(from next/worst): 0.011931%, 2.844833% (spearman)\t0.070705%, 2.832565% (pearson)\t0.091446%, 2.759908% (info)\t0.000997%, 2.666024% (irr)\t0.147969%, 2.665001% (anova)\t0.864375%, 2.513312% (df)\t1.013679%, 1.634806% (chi2)\t0.215101%, 0.614894% (lda)\t0.398934%, 0.398934% (red)\t\n",
      "====== f1_1 ======\n",
      "score: 0.9767040705635481 (spearman)\t0.9765741728922093 (pearson)\t0.9758752234187288 (info)\t0.974955531839203 (irr)\t0.9749297178036369 (anova)\t0.9737377670023183 (df)\t0.9654319897576267 (chi2)\t0.9549576810459145 (lda)\t0.9535561601243684 (red)\t0.946809574458414 (pca)\t\n",
      "improvement(from next/worst): 0.013301%, 3.157393% (spearman)\t0.071623%, 3.143673% (pearson)\t0.094332%, 3.069852% (info)\t0.002648%, 2.972716% (irr)\t0.12241%, 2.969989% (anova)\t0.860317%, 2.844098% (df)\t1.096835%, 1.96686% (chi2)\t0.146978%, 0.860586% (lda)\t0.71256%, 0.71256% (red)\t\n",
      "====== roc_auc ======\n",
      "score: 0.9769880475216708 (spearman)\t0.9768653984613045 (pearson)\t0.9761704466027585 (info)\t0.9752650544877386 (irr)\t0.9752480973517561 (anova)\t0.9739184734402427 (df)\t0.9655871977009873 (chi2)\t0.9555259282214198 (lda)\t0.9537788174509215 (red)\t0.9486206186823233 (pca)\t\n",
      "improvement(from next/worst): 0.012555%, 2.990387% (spearman)\t0.071192%, 2.977458% (pearson)\t0.092835%, 2.904199% (info)\t0.001739%, 2.808756% (irr)\t0.136523%, 2.806968% (anova)\t0.86282%, 2.666804% (df)\t1.052956%, 1.788553% (chi2)\t0.183178%, 0.727932% (lda)\t0.543758%, 0.543758% (red)\t\n",
      "====== cohen_kappa ======\n",
      "score: 0.9539265552295917 (spearman)\t0.9536804033357604 (pearson)\t0.9522911387982101 (info)\t0.9504798845077833 (irr)\t0.9504447763144874 (anova)\t0.9478058767657633 (df)\t0.9311517137936647 (chi2)\t0.9110012029087562 (lda)\t0.9075341647810721 (red)\t0.897117364291012 (pca)\t\n",
      "improvement(from next/worst): 0.025811%, 6.332415% (spearman)\t0.145887%, 6.304977% (pearson)\t0.190562%, 6.150118% (info)\t0.003694%, 5.948221% (irr)\t0.278422%, 5.944307% (anova)\t1.788555%, 5.650154% (df)\t2.211908%, 3.793745% (chi2)\t0.382028%, 1.547606% (lda)\t1.161141%, 1.161141% (red)\t\n",
      "====== matthews_corr ======\n",
      "score: 0.9542524248252952 (spearman)\t0.9540176943660524 (pearson)\t0.9526204040064358 (info)\t0.9508160436009094 (irr)\t0.9507971286559956 (anova)\t0.9479348498651148 (df)\t0.9312216702308086 (chi2)\t0.9113579660425011 (lda)\t0.9076110199395508 (red)\t0.8992902300426858 (pca)\t\n",
      "improvement(from next/worst): 0.024604%, 6.11173% (spearman)\t0.146679%, 6.085629% (pearson)\t0.18977%, 5.930252% (info)\t0.001989%, 5.729609% (irr)\t0.301949%, 5.727506% (anova)\t1.794758%, 5.409224% (df)\t2.179572%, 3.550738% (chi2)\t0.412836%, 1.341918% (lda)\t0.925262%, 0.925262% (red)\t\n",
      "====== balanced_accuracy ======\n",
      "score: 0.9769880475216708 (spearman)\t0.9768653984613044 (pearson)\t0.9761704466027585 (info)\t0.9752650544877386 (irr)\t0.9752480973517561 (anova)\t0.9739184734402428 (df)\t0.9655871977009873 (chi2)\t0.9555259282214198 (lda)\t0.9537788174509214 (red)\t0.9486206186823233 (pca)\t\n",
      "improvement(from next/worst): 0.012555%, 2.990387% (spearman)\t0.071192%, 2.977458% (pearson)\t0.092835%, 2.904199% (info)\t0.001739%, 2.808756% (irr)\t0.136523%, 2.806968% (anova)\t0.86282%, 2.666804% (df)\t1.052956%, 1.788553% (chi2)\t0.183178%, 0.727932% (lda)\t0.543758%, 0.543758% (red)\t\n",
      "====== g_mean ======\n",
      "score: 0.9769012768595277 (spearman)\t0.9767755458789678 (pearson)\t0.9760825661746112 (info)\t0.9751750976529008 (irr)\t0.9751537882052221 (anova)\t0.973883985617899 (df)\t0.965568178931802 (chi2)\t0.9554248013375465 (lda)\t0.9537572438512467 (red)\t0.9479879733177052 (pca)\t\n",
      "improvement(from next/worst): 0.012872%, 3.049965% (spearman)\t0.070996%, 3.036702% (pearson)\t0.093057%, 2.963602% (info)\t0.002185%, 2.867877% (irr)\t0.130385%, 2.865629% (anova)\t0.861235%, 2.731682% (df)\t1.061662%, 1.854476% (chi2)\t0.174841%, 0.784485% (lda)\t0.608581%, 0.608581% (red)\t\n",
      "====== classification_error ======\n",
      "score: 0.05144821764592089 (pca)\t0.046233930675483204 (red)\t0.04450169706135798 (lda)\t0.03442484568172621 (chi2)\t0.02609781404428191 (df)\t0.024778854439618025 (anova)\t0.024761268311555895 (irr)\t0.023855582716353352 (info)\t0.02316093065789704 (pearson)\t0.023037827761461793 (spearman)\t\n",
      "improvement(from next/worst): 11.278052%, 123.320611% (pca)\t3.892511%, 100.687023% (red)\t29.272031%, 93.167939% (lda)\t31.907008%, 49.427481% (chi2)\t5.322924%, 13.282443% (df)\t0.071023%, 7.557252% (anova)\t3.796535%, 7.480916% (irr)\t2.999241%, 3.549618% (info)\t0.534351%, 0.534351% (pearson)\t\n",
      "====== sensitivity_0 ======\n",
      "score: 0.9901145374449339 (pearson)\t0.9900088105726872 (spearman)\t0.9892687224669604 (info)\t0.9888105726872247 (anova)\t0.988511013215859 (irr)\t0.9832599118942731 (pca)\t0.9821145374449339 (df)\t0.971647577092511 (chi2)\t0.9694273127753305 (lda)\t0.960193832599119 (red)\t\n",
      "improvement(from next/worst): 0.010679%, 3.116111% (pearson)\t0.074812%, 3.1051% (spearman)\t0.046333%, 3.028023% (info)\t0.030304%, 2.980309% (anova)\t0.53405%, 2.949111% (irr)\t0.116623%, 2.402232% (pca)\t1.077238%, 2.282946% (df)\t0.229028%, 1.192858% (chi2)\t0.961627%, 0.961627% (lda)\t\n",
      "====== sensitivity_1 ======\n",
      "score: 0.9657224094355518 (df)\t0.9639672844706543 (spearman)\t0.9636162594776748 (pearson)\t0.9630721707385566 (info)\t0.9620190957596181 (irr)\t0.9616856220162876 (anova)\t0.9595268183094636 (chi2)\t0.947363802302724 (red)\t0.9416245436675091 (lda)\t0.9139813254703735 (pca)\t\n",
      "improvement(from next/worst): 0.182073%, 5.661066% (df)\t0.036428%, 5.469035% (spearman)\t0.056495%, 5.430629% (pearson)\t0.109465%, 5.371099% (info)\t0.034676%, 5.255881% (irr)\t0.224986%, 5.219395% (anova)\t1.28388%, 4.983197% (chi2)\t0.609506%, 3.652424% (red)\t3.024484%, 3.024484% (lda)\t\n",
      "====== specificity_0 ======\n",
      "score: 0.039806167400881054 (red)\t0.030572687224669548 (lda)\t0.02835242290748896 (chi2)\t0.017885462555066067 (df)\t0.016740088105726914 (pca)\t0.011488986784141009 (irr)\t0.01118942731277528 (anova)\t0.010731277533039596 (info)\t0.009991189427312808 (spearman)\t0.00988546255506606 (pearson)\t\n",
      "improvement(from next/worst): 30.201729%, 302.673797% (red)\t7.830951%, 209.269162% (lda)\t58.522167%, 186.809269% (chi2)\t6.842105%, 80.926916% (df)\t45.705521%, 69.340463% (pca)\t2.677165%, 16.221034% (irr)\t4.269294%, 13.190731% (anova)\t7.407407%, 8.55615% (info)\t1.069519%, 1.069519% (spearman)\t\n",
      "====== specificity_1 ======\n",
      "score: 0.08601867452962653 (pca)\t0.05837545633249086 (lda)\t0.05263619769727601 (red)\t0.040473181690536375 (chi2)\t0.0383143779837124 (anova)\t0.03798090424038192 (irr)\t0.036927829261443446 (info)\t0.03638374052232518 (pearson)\t0.03603271552934573 (spearman)\t0.034277590564448235 (df)\t\n",
      "improvement(from next/worst): 47.354179%, 150.947261% (pca)\t10.903635%, 70.302099% (lda)\t30.052038%, 53.558628% (red)\t5.634448%, 18.074757% (chi2)\t0.878004%, 11.776754% (anova)\t2.851711%, 10.803891% (irr)\t1.495417%, 7.731695% (info)\t0.974184%, 6.144393% (pearson)\t5.120328%, 5.120328% (spearman)\t\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison\n",
    "\n",
    "ans = [df_ans,\n",
    "       df_pca_ans,\n",
    "       df_lda_ans,\n",
    "       df_red_ans,\n",
    "       df_irr_ans,\n",
    "       df_pearson_ans,\n",
    "       df_spearman_ans,\n",
    "       df_anova_ans,\n",
    "       df_info_gain_ans,\n",
    "       df_chi2_ans\n",
    "       ]\n",
    "\n",
    "n = len(ans)\n",
    "name_str = 'method_name'\n",
    "for score_name in [v for v in df_ans if v != name_str]:\n",
    "    print(f'====== {score_name} ======')\n",
    "    ans = sorted(ans, key = lambda x: x[score_name], reverse = True)\n",
    "    print('score:',end=\" \")\n",
    "    for i in range(n):\n",
    "        text = f\"{ans[i][score_name]} ({ans[i][name_str]})\"\n",
    "        print(text,end='\\t')\n",
    "    print()\n",
    "    print('improvement(from next/worst):',end=\" \")\n",
    "    for i in range(n-1):\n",
    "        text = f\"{round(100*((ans[i][score_name]/ans[i+1][score_name])-1),6)}%, {round(100*((ans[i][score_name]/ans[n-1][score_name])-1),6)}% ({ans[i][name_str]})\"\n",
    "        print(text,end='\\t')\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman and pearson seems to have a good performance even having a better precision on 1 even though the unchaged df had a better recall.\n",
    "The metrics, though, were worse than before when we compared some other models with the decision tree classifier. Changing to this classifier we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def applyDecisionTree(df, method_name = \"\"):\n",
    "    # Split data into train and test sets\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "    # Initialize a decision tree classifier and fit it to the training data\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "    precision_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "    recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "    f1_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "    f1_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    matthews_corr = matthews_corrcoef(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    g_mean = (recall_0*recall_1)**0.5\n",
    "    classification_error = 1 - accuracy\n",
    "    sensitivity_0 = recall_0\n",
    "    sensitivity_1 = recall_1\n",
    "    specificity_0 = 1 - recall_0\n",
    "    specificity_1 = 1 - recall_1\n",
    "    \n",
    "    # Return dictionary with performance metrics\n",
    "    return {\n",
    "        'method_name': method_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_0': precision_0,\n",
    "        'precision_1': precision_1,\n",
    "        'recall_0': recall_0,\n",
    "        'recall_1': recall_1,\n",
    "        'f1_0': f1_0,\n",
    "        'f1_1': f1_1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cohen_kappa': cohen_kappa,\n",
    "        'matthews_corr': matthews_corr,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'g_mean': g_mean,\n",
    "        'classification_error': classification_error,\n",
    "        'sensitivity_0': sensitivity_0,\n",
    "        'sensitivity_1': sensitivity_1,\n",
    "        'specificity_0': specificity_0,\n",
    "        'specificity_1': specificity_1\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans = applyDecisionTree(df,method_name=\"df\")\n",
    "df_pca_ans = applyDecisionTree(df_pca,method_name=\"pca\")\n",
    "df_lda_ans = applyDecisionTree(df_lda,method_name=\"lda\")\n",
    "df_red_ans = applyDecisionTree(df_red,method_name=\"red\")\n",
    "df_irr_ans = applyDecisionTree(df_irr,method_name=\"irr\")\n",
    "df_pearson_ans = applyDecisionTree(df_pearson,method_name=\"pearson\")\n",
    "df_spearman_ans = applyDecisionTree(df_spearman,method_name=\"spearman\")\n",
    "df_anova_ans = applyDecisionTree(df_anova,method_name=\"anova\")\n",
    "df_info_gain_ans = applyDecisionTree(df_info_gain,method_name=\"info\")\n",
    "df_chi2_ans = applyDecisionTree(df_chi2,method_name=\"chi2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== accuracy ======\n",
      "score: 0.9986282820111496 (df)\t0.9983644900902169 (chi2)\t0.9982589733218438 (red)\t0.9980567328491287 (info)\t0.9980391467210664 (pearson)\t0.9979951814009109 (spearman)\t0.9978544923764134 (anova)\t0.9977841478641647 (irr)\t0.988278845646554 (pca)\t0.930912895907708 (lda)\t\n",
      "improvement(from next/worst): 0.026422%, 7.274084% (df)\t0.01057%, 7.245747% (chi2)\t0.020263%, 7.234412% (red)\t0.001762%, 7.212687% (info)\t0.004405%, 7.210798% (pearson)\t0.014099%, 7.206075% (spearman)\t0.00705%, 7.190962% (anova)\t0.961804%, 7.183406% (irr)\t6.162333%, 6.162333% (pca)\t\n",
      "====== precision_0 ======\n",
      "score: 0.9995233724050275 (df)\t0.999187910458301 (red)\t0.999011927447244 (chi2)\t0.9988526442137965 (spearman)\t0.9988351776354106 (info)\t0.9987823171269743 (pearson)\t0.9985178388679112 (anova)\t0.9984296704072271 (irr)\t0.9900776455190222 (pca)\t0.9320009189065013 (lda)\t\n",
      "improvement(from next/worst): 0.033573%, 7.244891% (df)\t0.017616%, 7.208898% (red)\t0.015947%, 7.190015% (chi2)\t0.001749%, 7.172925% (spearman)\t0.005292%, 7.171051% (info)\t0.026487%, 7.165379% (pearson)\t0.008831%, 7.137002% (anova)\t0.843573%, 7.127541% (irr)\t6.231402%, 6.231402% (pca)\t\n",
      "====== precision_1 ======\n",
      "score: 0.9977399348260275 (df)\t0.9977212971078002 (chi2)\t0.9973371640797449 (red)\t0.9973011338742749 (pearson)\t0.9972837991763778 (info)\t0.997195540910047 (anova)\t0.9971440585906016 (spearman)\t0.9971428571428571 (irr)\t0.9865004284190463 (pca)\t0.9298353081190822 (lda)\t\n",
      "improvement(from next/worst): 0.001868%, 7.302866% (df)\t0.038516%, 7.300862% (chi2)\t0.003613%, 7.25955% (red)\t0.001738%, 7.255675% (pearson)\t0.008851%, 7.25381% (info)\t0.005163%, 7.244319% (anova)\t0.00012%, 7.238782% (spearman)\t1.078806%, 7.238653% (irr)\t6.094103%, 6.094103% (pca)\t\n",
      "====== recall_0 ======\n",
      "score: 0.997726872246696 (df)\t0.9977092511013216 (chi2)\t0.9973215859030837 (red)\t0.9972863436123348 (pearson)\t0.9972687224669603 (info)\t0.9971806167400881 (anova)\t0.9971277533039647 (spearman)\t0.9971277533039647 (irr)\t0.9863964757709252 (pca)\t0.9293568281938326 (lda)\t\n",
      "improvement(from next/worst): 0.001766%, 7.356705% (df)\t0.038871%, 7.354809% (chi2)\t0.003534%, 7.313096% (red)\t0.001767%, 7.309304% (pearson)\t0.008835%, 7.307408% (info)\t0.005302%, 7.297928% (anova)\t0.0%, 7.292239% (spearman)\t1.087927%, 7.292239% (irr)\t6.13754%, 6.13754% (pca)\t\n",
      "====== recall_1 ======\n",
      "score: 0.9995261162594776 (df)\t0.9991926425161471 (red)\t0.9990171300196574 (chi2)\t0.9988591687728167 (spearman)\t0.9988416175231677 (info)\t0.9987889637742208 (pearson)\t0.9985256950294861 (anova)\t0.9984379387812412 (irr)\t0.990153748946925 (pca)\t0.9324627913507442 (lda)\t\n",
      "improvement(from next/worst): 0.033374%, 7.192064% (df)\t0.017569%, 7.156302% (red)\t0.015814%, 7.137479% (chi2)\t0.001757%, 7.120539% (spearman)\t0.005272%, 7.118657% (info)\t0.026366%, 7.11301% (pearson)\t0.008789%, 7.084776% (anova)\t0.836657%, 7.075365% (irr)\t6.186945%, 6.186945% (pca)\t\n",
      "====== f1_0 ======\n",
      "score: 0.9986243143618053 (df)\t0.9983601643362192 (chi2)\t0.998253875866448 (red)\t0.9980513354083819 (info)\t0.9980337697835383 (pearson)\t0.9979894534487928 (spearman)\t0.9978487797996897 (anova)\t0.9977782871652002 (irr)\t0.988233632568034 (pca)\t0.9306769955619866 (lda)\t\n",
      "improvement(from next/worst): 0.026458%, 7.300849% (df)\t0.010647%, 7.272466% (chi2)\t0.020294%, 7.261046% (red)\t0.00176%, 7.239283% (info)\t0.004441%, 7.237395% (pearson)\t0.014098%, 7.232634% (spearman)\t0.007065%, 7.217518% (anova)\t0.96583%, 7.209944% (irr)\t6.184384%, 6.184384% (pca)\t\n",
      "====== f1_1 ======\n",
      "score: 0.9986322268399179 (df)\t0.9983687930822795 (chi2)\t0.9982640411018956 (red)\t0.9980621004726371 (info)\t0.9980444943307875 (pearson)\t0.9980008768084173 (spearman)\t0.9978601746939348 (anova)\t0.9977899777243786 (irr)\t0.9883237125863897 (pca)\t0.9311471962002577 (lda)\t\n",
      "improvement(from next/worst): 0.026386%, 7.247515% (df)\t0.010493%, 7.219223% (chi2)\t0.020233%, 7.207974% (red)\t0.001764%, 7.186286% (info)\t0.00437%, 7.184396% (pearson)\t0.0141%, 7.179711% (spearman)\t0.007035%, 7.164601% (anova)\t0.95781%, 7.157062% (irr)\t6.140438%, 6.140438% (pca)\t\n",
      "====== roc_auc ======\n",
      "score: 0.9986264942530867 (df)\t0.9983631905604896 (chi2)\t0.9982571142096155 (red)\t0.998055169995064 (info)\t0.9980376536932778 (pearson)\t0.9979934610383907 (spearman)\t0.9978531558847871 (anova)\t0.9977828460426029 (irr)\t0.9882751123589251 (pca)\t0.9309098097722883 (lda)\t\n",
      "improvement(from next/worst): 0.026374%, 7.274248% (df)\t0.010626%, 7.245963% (chi2)\t0.020234%, 7.234568% (red)\t0.001755%, 7.212875% (info)\t0.004428%, 7.210993% (pearson)\t0.014061%, 7.206246% (spearman)\t0.007047%, 7.191174% (anova)\t0.962053%, 7.183621% (irr)\t6.162284%, 6.162284% (pca)\t\n",
      "====== cohen_kappa ======\n",
      "score: 0.9972565434086226 (df)\t0.996728958803445 (chi2)\t0.9965179199934081 (red)\t0.9961134382613063 (info)\t0.9960782663051103 (pearson)\t0.9959903332346598 (spearman)\t0.9957089564099693 (anova)\t0.9955682667659578 (irr)\t0.9765574258513685 (pca)\t0.8618244686912176 (lda)\t\n",
      "improvement(from next/worst): 0.052932%, 15.714578% (df)\t0.021178%, 15.65336% (chi2)\t0.040606%, 15.628873% (red)\t0.003531%, 15.58194% (info)\t0.008829%, 15.577859% (pearson)\t0.028259%, 15.567656% (spearman)\t0.014132%, 15.535007% (anova)\t1.94672%, 15.518682% (irr)\t13.312799%, 13.312799% (pca)\t\n",
      "====== matthews_corr ======\n",
      "score: 0.9972581478552683 (df)\t0.9967298028321383 (chi2)\t0.9965196514638822 (red)\t0.9961146583915976 (info)\t0.99607937918558 (pearson)\t0.9959918124285838 (spearman)\t0.9957098457674948 (anova)\t0.9955691098117787 (irr)\t0.9765641492286853 (pca)\t0.8618279232450767 (lda)\t\n",
      "improvement(from next/worst): 0.053008%, 15.7143% (df)\t0.021089%, 15.652995% (chi2)\t0.040657%, 15.62861% (red)\t0.003542%, 15.581618% (info)\t0.008792%, 15.577525% (pearson)\t0.028318%, 15.567364% (spearman)\t0.014136%, 15.534647% (anova)\t1.946105%, 15.518317% (irr)\t13.313125%, 13.313125% (pca)\t\n",
      "====== balanced_accuracy ======\n",
      "score: 0.9986264942530868 (df)\t0.9983631905604895 (chi2)\t0.9982571142096155 (red)\t0.9980551699950639 (info)\t0.9980376536932778 (pearson)\t0.9979934610383907 (spearman)\t0.9978531558847871 (anova)\t0.997782846042603 (irr)\t0.9882751123589251 (pca)\t0.9309098097722883 (lda)\t\n",
      "improvement(from next/worst): 0.026374%, 7.274248% (df)\t0.010626%, 7.245963% (chi2)\t0.020234%, 7.234568% (red)\t0.001755%, 7.212875% (info)\t0.004428%, 7.210993% (pearson)\t0.014061%, 7.206246% (spearman)\t0.007047%, 7.191174% (anova)\t0.962053%, 7.183621% (irr)\t6.162284%, 6.162284% (pca)\t\n",
      "====== g_mean ======\n",
      "score: 0.9986260890365604 (df)\t0.9983629763915046 (chi2)\t0.9982566758388831 (red)\t0.9980548601425483 (info)\t0.9980373709048902 (pearson)\t0.9979930855599675 (spearman)\t0.9978529292437479 (anova)\t0.9977826309925326 (irr)\t0.9882733267839501 (pca)\t0.9309085143978946 (lda)\t\n",
      "improvement(from next/worst): 0.026354%, 7.274353% (df)\t0.010649%, 7.246089% (chi2)\t0.020221%, 7.23467% (red)\t0.001752%, 7.212991% (info)\t0.004437%, 7.211112% (pearson)\t0.014046%, 7.206355% (spearman)\t0.007045%, 7.191299% (anova)\t0.962214%, 7.183747% (irr)\t6.16224%, 6.16224% (pca)\t\n",
      "====== classification_error ======\n",
      "score: 0.06908710409229202 (lda)\t0.011721154353446006 (pca)\t0.0022158521358353056 (irr)\t0.0021455076235865622 (anova)\t0.0020048185990890754 (spearman)\t0.001960853278933583 (pearson)\t0.0019432671508713417 (info)\t0.0017410266781562322 (red)\t0.0016355099097831172 (chi2)\t0.001371717988850385 (df)\t\n",
      "improvement(from next/worst): 489.422356%, 4936.538462% (lda)\t428.968254%, 754.487179% (pca)\t3.278689%, 61.538462% (irr)\t7.017544%, 56.410256% (anova)\t2.242152%, 46.153846% (spearman)\t0.904977%, 42.948718% (pearson)\t11.616162%, 41.666667% (info)\t6.451613%, 26.923077% (red)\t19.230769%, 19.230769% (chi2)\t\n",
      "====== sensitivity_0 ======\n",
      "score: 0.997726872246696 (df)\t0.9977092511013216 (chi2)\t0.9973215859030837 (red)\t0.9972863436123348 (pearson)\t0.9972687224669603 (info)\t0.9971806167400881 (anova)\t0.9971277533039647 (irr)\t0.9971277533039647 (spearman)\t0.9863964757709252 (pca)\t0.9293568281938326 (lda)\t\n",
      "improvement(from next/worst): 0.001766%, 7.356705% (df)\t0.038871%, 7.354809% (chi2)\t0.003534%, 7.313096% (red)\t0.001767%, 7.309304% (pearson)\t0.008835%, 7.307408% (info)\t0.005302%, 7.297928% (anova)\t0.0%, 7.292239% (irr)\t1.087927%, 7.292239% (spearman)\t6.13754%, 6.13754% (pca)\t\n",
      "====== sensitivity_1 ======\n",
      "score: 0.9995261162594776 (df)\t0.9991926425161471 (red)\t0.9990171300196574 (chi2)\t0.9988591687728167 (spearman)\t0.9988416175231677 (info)\t0.9987889637742208 (pearson)\t0.9985256950294861 (anova)\t0.9984379387812412 (irr)\t0.990153748946925 (pca)\t0.9324627913507442 (lda)\t\n",
      "improvement(from next/worst): 0.033374%, 7.192064% (df)\t0.017569%, 7.156302% (red)\t0.015814%, 7.137479% (chi2)\t0.001757%, 7.120539% (spearman)\t0.005272%, 7.118657% (info)\t0.026366%, 7.11301% (pearson)\t0.008789%, 7.084776% (anova)\t0.836657%, 7.075365% (irr)\t6.186945%, 6.186945% (pca)\t\n",
      "====== specificity_0 ======\n",
      "score: 0.07064317180616742 (lda)\t0.013603524229074848 (pca)\t0.002872246696035252 (spearman)\t0.002872246696035252 (irr)\t0.002819383259911934 (anova)\t0.0027312775330397 (info)\t0.0027136563876651865 (pearson)\t0.0026784140969162706 (red)\t0.0022907488986784186 (chi2)\t0.002273127753304016 (df)\t\n",
      "improvement(from next/worst): 419.300518%, 3007.751938% (lda)\t373.619632%, 498.449612% (pca)\t0.0%, 26.356589% (spearman)\t1.875%, 26.356589% (irr)\t3.225806%, 24.031008% (anova)\t0.649351%, 20.155039% (info)\t1.315789%, 19.379845% (pearson)\t16.923077%, 17.829457% (red)\t0.775194%, 0.775194% (chi2)\t\n",
      "====== specificity_1 ======\n",
      "score: 0.06753720864925583 (lda)\t0.009846251053075017 (pca)\t0.0015620612187587923 (irr)\t0.0014743049705139288 (anova)\t0.0012110362257792273 (pearson)\t0.0011583824768323092 (info)\t0.0011408312271833365 (spearman)\t0.0009828699803425822 (chi2)\t0.0008073574838528552 (red)\t0.0004738837405223739 (df)\t\n",
      "improvement(from next/worst): 585.918004%, 14151.851852% (lda)\t530.337079%, 1977.777778% (pca)\t5.952381%, 229.62963% (irr)\t21.73913%, 211.111111% (anova)\t4.545455%, 155.555556% (pearson)\t1.538462%, 144.444444% (info)\t16.071429%, 140.740741% (spearman)\t21.73913%, 107.407407% (chi2)\t70.37037%, 70.37037% (red)\t\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison\n",
    "\n",
    "ans = [df_ans,\n",
    "       df_pca_ans,\n",
    "       df_lda_ans,\n",
    "       df_red_ans,\n",
    "       df_irr_ans,\n",
    "       df_pearson_ans,\n",
    "       df_spearman_ans,\n",
    "       df_anova_ans,\n",
    "       df_info_gain_ans,\n",
    "       df_chi2_ans\n",
    "       ]\n",
    "\n",
    "n = len(ans)\n",
    "name_str = 'method_name'\n",
    "for score_name in [v for v in df_ans if v != name_str]:\n",
    "    print(f'====== {score_name} ======')\n",
    "    ans = sorted(ans, key = lambda x: x[score_name], reverse = True)\n",
    "    print('score:',end=\" \")\n",
    "    for i in range(n):\n",
    "        text = f\"{ans[i][score_name]} ({ans[i][name_str]})\"\n",
    "        print(text,end='\\t')\n",
    "    print()\n",
    "    print('improvement(from next/worst):',end=\" \")\n",
    "    for i in range(n-1):\n",
    "        text = f\"{round(100*((ans[i][score_name]/ans[i+1][score_name])-1),6)}%, {round(100*((ans[i][score_name]/ans[n-1][score_name])-1),6)}% ({ans[i][name_str]})\"\n",
    "        print(text,end='\\t')\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the unchanged df performed better and, thus, we shall keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results after feature selection\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"data/feature.pk\",'wb') as f:\n",
    "    pickle.dump(df,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
