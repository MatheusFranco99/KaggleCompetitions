{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "df = None\n",
    "with open('data/normalisation.pk','rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data balancing\n",
    "    - class weights, cost-sensitive learning, undersampling, near miss undersampling, oversampling, SMOTE (Syntheic Minority Over-sampling technique), ensemble methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling\n",
    "The undersampling method restricts the majority class to the number of observartions of the the minority class. This is useful when the minority class still presents a big enough number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "Number of zeros: 284315, percentage: 99.8273%\n",
      "Number of ones: 492, percentage: 0.1727%\n",
      "(984, 31)\n",
      "Number of zeros: 492, percentage: 50.0%\n",
      "Number of ones: 492, percentage: 50.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def countClass(df):\n",
    "    class_counts = df['Class'].value_counts()\n",
    "    if 1 not in class_counts:\n",
    "        class_counts[1] = 0\n",
    "    print(f\"Number of zeros: {(class_counts[0])}, percentage: {round(class_counts[0]/len(df)*100,4)}%\")\n",
    "    print(f\"Number of ones: {(class_counts[1])}, percentage: {round(class_counts[1]/len(df)*100,4)}%\")\n",
    "    \n",
    "\n",
    "def random_under_sampling(df):\n",
    "    # Separate the features from the target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Apply random under sampling to balance the class distribution\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    \n",
    "    # Combine the resampled features and target variable into a new DataFrame\n",
    "    df_resampled = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)], axis=1)\n",
    "    df_resampled.columns = df.columns\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "print(df.shape)\n",
    "countClass(df)\n",
    "df_undersampling = random_under_sampling(df)\n",
    "print(df_undersampling.shape)\n",
    "countClass(df_undersampling)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss Undersampling\n",
    "NearMiss undersamples the data as the normal undersampling method. Nevertheless, the observations on the majority class are not randomly selected. Actually, they are selected based on their distance from the minority class. There are 3 versions that differs on the distance metric:\n",
    "- NearMiss-1: select the samples whose average distance to the k nearest samples of the minority class is the smallest.\n",
    "- NearMiss-2: select the samples whose average distance to the farthest k samples of the minority class is the smallest.\n",
    "- NearMiss-3: select the samples which are farthest from a decision boundary (a hyperplane that separates the classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "Number of zeros: 284315, percentage: 99.8273%\n",
      "Number of ones: 492, percentage: 0.1727%\n",
      "(984, 31)\n",
      "Number of zeros: 492, percentage: 50.0%\n",
      "Number of ones: 492, percentage: 50.0%\n",
      "(984, 31)\n",
      "Number of zeros: 492, percentage: 50.0%\n",
      "Number of ones: 492, percentage: 50.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "def nearmiss_under_sampling(df,v = 1):\n",
    "    # Separate the features from the target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Apply NearMiss under sampling to balance the class distribution\n",
    "    nm = NearMiss(version=v)\n",
    "    X_resampled, y_resampled = nm.fit_resample(X, y)\n",
    "    \n",
    "    # Combine the resampled features and target variable into a new DataFrame\n",
    "    df_resampled = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)], axis=1)\n",
    "    df_resampled.columns = df.columns\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "countClass(df)\n",
    "df_nearmiss_v1 = nearmiss_under_sampling(df,v=1)\n",
    "print(df_nearmiss_v1.shape)\n",
    "countClass(df_nearmiss_v1)\n",
    "df_nearmiss_v3 = nearmiss_under_sampling(df,v=3)\n",
    "print(df_nearmiss_v3.shape)\n",
    "countClass(df_nearmiss_v3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "The oversampling method increases the number of instances of the minority class to match the number of the majority class. This may be challenging since we can't produce new observations of a certain class out of the blue. I way to do it is to duplicate observations, as if we have got multiple equal observations. Thus, random oversampling produces new instances by randomly selecting pre-existing instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "Number of zeros: 284315, percentage: 99.8273%\n",
      "Number of ones: 492, percentage: 0.1727%\n",
      "(568630, 31)\n",
      "Number of zeros: 284315, percentage: 50.0%\n",
      "Number of ones: 284315, percentage: 50.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def random_over_sampling(df):\n",
    "    # Separate the features from the target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Apply random over-sampling to balance the class distribution\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    \n",
    "    # Combine the resampled features and target variable into a new DataFrame\n",
    "    df_resampled = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)], axis=1)\n",
    "    df_resampled.columns = df.columns\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "print(df.shape)\n",
    "countClass(df)\n",
    "df_oversampling = random_over_sampling(df)\n",
    "print(df_oversampling.shape)\n",
    "countClass(df_oversampling)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE (Synthetic Minority Over-sampling technique)\n",
    "The SMOTE method is similar to the oversampling method but they differ on how new instances of the minority class are made. SMOTE is a data augmentation method, i.e. it produces new samples by making small perturbations to the existing ones. The algorithm is:\n",
    "- select a random sample\n",
    "- identify the k nearest neighbors for the sample\n",
    "- take one of the neighbors randomly\n",
    "- get the vector that transforms the sample into the neighbors\n",
    "- multiple the vector by a random number between 0 and 1 and add it to the sample\n",
    "In other words, the new entry is a random point that lies in a line that connects two near already existing points.\n",
    "\n",
    "https://towardsdatascience.com/smote-fdce2f605729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "Number of zeros: 284315, percentage: 99.8273%\n",
      "Number of ones: 492, percentage: 0.1727%\n",
      "(568630, 31)\n",
      "Number of zeros: 284315, percentage: 50.0%\n",
      "Number of ones: 284315, percentage: 50.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def smote_over_sampling(df):\n",
    "    # Separate the features from the target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Apply SMOTE over-sampling to balance the class distribution\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Combine the resampled features and target variable into a new DataFrame\n",
    "    df_resampled = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)], axis=1)\n",
    "    df_resampled.columns = df.columns\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "print(df.shape)\n",
    "countClass(df)\n",
    "df_smote = smote_over_sampling(df)\n",
    "print(df_smote.shape)\n",
    "countClass(df_smote)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights\n",
    "Class weights is a form of training a model but addressing different weights to the classification of each class.\n",
    "The weight of class i has the weight:\n",
    "$$\n",
    "w_i = n_samples/(n_classes * n_classes_i)\n",
    "$$\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/10/improve-class-imbalance-class-weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "Number of zeros: 284315, percentage: 99.8273%\n",
      "Number of ones: 492, percentage: 0.1727%\n",
      "[  0.50086524 289.43800813]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def class_weight_resampling(df):\n",
    "    # Separate the features from the target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Calculate class weights for the imbalanced dataset\n",
    "    class_weights = compute_class_weight('balanced', classes=[0, 1], y=y)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "    # Create a dictionary of class weights\n",
    "    # class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    \n",
    "    # # Apply class weights to balance the class distribution\n",
    "    # df_resampled = df.sample(frac=1, replace=True, weights=y.map(class_weights_dict), random_state=42)\n",
    "    \n",
    "    # return df_resampled\n",
    "\n",
    "print(df.shape)\n",
    "countClass(df)\n",
    "class_weights = class_weight_resampling(df)\n",
    "print(class_weights)\n",
    "# df_class_weight = class_weight_resampling(df)\n",
    "# print(df_class_weight.shape)\n",
    "# countClass(df_class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_class_weight.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble methods\n",
    "\n",
    "We have seen balancing methods that alter data (undersampling, oversampling, etc) and a method that change the weights of the classifier for each class. The ensemble method has a different approach. As usual ensemble methods, it combines models to improve performance. Typically, it has two steps: resampling (undersampling, oversampling, SMOTE, etc) and model aggregation (prediction voting, weighted voting or stacking (meta-classifier that uses the predictions of all classifiers as input and, then, learn to combine them to produce the final prediction)).\n",
    "\n",
    "There are many methods:\n",
    "- Bagging: creates multiples partitions from the original dataset and trains a classifier on each partition. The final prediciton is made by aggregating the predictions from all classifiers.\n",
    "- Boosting: creates multiple resampled datasets. Each dataset gives a higher weight to misclassified examples from the minority class. A classifier is trained in each dataset and the final prediction is a combination of all classifiers.\n",
    "- Adaboost: similar to boosting but each classifier is given a weight based on its performance. The aggregation of prediction is, then, also weighted.\n",
    "- RUSBoost: variant of boosting but uses undersampling instead of SMOTE.\n",
    "- Gradient Boosting: improves a weak model, usually a decision tree, by adding new models that focus on the mistakes of the previous models. It minimizes a loss function by adjusting the weights of the training examples.\n",
    "- XGBoost: (extreme gradient boosting) is an effective and optimized implementation of gradient boosting. It includes tree pruning, regularization and early stopping which help prevent overfitting.\n",
    "- EasyEnsemble: similar to bagging with undersampling. Selects multiples subsets of the majority class and combines with the minority class. The train models on each balanced samples.\n",
    "- Balanced Random Forest (BRF): a random subset of features is selected, as in Random Forest, then a new dataset is constructed undersampling the majority class or oversampling the minority and, at last, multiple decision trees are trained.The final prediction is an aggregation of all predictions.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "def random_forest_ensemble(df):\n",
    "    # Separate the features from the target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Create an ensemble classifier with balanced sampling\n",
    "    clf = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Fit the classifier to the data and make predictions\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    \n",
    "    # Combine the predictions and the original target variable into a new DataFrame\n",
    "    # df_resampled = pd.concat([X, pd.Series(y_pred, name='Class')], axis=1)\n",
    "    \n",
    "    return y_pred,y\n",
    "\n",
    "ensemble_RF_y_pred, ensemble_RF_y_test = random_forest_ensemble(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "def easy_ensemble(df, method='brf'):\n",
    "    # Separate the features from the target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Create an ensemble classifier with balanced sampling\n",
    "    clf = EasyEnsembleClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Fit the classifier to the data and make predictions\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    \n",
    "    # Combine the predictions and the original target variable into a new DataFrame\n",
    "    # df_resampled = pd.concat([X, pd.Series(y_pred, name='Class')], axis=1)\n",
    "    \n",
    "    return y_pred,y\n",
    "\n",
    "ensemble_easy_y_pred, ensemble_easy_y_test = easy_ensemble(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def bagging_ensemble(df):\n",
    "    # Separate the features and target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Create an instance of the Decision Tree classifier\n",
    "    tree = DecisionTreeClassifier()\n",
    "    \n",
    "    # Create an instance of the Random Under-Sampler\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    \n",
    "    # Create an instance of the Bagging classifier\n",
    "    bagging = BaggingClassifier(base_estimator=tree, random_state=42)\n",
    "    \n",
    "    # Fit the Bagging classifier to the data after under-sampling\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    bagging.fit(X_res, y_res)\n",
    "    y_pred = bagging.predict(X)\n",
    "    \n",
    "    return y_pred,y\n",
    "\n",
    "ensemble_bag_y_pred, ensemble_bag_y_test = bagging_ensemble(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def adaboosting_ensemble(df):\n",
    "    # Separate the features and target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Create an instance of the Decision Tree classifier\n",
    "    tree = DecisionTreeClassifier()\n",
    "    \n",
    "    # Create an instance of the Random Under-Sampler\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    \n",
    "    # Create an instance of the AdaBoost classifier\n",
    "    boosting = AdaBoostClassifier(base_estimator=tree, random_state=42)\n",
    "    \n",
    "    # Fit the AdaBoost classifier to the data after under-sampling\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    boosting.fit(X_res, y_res)\n",
    "    y_pred = boosting.predict(X)\n",
    "    \n",
    "    return y_pred,y\n",
    "\n",
    "ensemble_adaB_y_pred, ensemble_adaB_y_test = adaboosting_ensemble(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def gradient_boosting_ensemble(df):\n",
    "    # Separate the features and target variable\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Create an instance of the Random Under-Sampler\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    \n",
    "    # Create an instance of the Gradient Boosting classifier\n",
    "    boosting = GradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    # Fit the Gradient Boosting classifier to the data after under-sampling\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    boosting.fit(X_res, y_res)\n",
    "    y_pred = boosting.predict(X)\n",
    "    \n",
    "    return y_pred,y\n",
    "\n",
    "ensemble_gB_y_pred, ensemble_gB_y_test = gradient_boosting_ensemble(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def xgb_ensemble(df):\n",
    "    # Split the data into features and target\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a DMatrix for the XGBoost model\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "    # Set the XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'scale_pos_weight': len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    # Return the predicted probabilities\n",
    "    return y_pred,y_test\n",
    "\n",
    "ensemble_xgB_y_pred, ensemble_xgB_y_test = xgb_ensemble(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing methods comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def applyLogisticRegression(df, method_name = \"\",class_weights = None):\n",
    "    # Split data into train and test sets\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(random_state=42)\n",
    "    if class_weights != None:\n",
    "        clf = LogisticRegression(random_state=42,class_weight={0:class_weights[0], 1:class_weights[1]})\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "    precision_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "    recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "    f1_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "    f1_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    matthews_corr = matthews_corrcoef(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    g_mean = (recall_0*recall_1)**0.5\n",
    "    classification_error = 1 - accuracy\n",
    "    sensitivity_0 = recall_0\n",
    "    sensitivity_1 = recall_1\n",
    "    specificity_0 = 1 - recall_0\n",
    "    specificity_1 = 1 - recall_1\n",
    "    \n",
    "    # Return dictionary with performance metrics\n",
    "    return {\n",
    "        'method_name': method_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_0': precision_0,\n",
    "        'precision_1': precision_1,\n",
    "        'recall_0': recall_0,\n",
    "        'recall_1': recall_1,\n",
    "        'f1_0': f1_0,\n",
    "        'f1_1': f1_1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cohen_kappa': cohen_kappa,\n",
    "        'matthews_corr': matthews_corr,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'g_mean': g_mean,\n",
    "        'classification_error': classification_error,\n",
    "        'sensitivity_0': sensitivity_0,\n",
    "        'sensitivity_1': sensitivity_1,\n",
    "        'specificity_0': specificity_0,\n",
    "        'specificity_1': specificity_1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "df_ans = applyLogisticRegression(df,method_name=\"df\")\n",
    "df_undersampling_ans = applyLogisticRegression(df_undersampling,method_name=\"under\")\n",
    "df_nearmiss_v1_ans = applyLogisticRegression(df_nearmiss_v1,method_name=\"NM1\")\n",
    "df_nearmiss_v3_ans = applyLogisticRegression(df_nearmiss_v3,method_name=\"NM2\")\n",
    "df_oversampling_ans = applyLogisticRegression(df_oversampling,method_name=\"over\")\n",
    "df_smote_ans = applyLogisticRegression(df_smote,method_name=\"smote\")\n",
    "df_weight_ans = applyLogisticRegression(df,method_name=\"weights\",class_weights=[class_weights[0],class_weights[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score, classification_report\n",
    "\n",
    "def getMetrics(y_pred,y_test,method_name=\"\"):\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "    precision_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "    recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "    f1_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "    f1_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    matthews_corr = matthews_corrcoef(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    g_mean = (recall_0*recall_1)**0.5\n",
    "    classification_error = 1 - accuracy\n",
    "    sensitivity_0 = recall_0\n",
    "    sensitivity_1 = recall_1\n",
    "    specificity_0 = 1 - recall_0\n",
    "    specificity_1 = 1 - recall_1\n",
    "    \n",
    "    # Return dictionary with performance metrics\n",
    "    return {\n",
    "        'method_name': method_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_0': precision_0,\n",
    "        'precision_1': precision_1,\n",
    "        'recall_0': recall_0,\n",
    "        'recall_1': recall_1,\n",
    "        'f1_0': f1_0,\n",
    "        'f1_1': f1_1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cohen_kappa': cohen_kappa,\n",
    "        'matthews_corr': matthews_corr,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'g_mean': g_mean,\n",
    "        'classification_error': classification_error,\n",
    "        'sensitivity_0': sensitivity_0,\n",
    "        'sensitivity_1': sensitivity_1,\n",
    "        'specificity_0': specificity_0,\n",
    "        'specificity_1': specificity_1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_e_rf_ans = getMetrics(ensemble_RF_y_pred,ensemble_RF_y_test,method_name=\"e_rf\")\n",
    "df_e_easy_ans = getMetrics(ensemble_easy_y_pred, ensemble_easy_y_test,method_name=\"e_easy\")\n",
    "df_e_bag_ans = getMetrics(ensemble_bag_y_pred, ensemble_bag_y_test,method_name=\"e_bag\")\n",
    "df_e_adab_ans = getMetrics(ensemble_adaB_y_pred, ensemble_adaB_y_test,method_name=\"e_adab\")\n",
    "df_e_gb_ans = getMetrics(ensemble_gB_y_pred, ensemble_gB_y_test,method_name=\"e_gb\")\n",
    "\n",
    "ensemble_xgB_y_pred_b = np.where(ensemble_xgB_y_pred > 0.5, 1, 0)\n",
    "\n",
    "df_e_xgb_ans = getMetrics(ensemble_xgB_y_pred_b, ensemble_xgB_y_test,method_name=\"e_xgb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== accuracy ======\n",
      "score: 0.9995259997893332 (e_xgb)\t0.9989817773252344 (df)\t0.9780377589033977 (e_rf)\t0.9739021859557181 (smote)\t0.9671356392223506 (e_easy)\t0.9637157794576678 (e_bag)\t0.9620690502691296 (e_gb)\t0.9562164249850778 (weights)\t0.949238578680203 (NM1)\t0.9289340101522843 (under)\t0.9178991611416915 (over)\t0.9035532994923858 (NM2)\t0.9010803807490687 (e_adab)\t\n",
      "improvement(from next/worst): 0.054478%, 10.925287% (e_xgb)\t2.141432%, 10.86489% (df)\t0.424639%, 8.540568% (e_rf)\t0.699648%, 8.08161% (smote)\t0.354862%, 7.330673% (e_easy)\t0.171165%, 6.951144% (e_bag)\t0.612061%, 6.768394% (e_gb)\t0.735099%, 6.118882% (weights)\t2.185792%, 5.344495% (NM1)\t1.202185%, 3.091137% (under)\t1.587716%, 1.866513% (over)\t0.274439%, 0.274439% (NM2)\t\n",
      "====== precision_0 ======\n",
      "score: 1.0 (e_rf)\t1.0 (e_adab)\t0.99999268781351 (e_gb)\t0.9999854525881664 (e_easy)\t0.9999854008737578 (e_bag)\t0.9998529033207075 (weights)\t0.999736226634076 (e_xgb)\t0.9994724439911371 (df)\t0.9661454721952573 (smote)\t0.9320388349514563 (NM1)\t0.9047619047619048 (under)\t0.8880903658836048 (over)\t0.8636363636363636 (NM2)\t\n",
      "improvement(from next/worst): 0.0%, 15.789474% (e_rf)\t0.000731%, 15.789474% (e_adab)\t0.000724%, 15.788627% (e_gb)\t5e-06%, 15.787789% (e_easy)\t0.013252%, 15.787783% (e_bag)\t0.011671%, 15.772441% (weights)\t0.026392%, 15.758932% (e_xgb)\t3.449478%, 15.728388% (df)\t3.659358%, 11.869476% (smote)\t3.014819%, 7.920286% (NM1)\t1.877235%, 4.761905% (under)\t2.831516%, 2.831516% (over)\t\n",
      "====== precision_1 ======\n",
      "score: 0.9818872907669796 (smote)\t0.9680851063829787 (NM1)\t0.9565217391304348 (under)\t0.9540229885057471 (NM2)\t0.9524895043787162 (over)\t0.8736842105263158 (e_xgb)\t0.7083333333333334 (df)\t0.07292129835482436 (e_rf)\t0.04957334416903698 (e_easy)\t0.045110001848770565 (e_bag)\t0.04339739615623062 (e_gb)\t0.03493788819875776 (weights)\t0.01716378859236002 (e_adab)\t\n",
      "improvement(from next/worst): 1.42572%, 5620.690892% (smote)\t1.208897%, 5540.276336% (NM1)\t0.261917%, 5472.90562% (under)\t0.160997%, 5458.347351% (NM2)\t9.019883%, 5449.412936% (over)\t23.343653%, 4990.275995% (e_xgb)\t871.36687%, 4026.905488% (df)\t47.097799%, 324.855491% (e_rf)\t9.894352%, 188.825185% (e_easy)\t3.946333%, 162.820773% (e_bag)\t24.212992%, 152.842756% (e_gb)\t103.555806%, 103.555806% (weights)\t\n",
      "====== recall_0 ======\n",
      "score: 0.9997889701744513 (e_xgb)\t0.9995075970737197 (df)\t0.9821145374449339 (smote)\t0.9779997537942071 (e_rf)\t0.9696969696969697 (NM1)\t0.9670928371700402 (e_easy)\t0.9636670594235267 (e_bag)\t0.9620104461600689 (e_gb)\t0.9595959595959596 (under)\t0.9595959595959596 (NM2)\t0.9562816544738323 (weights)\t0.9559295154185022 (over)\t0.900909202820815 (e_adab)\t\n",
      "improvement(from next/worst): 0.028151%, 10.975553% (e_xgb)\t1.770981%, 10.944321% (df)\t0.420735%, 9.013709% (smote)\t0.856225%, 8.556972% (e_rf)\t0.269274%, 7.635372% (NM1)\t0.355494%, 7.346316% (e_easy)\t0.172203%, 6.966058% (e_bag)\t0.251615%, 6.782176% (e_gb)\t0.0%, 6.51417% (under)\t0.346583%, 6.51417% (NM2)\t0.036837%, 6.146285% (weights)\t6.107198%, 6.107198% (over)\t\n",
      "====== recall_1 ======\n",
      "score: 1.0 (e_rf)\t1.0 (e_adab)\t0.9959349593495935 (e_gb)\t0.991869918699187 (e_easy)\t0.991869918699187 (e_bag)\t0.9657224094355518 (smote)\t0.9285714285714286 (NM1)\t0.9183673469387755 (weights)\t0.8979591836734694 (under)\t0.8800196573996069 (over)\t0.8469387755102041 (e_xgb)\t0.8469387755102041 (NM2)\t0.6938775510204082 (df)\t\n",
      "improvement(from next/worst): 0.0%, 44.117647% (e_rf)\t0.408163%, 44.117647% (e_adab)\t0.409836%, 43.531803% (e_gb)\t0.0%, 42.945959% (e_easy)\t2.70756%, 42.945959% (e_bag)\t4.000875%, 39.177641% (smote)\t1.111111%, 33.823529% (NM1)\t2.272727%, 32.352941% (weights)\t2.038537%, 29.411765% (under)\t3.905935%, 26.826362% (over)\t0.0%, 22.058824% (e_xgb)\t22.058824%, 22.058824% (NM2)\t\n",
      "====== f1_0 ======\n",
      "score: 0.9997625977086282 (e_xgb)\t0.999490020223336 (df)\t0.988877528339631 (e_rf)\t0.983264136976602 (e_easy)\t0.9814903708373933 (e_bag)\t0.9806339195921344 (e_gb)\t0.9775820224719101 (weights)\t0.9740645589751656 (smote)\t0.9504950495049505 (NM1)\t0.9478718936011561 (e_adab)\t0.9313725490196078 (under)\t0.9207620825730894 (over)\t0.9090909090909092 (NM2)\t\n",
      "improvement(from next/worst): 0.027272%, 9.973886% (e_xgb)\t1.073186%, 9.943902% (df)\t0.570894%, 8.776528% (e_rf)\t0.180722%, 8.159055% (e_easy)\t0.087336%, 7.963941% (e_bag)\t0.312188%, 7.869731% (e_gb)\t0.361112%, 7.534022% (weights)\t2.479709%, 7.147101% (smote)\t0.276742%, 4.554455% (NM1)\t1.771509%, 4.265908% (e_adab)\t1.152357%, 2.45098% (under)\t1.283829%, 1.283829% (over)\t\n",
      "====== f1_1 ======\n",
      "score: 0.9737377670023183 (smote)\t0.9479166666666666 (NM1)\t0.9263157894736843 (under)\t0.9148216061377341 (over)\t0.8972972972972973 (NM2)\t0.8601036269430051 (e_xgb)\t0.7010309278350516 (df)\t0.13593037712391212 (e_rf)\t0.09442724458204334 (e_easy)\t0.08629531388152077 (e_bag)\t0.08317066960875837 (e_gb)\t0.06731488406881077 (weights)\t0.03374832801728573 (e_adab)\t\n",
      "improvement(from next/worst): 2.723984%, 2785.291877% (smote)\t2.331913%, 2708.781123% (NM1)\t1.25644%, 2644.775353% (under)\t1.95301%, 2610.716826% (over)\t4.324324%, 2558.790376% (NM2)\t22.691253%, 2448.581448% (e_xgb)\t415.727936%, 1977.231582% (df)\t43.952498%, 302.776627% (e_rf)\t9.423375%, 179.79829% (e_easy)\t3.756906%, 155.702486% (e_bag)\t23.55465%, 146.443823% (e_gb)\t99.46139%, 99.46139% (weights)\t\n",
      "====== roc_auc ======\n",
      "score: 0.9889998768971036 (e_rf)\t0.9794813779346135 (e_easy)\t0.9789727027548313 (e_gb)\t0.9777684890613567 (e_bag)\t0.9739184734402427 (smote)\t0.9504546014104075 (e_adab)\t0.9491341991341993 (NM1)\t0.9373245007063039 (weights)\t0.9287775716347145 (under)\t0.9233638728423278 (e_xgb)\t0.9179745864090546 (over)\t0.9032673675530819 (NM2)\t0.8466925740470639 (df)\t\n",
      "improvement(from next/worst): 0.97179%, 16.807435% (e_rf)\t0.05196%, 15.683237% (e_easy)\t0.123159%, 15.623159% (e_gb)\t0.395312%, 15.480934% (e_bag)\t2.4687%, 15.026221% (smote)\t0.139116%, 12.254983% (e_adab)\t1.259937%, 12.099034% (NM1)\t0.920234%, 10.704231% (weights)\t0.586302%, 9.694782% (under)\t0.587084%, 9.055388% (e_xgb)\t1.628224%, 8.418878% (over)\t6.681858%, 6.681858% (NM2)\t\n",
      "====== cohen_kappa ======\n",
      "score: 0.9478058767657633 (smote)\t0.8984536082474227 (NM1)\t0.8598662821094326 (e_xgb)\t0.8578203938550366 (under)\t0.8358225465506952 (over)\t0.8069922136853503 (NM2)\t0.7005210023521068 (df)\t0.1331389514949628 (e_rf)\t0.09143760868556627 (e_easy)\t0.08326580491191837 (e_bag)\t0.08012523222007784 (e_gb)\t0.06421295167825636 (weights)\t0.030455102486689256 (e_adab)\t\n",
      "improvement(from next/worst): 5.493024%, 3012.141478% (smote)\t4.487596%, 2850.092217% (NM1)\t0.238498%, 2723.389882% (e_xgb)\t2.63188%, 2716.672163% (under)\t3.572566%, 2644.441746% (over)\t15.198861%, 2549.776713% (NM2)\t426.157818%, 2200.176145% (df)\t45.606336%, 337.164681% (e_rf)\t9.814117%, 200.237403% (e_easy)\t3.91958%, 173.405105% (e_bag)\t24.780485%, 163.092965% (e_gb)\t110.844642%, 110.844642% (weights)\t\n",
      "====== matthews_corr ======\n",
      "score: 0.9479348498651148 (smote)\t0.899195691173796 (NM1)\t0.8599705330950153 (e_xgb)\t0.8594173716150444 (under)\t0.8382613239517499 (over)\t0.8120779943989461 (NM2)\t0.7005583585483848 (df)\t0.26705245147231316 (e_rf)\t0.21800238603247138 (e_easy)\t0.20758209182082993 (e_bag)\t0.20387577488140046 (e_gb)\t0.17444119657023266 (weights)\t0.12435037232806369 (e_adab)\t\n",
      "improvement(from next/worst): 5.420306%, 662.309619% (smote)\t4.561221%, 623.114595% (NM1)\t0.064365%, 591.570533% (e_xgb)\t2.523801%, 591.125692% (under)\t3.224238%, 574.112436% (over)\t15.918679%, 553.056343% (NM2)\t162.329874%, 463.374556% (df)\t22.499784%, 114.758063% (e_rf)\t5.019843%, 75.313014% (e_easy)\t1.817929%, 66.933229% (e_bag)\t16.873639%, 63.952686% (e_gb)\t40.282006%, 40.282006% (weights)\t\n",
      "====== balanced_accuracy ======\n",
      "score: 0.9889998768971036 (e_rf)\t0.9794813779346137 (e_easy)\t0.9789727027548312 (e_gb)\t0.9777684890613568 (e_bag)\t0.9739184734402428 (smote)\t0.9504546014104075 (e_adab)\t0.9491341991341992 (NM1)\t0.9373245007063039 (weights)\t0.9287775716347144 (under)\t0.9233638728423277 (e_xgb)\t0.9179745864090545 (over)\t0.9032673675530818 (NM2)\t0.8466925740470639 (df)\t\n",
      "improvement(from next/worst): 0.97179%, 16.807435% (e_rf)\t0.05196%, 15.683237% (e_easy)\t0.123159%, 15.623159% (e_gb)\t0.395312%, 15.480934% (e_bag)\t2.4687%, 15.026221% (smote)\t0.139116%, 12.254983% (e_adab)\t1.259937%, 12.099034% (NM1)\t0.920234%, 10.704231% (weights)\t0.586302%, 9.694782% (under)\t0.587084%, 9.055388% (e_xgb)\t1.628224%, 8.418878% (over)\t6.681858%, 6.681858% (NM2)\t\n",
      "====== g_mean ======\n",
      "score: 0.9889387007262923 (e_rf)\t0.9794030292879504 (e_easy)\t0.9788257427092488 (e_gb)\t0.9776667979856419 (e_bag)\t0.973883985617899 (smote)\t0.9491623690501089 (e_adab)\t0.9489114291823555 (NM1)\t0.9371327792502812 (weights)\t0.9282661280770441 (under)\t0.9201956564601671 (e_xgb)\t0.9171896012585198 (over)\t0.9015093050017516 (NM2)\t0.8327880184559606 (df)\t\n",
      "improvement(from next/worst): 0.973621%, 18.750352% (e_rf)\t0.058977%, 17.605322% (e_easy)\t0.118542%, 17.536002% (e_gb)\t0.388425%, 17.396838% (e_bag)\t2.604572%, 16.942603% (smote)\t0.026445%, 13.974066% (e_adab)\t1.256882%, 13.943934% (NM1)\t0.955184%, 12.52957% (weights)\t0.877039%, 11.464876% (under)\t0.327746%, 10.495785% (e_xgb)\t1.739338%, 10.134822% (over)\t8.251954%, 8.251954% (NM2)\t\n",
      "====== classification_error ======\n",
      "score: 0.09891961925093129 (e_adab)\t0.09644670050761417 (NM2)\t0.08210083885830854 (over)\t0.07106598984771573 (under)\t0.050761421319796995 (NM1)\t0.04378357501492225 (weights)\t0.03793094973087041 (e_gb)\t0.03628422054233216 (e_bag)\t0.032864360777649426 (e_easy)\t0.02609781404428191 (smote)\t0.021962241096602275 (e_rf)\t0.0010182226747655898 (df)\t0.00047400021066679177 (e_xgb)\t\n",
      "improvement(from next/worst): 2.564026%, 20769.10871% (e_adab)\t17.473465%, 20247.396127% (NM2)\t15.527609%, 17220.844382% (over)\t40.0%, 14892.818199% (under)\t15.937132%, 10609.155856% (NM1)\t15.429683%, 9137.037037% (weights)\t4.538417%, 7902.306513% (e_gb)\t10.405983%, 7554.895446% (e_bag)\t25.927638%, 6833.406365% (e_easy)\t18.830378%, 5405.865495% (smote)\t2056.919271%, 4533.382138% (e_rf)\t114.814815%, 114.814815% (df)\t\n",
      "====== sensitivity_0 ======\n",
      "score: 0.9997889701744513 (e_xgb)\t0.9995075970737197 (df)\t0.9821145374449339 (smote)\t0.9779997537942071 (e_rf)\t0.9696969696969697 (NM1)\t0.9670928371700402 (e_easy)\t0.9636670594235267 (e_bag)\t0.9620104461600689 (e_gb)\t0.9595959595959596 (NM2)\t0.9595959595959596 (under)\t0.9562816544738323 (weights)\t0.9559295154185022 (over)\t0.900909202820815 (e_adab)\t\n",
      "improvement(from next/worst): 0.028151%, 10.975553% (e_xgb)\t1.770981%, 10.944321% (df)\t0.420735%, 9.013709% (smote)\t0.856225%, 8.556972% (e_rf)\t0.269274%, 7.635372% (NM1)\t0.355494%, 7.346316% (e_easy)\t0.172203%, 6.966058% (e_bag)\t0.251615%, 6.782176% (e_gb)\t0.0%, 6.51417% (NM2)\t0.346583%, 6.51417% (under)\t0.036837%, 6.146285% (weights)\t6.107198%, 6.107198% (over)\t\n",
      "====== sensitivity_1 ======\n",
      "score: 1.0 (e_rf)\t1.0 (e_adab)\t0.9959349593495935 (e_gb)\t0.991869918699187 (e_easy)\t0.991869918699187 (e_bag)\t0.9657224094355518 (smote)\t0.9285714285714286 (NM1)\t0.9183673469387755 (weights)\t0.8979591836734694 (under)\t0.8800196573996069 (over)\t0.8469387755102041 (e_xgb)\t0.8469387755102041 (NM2)\t0.6938775510204082 (df)\t\n",
      "improvement(from next/worst): 0.0%, 44.117647% (e_rf)\t0.408163%, 44.117647% (e_adab)\t0.409836%, 43.531803% (e_gb)\t0.0%, 42.945959% (e_easy)\t2.70756%, 42.945959% (e_bag)\t4.000875%, 39.177641% (smote)\t1.111111%, 33.823529% (NM1)\t2.272727%, 32.352941% (weights)\t2.038537%, 29.411765% (under)\t3.905935%, 26.826362% (over)\t0.0%, 22.058824% (e_xgb)\t22.058824%, 22.058824% (NM2)\t\n",
      "====== specificity_0 ======\n",
      "score: 0.09909079717918501 (e_adab)\t0.04407048458149776 (over)\t0.04371834552616771 (weights)\t0.04040404040404044 (under)\t0.04040404040404044 (NM2)\t0.03798955383993108 (e_gb)\t0.036332940576473294 (e_bag)\t0.03290716282995976 (e_easy)\t0.030303030303030276 (NM1)\t0.022000246205792884 (e_rf)\t0.017885462555066067 (smote)\t0.0004924029262802776 (df)\t0.00021102982554865868 (e_xgb)\t\n",
      "improvement(from next/worst): 124.846171%, 46855.825757% (e_adab)\t0.805472%, 20783.533627% (over)\t8.202905%, 20616.666667% (weights)\t0.0%, 19046.127946% (under)\t6.355659%, 19046.127946% (NM2)\t4.559535%, 17901.983246% (e_gb)\t10.410432%, 17116.969441% (e_bag)\t8.593637%, 15493.60756% (e_easy)\t37.739505%, 14259.59596% (NM1)\t23.006303%, 10325.183335% (e_rf)\t3532.281938%, 8375.324523% (smote)\t133.333333%, 133.333333% (df)\t\n",
      "====== specificity_1 ======\n",
      "score: 0.30612244897959184 (df)\t0.15306122448979587 (NM2)\t0.15306122448979587 (e_xgb)\t0.11998034260039314 (over)\t0.10204081632653061 (under)\t0.08163265306122447 (weights)\t0.0714285714285714 (NM1)\t0.034277590564448235 (smote)\t0.008130081300813052 (e_bag)\t0.008130081300813052 (e_easy)\t0.004065040650406471 (e_gb)\t0.0 (e_adab)\t0.0 (e_rf)\t\n",
      "improvement(from next/worst): 100.0%, inf% (df)\t0.0%, inf% (NM2)\t27.571918%, inf% (e_xgb)\t17.580736%, inf% (over)\t25.0%, inf% (under)\t14.285714%, inf% (weights)\t108.382708%, inf% (NM1)\t321.614364%, inf% (smote)\t0.0%, inf% (e_bag)\t100.0%, inf% (e_easy)\tinf%, inf% (e_gb)\tnan%, nan% (e_adab)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/4b_rm7gs2fj181km0x332skh0000gn/T/ipykernel_19004/3647650569.py:30: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  text = f\"{round(100*((ans[i][score_name]/ans[i+1][score_name])-1),6)}%, {round(100*((ans[i][score_name]/ans[n-1][score_name])-1),6)}% ({ans[i][name_str]})\"\n",
      "/var/folders/xs/4b_rm7gs2fj181km0x332skh0000gn/T/ipykernel_19004/3647650569.py:30: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  text = f\"{round(100*((ans[i][score_name]/ans[i+1][score_name])-1),6)}%, {round(100*((ans[i][score_name]/ans[n-1][score_name])-1),6)}% ({ans[i][name_str]})\"\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison\n",
    "\n",
    "ans = [df_ans,\n",
    "       df_undersampling_ans,\n",
    "       df_nearmiss_v1_ans,\n",
    "       df_nearmiss_v3_ans,\n",
    "       df_oversampling_ans,\n",
    "       df_smote_ans,\n",
    "       df_weight_ans,\n",
    "       df_e_rf_ans,\n",
    "       df_e_easy_ans,\n",
    "       df_e_bag_ans,\n",
    "       df_e_adab_ans,\n",
    "       df_e_gb_ans,\n",
    "       df_e_xgb_ans\n",
    "       ]\n",
    "\n",
    "n = len(ans)\n",
    "name_str = 'method_name'\n",
    "for score_name in [v for v in df_ans if v != name_str]:\n",
    "    print(f'====== {score_name} ======')\n",
    "    ans = sorted(ans, key = lambda x: x[score_name], reverse = True)\n",
    "    print('score:',end=\" \")\n",
    "    for i in range(n):\n",
    "        text = f\"{ans[i][score_name]} ({ans[i][name_str]})\"\n",
    "        print(text,end='\\t')\n",
    "    print()\n",
    "    print('improvement(from next/worst):',end=\" \")\n",
    "    for i in range(n-1):\n",
    "        text = f\"{round(100*((ans[i][score_name]/ans[i+1][score_name])-1),6)}%, {round(100*((ans[i][score_name]/ans[n-1][score_name])-1),6)}% ({ans[i][name_str]})\"\n",
    "        print(text,end='\\t')\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For accuracy, ensemble XGBoost > df > others.  \n",
    "For 1 precision, smote > NM1 > under > NM2 > over > ensemble XGBoost > df > others.  \n",
    "For 1 recall, ensemble > smote > others. The worst was df.  \n",
    "For 1 F1, smote > NM1 > under > over > ...  \n",
    "For ROC AUX, ensemble > smote > others. The worst was df.  \n",
    "For Cohen kappa and MCC, smote was the best.  \n",
    "For balanced accuracy and g-mean, ensemble > smote > others.  \n",
    "For classification error, ensemble adaBoost > NM2 > over > ...  \n",
    "For 1 sensitivity, ensemble adaBoost > ensemble > smote > ...  \n",
    "For 1 specificity, df >> (way better) others.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are in favor of balancing. Nevertheless, there isn't a best method over all metrics. The best performance methods seemed to be smote, ensemble adaBoost and ensemble random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results after normalisation\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"data/balancing.pk\",'wb') as f:\n",
    "    pickle.dump(df_smote,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
