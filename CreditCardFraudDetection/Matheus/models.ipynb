{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "df = None\n",
    "with open('data/feature.pk','rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_col = 'Class'\n",
    "test_size = 0.2\n",
    "X = df.drop(target_col, axis=1)\n",
    "y = df[target_col]\n",
    "    \n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score, classification_report\n",
    "\n",
    "def getMetrics(y_pred,y_test,method_name=\"\"):\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "    precision_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "    recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "    f1_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "    f1_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    matthews_corr = matthews_corrcoef(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    g_mean = (recall_0*recall_1)**0.5\n",
    "    classification_error = 1 - accuracy\n",
    "    sensitivity_0 = recall_0\n",
    "    sensitivity_1 = recall_1\n",
    "    specificity_0 = 1 - recall_0\n",
    "    specificity_1 = 1 - recall_1\n",
    "    \n",
    "    # Return dictionary with performance metrics\n",
    "    return {\n",
    "        'method_name': method_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_0': precision_0,\n",
    "        'precision_1': precision_1,\n",
    "        'recall_0': recall_0,\n",
    "        'recall_1': recall_1,\n",
    "        'f1_0': f1_0,\n",
    "        'f1_1': f1_1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cohen_kappa': cohen_kappa,\n",
    "        'matthews_corr': matthews_corr,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'g_mean': g_mean,\n",
    "        'classification_error': classification_error,\n",
    "        'sensitivity_0': sensitivity_0,\n",
    "        'sensitivity_1': sensitivity_1,\n",
    "        'specificity_0': specificity_0,\n",
    "        'specificity_1': specificity_1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareModels(ans):\n",
    "    n = len(ans)\n",
    "    name_str = 'method_name'\n",
    "    for score_name in [v for v in ans[0] if v != name_str]:\n",
    "        print(f'====== {score_name} ======')\n",
    "        ans = sorted(ans, key = lambda x: x[score_name], reverse = True)\n",
    "        print('score:',end=\" \")\n",
    "        for i in range(n):\n",
    "            text = f\"{ans[i][score_name]} ({ans[i][name_str]})\"\n",
    "            print(text,end='\\t')\n",
    "        print()\n",
    "        print('improvement(from next/worst):',end=\" \")\n",
    "        for i in range(n-1):\n",
    "            text = f\"{round(100*((ans[i][score_name]/ans[i+1][score_name])-1),6)}%, {round(100*((ans[i][score_name]/ans[n-1][score_name])-1),6)}% ({ans[i][name_str]})\"\n",
    "            print(text,end='\\t')\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Selection\n",
    "    - Linear models:\n",
    "        - linear regression, logistic regression, ridge regression, lasso regression\n",
    "    - Tree-based models:\n",
    "        - decision trees, random forest, gradient boosting trees, XGBosst, LightGBM, CatBoost\n",
    "    - Neural Networks:\n",
    "        - multi-layer perceptron, convolutional neural networks, recurrent neural networks, autoencoders, generative adversarial networks\n",
    "    - Supposert Vector Machine:\n",
    "        - Linear SVM, Non-Linear SVM\n",
    "    - Clusteering models:\n",
    "        - K-Means, Hierarchical, DBSCAN, Gaussian Mixture models\n",
    "    - Ensemble Models:\n",
    "        - Bagging, Boosting, Stacking\n",
    "    - Bayesian Models:\n",
    "        - Naive Bayes, Bayesian Networks, Gaussian Processes\n",
    "    - Evolutionary models:\n",
    "        - genetic algorithms, particle swarm optimization\n",
    "    - k-Nearest Neighbors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models\n",
    "- linear regression, logistic regression, ridge regression, lasso regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "The model tries to find the best fit line or hyperplane. If there is only one variable, it estimates $b_0$ and $b_1$ considering that\n",
    "$$y = b_0 + b_1 x_1$$\n",
    "in a way that it minimizes the sum of squared residuals.\n",
    "\n",
    "If there are more variables, it considers:\n",
    "$$y = b_0 + b_1 x_1 + ... + b_n x_n$$\n",
    "Note that it assumes that the variables are independent.\n",
    "\n",
    "Also, the output is not categorical, it's numeric. So it may need adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "def apply_fit_predict_model(modelGenerator, X_train, y_train, X_test):\n",
    "    model = modelGenerator()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "def apply_linear_regression(X, y, X_test):\n",
    "\n",
    "    return np.array(list(map(lambda num: 1 if num >= 0.5 else 0, apply_fit_predict_model(lambda : LinearRegression(),X,y,X_test))))\n",
    "\n",
    "\n",
    "lin_reg_y_pred = apply_linear_regression(X_train,y_train,X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "This model is used to predict the probability of a binary outcome. It uses the logistic mathematical function (or sigmoid function) to transform a linear combination of the independent vaiables into a probability between 0 and 1. The formula is:\n",
    "$$p = \\frac{1}{a+e^{-(b_0+b_1 x_1+...+b_n x_n)}}$$\n",
    "Once the probability is calculated, a threshold is used to classify it as 0 or 1 (e.g. 0.5 as we have done above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_test):\n",
    "    lr_y_pred = apply_fit_predict_model(lambda : LogisticRegression(), X_train, y_train, X_test)\n",
    "\n",
    "    return lr_y_pred\n",
    "\n",
    "lr_y_pred = logistic_regression(X_train, y_train, X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge is similar to linear regression but it tries to overcome overfitting by adding a cost function. The cost function to be minimizes is:\n",
    "$$J(w) = MSE(w) + alpha * sum(w^2)$$\n",
    "It tries to find coefficient with smaller values, trying to reduce theirvariance and making the model more stable. The alpha parameter may be tunned. It's useful in cases in which there may be many featues and some may be irrelevant or redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "def ridge_regression(X_train, y_train, X_test, alpha=1.0):\n",
    "    ridge_y_pred = apply_fit_predict_model(lambda: Ridge(alpha=alpha), X_train, y_train, X_test)\n",
    "\n",
    "    return np.array(list(map(lambda x: 1 if x >= 0.5 else 0,ridge_y_pred)))\n",
    "\n",
    "ridge_y_pred = ridge_regression(X_train, y_train, X_test, alpha=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (Least absolute shrinkage and selection operator) Regression\n",
    "\n",
    "Lasso is similar to ridge but the cost function is\n",
    "$$J(w) = MSE(w) + alpha * sum(|w|)$$\n",
    "The cost functino on Lasso tends to shrink some coefficients all the way to zero more effectively. It, as Ridge, tries to perform feature selection and produce sparse models, with only the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def lasso_regression(X_train, y_train, X_test, alpha=1.0):\n",
    "    lasso_y_pred = apply_fit_predict_model(lambda: Lasso(alpha=alpha), X_train, y_train, X_test)\n",
    "\n",
    "    return np.array(list(map(lambda x: 1 if x >= 0.5 else 0,lasso_y_pred)))\n",
    "\n",
    "lasso_y_pred = lasso_regression(X_train, y_train, X_test, alpha=1.0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== accuracy ======\n",
      "score: 0.9739021859557181 (logistic)\t0.9337794347818441 (linear)\t0.9337794347818441 (ridge)\t0.9033114679141093 (lasso)\t\n",
      "improvement(from next/worst): 4.296812%, 7.81466% (logistic)\t0.0%, 3.372919% (linear)\t3.372919%, 3.372919% (ridge)\t\n",
      "====== precision_0 ======\n",
      "score: 0.9661454721952573 (logistic)\t0.8905711882429495 (linear)\t0.8905711882429495 (ridge)\t0.8424905682975028 (lasso)\t\n",
      "improvement(from next/worst): 8.486046%, 14.677304% (logistic)\t0.0%, 5.706962% (linear)\t5.706962%, 5.706962% (ridge)\t\n",
      "====== precision_1 ======\n",
      "score: 0.9898785425101214 (lasso)\t0.9874598260938147 (linear)\t0.9874598260938147 (ridge)\t0.9818872907669796 (logistic)\t\n",
      "improvement(from next/worst): 0.244943%, 0.813867% (lasso)\t0.0%, 0.567533% (linear)\t0.567533%, 0.567533% (ridge)\t\n",
      "====== recall_0 ======\n",
      "score: 0.9916299559471365 (lasso)\t0.9887929515418502 (linear)\t0.9887929515418502 (ridge)\t0.9821145374449339 (logistic)\t\n",
      "improvement(from next/worst): 0.286916%, 0.968871% (lasso)\t0.0%, 0.680004% (linear)\t0.680004%, 0.680004% (ridge)\t\n",
      "====== recall_1 ======\n",
      "score: 0.9657224094355518 (logistic)\t0.8789841336703174 (linear)\t0.8789841336703174 (ridge)\t0.8153433024431339 (lasso)\t\n",
      "improvement(from next/worst): 9.868014%, 18.443655% (logistic)\t0.0%, 7.805403% (linear)\t7.805403%, 7.805403% (ridge)\t\n",
      "====== f1_0 ======\n",
      "score: 0.9740645589751656 (logistic)\t0.9371153733748613 (linear)\t0.9371153733748613 (ridge)\t0.9109967137746264 (lasso)\t\n",
      "improvement(from next/worst): 3.942864%, 6.92295% (logistic)\t0.0%, 2.867042% (linear)\t2.867042%, 2.867042% (ridge)\t\n",
      "====== f1_1 ======\n",
      "score: 0.9737377670023183 (logistic)\t0.9300697352659876 (linear)\t0.9300697352659876 (ridge)\t0.8941735799665081 (lasso)\t\n",
      "improvement(from next/worst): 4.695135%, 8.89807% (logistic)\t0.0%, 4.01445% (linear)\t4.01445%, 4.01445% (ridge)\t\n",
      "====== roc_auc ======\n",
      "score: 0.9739184734402427 (logistic)\t0.9338885426060839 (linear)\t0.9338885426060839 (ridge)\t0.9034866291951353 (lasso)\t\n",
      "improvement(from next/worst): 4.286371%, 7.79556% (logistic)\t0.0%, 3.364954% (linear)\t3.364954%, 3.364954% (ridge)\t\n",
      "====== cohen_kappa ======\n",
      "score: 0.9478058767657633 (logistic)\t0.8675873103153087 (linear)\t0.8675873103153087 (ridge)\t0.8066900406258889 (lasso)\t\n",
      "improvement(from next/worst): 9.246166%, 17.493192% (logistic)\t0.0%, 7.54903% (linear)\t7.54903%, 7.54903% (ridge)\t\n",
      "====== matthews_corr ======\n",
      "score: 0.9479348498651148 (logistic)\t0.8728889931411897 (linear)\t0.8728889931411897 (ridge)\t0.8195728238124057 (lasso)\t\n",
      "improvement(from next/worst): 8.597411%, 15.662065% (logistic)\t0.0%, 6.505361% (linear)\t6.505361%, 6.505361% (ridge)\t\n",
      "====== balanced_accuracy ======\n",
      "score: 0.9739184734402428 (logistic)\t0.9338885426060838 (linear)\t0.9338885426060838 (ridge)\t0.9034866291951352 (lasso)\t\n",
      "improvement(from next/worst): 4.286371%, 7.79556% (logistic)\t0.0%, 3.364954% (linear)\t3.364954%, 3.364954% (ridge)\t\n",
      "====== g_mean ======\n",
      "score: 0.973883985617899 (logistic)\t0.9322731980971722 (linear)\t0.9322731980971722 (ridge)\t0.8991767585316458 (lasso)\t\n",
      "improvement(from next/worst): 4.463368%, 8.308403% (logistic)\t0.0%, 3.680749% (linear)\t3.680749%, 3.680749% (ridge)\t\n",
      "====== classification_error ======\n",
      "score: 0.09668853208589068 (lasso)\t0.06622056521815589 (linear)\t0.06622056521815589 (ridge)\t0.02609781404428191 (logistic)\t\n",
      "improvement(from next/worst): 46.009826%, 270.485175% (lasso)\t0.0%, 153.739892% (linear)\t153.739892%, 153.739892% (ridge)\t\n",
      "====== sensitivity_0 ======\n",
      "score: 0.9916299559471365 (lasso)\t0.9887929515418502 (linear)\t0.9887929515418502 (ridge)\t0.9821145374449339 (logistic)\t\n",
      "improvement(from next/worst): 0.286916%, 0.968871% (lasso)\t0.0%, 0.680004% (linear)\t0.680004%, 0.680004% (ridge)\t\n",
      "====== sensitivity_1 ======\n",
      "score: 0.9657224094355518 (logistic)\t0.8789841336703174 (linear)\t0.8789841336703174 (ridge)\t0.8153433024431339 (lasso)\t\n",
      "improvement(from next/worst): 9.868014%, 18.443655% (logistic)\t0.0%, 7.805403% (linear)\t7.805403%, 7.805403% (ridge)\t\n",
      "====== specificity_0 ======\n",
      "score: 0.017885462555066067 (logistic)\t0.011207048458149793 (linear)\t0.011207048458149793 (ridge)\t0.008370044052863457 (lasso)\t\n",
      "improvement(from next/worst): 59.591195%, 113.684211% (logistic)\t0.0%, 33.894737% (linear)\t33.894737%, 33.894737% (ridge)\t\n",
      "====== specificity_1 ======\n",
      "score: 0.1846566975568661 (lasso)\t0.12101586632968264 (linear)\t0.12101586632968264 (ridge)\t0.034277590564448235 (logistic)\t\n",
      "improvement(from next/worst): 52.588832%, 438.709677% (lasso)\t0.0%, 253.046595% (linear)\t253.046595%, 253.046595% (ridge)\t\n"
     ]
    }
   ],
   "source": [
    "ans = {'linear':lin_reg_y_pred, 'logistic':lr_y_pred, 'ridge':ridge_y_pred, 'lasso':lasso_y_pred}\n",
    "\n",
    "lst = []\n",
    "for model_name in ans:\n",
    "    lst += [getMetrics(ans[model_name],y_test,method_name=model_name)]\n",
    "\n",
    "compareModels(lst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic model seemed to be the better even though lasso had a better precision for 1 (while logistic got the worst precision). But lasso also got the highest classification error (due to its poor performance on 0)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based models\n",
    "- decision trees, random forest, gradient boosting trees, XGBoost, LightGBM, CatBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "\n",
    "This method tries to subdivide the feature space into numerous regions and assign a classification to each region. It does that by recursively selecting a features and a split value. The feature and split value selection are based on some impurity measure, like Gini impurity of entropy. The goal is that after splitting the new regions are more \"pure\" (in terms of the target variable).\n",
    "\n",
    "This process can be visualized as a tree where each node is composed by a selected feature and a split value and the leafs are the classification. It's easy to interpret and handle both numeric and categorical variables. However, it has some problems as overfitting and is sensitive to small changes in data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def apply_decision_tree(X_train, y_train, X_test):\n",
    "    return apply_fit_predict_model(lambda: DecisionTreeClassifier(), X_train, y_train, X_test)\n",
    "\n",
    "dt_y_pred = apply_decision_tree(X_train, y_train, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "This is an ensemble method, that is, produces many models and combines them to form a single one. In this case, it generates random subsets of the training set and trains a decision tree for each. Then, the outputs are combined to make a final prediction.\n",
    "\n",
    "It's an ensemble method that uses bagging to create the random subsets of data. Bagging creates data with replacement which means that some points may be selected multiple times and some may not be selected at all.\n",
    "\n",
    "It has the advantages of decision tree: handles numerical and categorical data, as well as missing values and noisy data. But, also, it has a low risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def apply_random_forest(X_train, y_train, X_test):\n",
    "    return apply_fit_predict_model(lambda: RandomForestClassifier(), X_train, y_train, X_test)\n",
    "random_forest_y_pred = apply_random_forest(X_train, y_train, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting trees\n",
    "\n",
    "This method builds a series of trees in which each subsequent tree correct the errors of the previous tree. The algorithm is:\n",
    "1. builds a decision tree\n",
    "2. the error is calculated for each observation\n",
    "3. a second tree is built to predict the errors\n",
    "4. performs 2 and 3 repeatedly\n",
    "5. the final prediction is made by combining the predictions of all trees\n",
    "\n",
    "The algorithm performs gradient descent optimization to minimize the loss function (error) and update the predictions.\n",
    "\n",
    "A problem is that it's expensive computationally and may require hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def apply_gradient_boosting(X_train, y_train, X_test):\n",
    "    return apply_fit_predict_model(lambda: GradientBoostingClassifier(), X_train, y_train, X_test)\n",
    "gb_y_pred = apply_gradient_boosting(X_train, y_train, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "It's a popular ensemble method. It's based in gradient boosting but its' designed to be more efficient.\n",
    "\n",
    "For example, it uses a technique called \"regularization\" to prevent ovverfitting by adding a penalty to the loss function that encourages simpler trees.\n",
    "It uses \"shrinkage\" to reduce the contribution of each individual tree making the model more robust to outliers and reducing overfitting again.\n",
    "\n",
    "\n",
    "XGBoost is also good for handling missing data since it can learn how to impute missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def apply_xgboost(X_train, y_train, X_test):\n",
    "    return apply_fit_predict_model(lambda: XGBClassifier(), X_train, y_train, X_test)\n",
    "    \n",
    "xgb_y_pred = apply_xgboost(X_train, y_train, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "This model is also based on gradient boosting and was developed by Microsoft. It was built to be fast, scalable and handle large datasets.\n",
    "It follows the approach of gradient boosting but uses gradient-based one-sided sampling to build the trees more efficiently.\n",
    "\n",
    "Instead of selecting a random subset of the data, each sample is assigned a weight (an importance, based on its gradient value) and LightGBM focus on samples that are more informative. So it can build more accurate trees with less data.\n",
    "\n",
    "LightGBM applies a gradient-based exclusive feature bundling technique to group categorical features together based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def apply_lightgbm(X_train, y_train, X_test):\n",
    "    return apply_fit_predict_model(lambda: LGBMClassifier(), X_train, y_train, X_test)\n",
    "lgbm_y_pred = apply_lightgbm(X_train, y_train, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost (Categorical Boosting)\n",
    "\n",
    "This method comes with an open-source library released by a Russian internet company in 2017.\n",
    "\n",
    "It's very effective on categorical data since it can handle them without preprocessing. It also can handle missing values without the need for imputation. It includes support for text features, useful for natural language processing. it's fast with parallel computation and GPU acceleration. It prevents overfitting by applying ordered boosting, random permutation and gradient-based feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.140561\n",
      "0:\tlearn: 0.4161384\ttotal: 478ms\tremaining: 7m 57s\n",
      "1:\tlearn: 0.2570333\ttotal: 1.03s\tremaining: 8m 33s\n",
      "2:\tlearn: 0.1801662\ttotal: 1.39s\tremaining: 7m 40s\n",
      "3:\tlearn: 0.1421400\ttotal: 1.68s\tremaining: 6m 57s\n",
      "4:\tlearn: 0.1158392\ttotal: 1.98s\tremaining: 6m 34s\n",
      "5:\tlearn: 0.1033104\ttotal: 2.27s\tremaining: 6m 15s\n",
      "6:\tlearn: 0.0929931\ttotal: 2.44s\tremaining: 5m 46s\n",
      "7:\tlearn: 0.0851909\ttotal: 2.62s\tremaining: 5m 24s\n",
      "8:\tlearn: 0.0792788\ttotal: 2.74s\tremaining: 5m 1s\n",
      "9:\tlearn: 0.0730083\ttotal: 2.91s\tremaining: 4m 48s\n",
      "10:\tlearn: 0.0696011\ttotal: 3.04s\tremaining: 4m 33s\n",
      "11:\tlearn: 0.0669351\ttotal: 3.33s\tremaining: 4m 33s\n",
      "12:\tlearn: 0.0644683\ttotal: 3.46s\tremaining: 4m 22s\n",
      "13:\tlearn: 0.0613024\ttotal: 3.61s\tremaining: 4m 14s\n",
      "14:\tlearn: 0.0585287\ttotal: 3.75s\tremaining: 4m 5s\n",
      "15:\tlearn: 0.0567194\ttotal: 3.88s\tremaining: 3m 58s\n",
      "16:\tlearn: 0.0546738\ttotal: 4.01s\tremaining: 3m 52s\n",
      "17:\tlearn: 0.0527617\ttotal: 4.18s\tremaining: 3m 47s\n",
      "18:\tlearn: 0.0509587\ttotal: 4.32s\tremaining: 3m 43s\n",
      "19:\tlearn: 0.0488968\ttotal: 4.49s\tremaining: 3m 40s\n",
      "20:\tlearn: 0.0475945\ttotal: 4.65s\tremaining: 3m 36s\n",
      "21:\tlearn: 0.0461876\ttotal: 4.79s\tremaining: 3m 32s\n",
      "22:\tlearn: 0.0449147\ttotal: 4.91s\tremaining: 3m 28s\n",
      "23:\tlearn: 0.0438369\ttotal: 5.07s\tremaining: 3m 26s\n",
      "24:\tlearn: 0.0424281\ttotal: 5.22s\tremaining: 3m 23s\n",
      "25:\tlearn: 0.0410726\ttotal: 5.38s\tremaining: 3m 21s\n",
      "26:\tlearn: 0.0396088\ttotal: 5.53s\tremaining: 3m 19s\n",
      "27:\tlearn: 0.0385245\ttotal: 5.78s\tremaining: 3m 20s\n",
      "28:\tlearn: 0.0376813\ttotal: 5.99s\tremaining: 3m 20s\n",
      "29:\tlearn: 0.0369135\ttotal: 6.2s\tremaining: 3m 20s\n",
      "30:\tlearn: 0.0358535\ttotal: 6.47s\tremaining: 3m 22s\n",
      "31:\tlearn: 0.0346515\ttotal: 6.74s\tremaining: 3m 23s\n",
      "32:\tlearn: 0.0334845\ttotal: 6.99s\tremaining: 3m 24s\n",
      "33:\tlearn: 0.0323217\ttotal: 7.31s\tremaining: 3m 27s\n",
      "34:\tlearn: 0.0313691\ttotal: 7.54s\tremaining: 3m 27s\n",
      "35:\tlearn: 0.0304896\ttotal: 7.73s\tremaining: 3m 27s\n",
      "36:\tlearn: 0.0298005\ttotal: 7.92s\tremaining: 3m 26s\n",
      "37:\tlearn: 0.0291159\ttotal: 8.09s\tremaining: 3m 24s\n",
      "38:\tlearn: 0.0283260\ttotal: 8.25s\tremaining: 3m 23s\n",
      "39:\tlearn: 0.0279857\ttotal: 8.42s\tremaining: 3m 22s\n",
      "40:\tlearn: 0.0273272\ttotal: 8.65s\tremaining: 3m 22s\n",
      "41:\tlearn: 0.0265822\ttotal: 8.8s\tremaining: 3m 20s\n",
      "42:\tlearn: 0.0257724\ttotal: 9.02s\tremaining: 3m 20s\n",
      "43:\tlearn: 0.0250198\ttotal: 9.15s\tremaining: 3m 18s\n",
      "44:\tlearn: 0.0241822\ttotal: 9.36s\tremaining: 3m 18s\n",
      "45:\tlearn: 0.0236002\ttotal: 9.5s\tremaining: 3m 17s\n",
      "46:\tlearn: 0.0231705\ttotal: 9.69s\tremaining: 3m 16s\n",
      "47:\tlearn: 0.0226040\ttotal: 9.83s\tremaining: 3m 14s\n",
      "48:\tlearn: 0.0221911\ttotal: 10.1s\tremaining: 3m 15s\n",
      "49:\tlearn: 0.0216668\ttotal: 10.3s\tremaining: 3m 15s\n",
      "50:\tlearn: 0.0212400\ttotal: 10.6s\tremaining: 3m 16s\n",
      "51:\tlearn: 0.0208421\ttotal: 10.7s\tremaining: 3m 15s\n",
      "52:\tlearn: 0.0201830\ttotal: 10.9s\tremaining: 3m 14s\n",
      "53:\tlearn: 0.0197746\ttotal: 11s\tremaining: 3m 13s\n",
      "54:\tlearn: 0.0190604\ttotal: 11.2s\tremaining: 3m 12s\n",
      "55:\tlearn: 0.0186015\ttotal: 11.4s\tremaining: 3m 11s\n",
      "56:\tlearn: 0.0181703\ttotal: 11.5s\tremaining: 3m 10s\n",
      "57:\tlearn: 0.0177696\ttotal: 11.7s\tremaining: 3m 9s\n",
      "58:\tlearn: 0.0174571\ttotal: 11.8s\tremaining: 3m 8s\n",
      "59:\tlearn: 0.0170164\ttotal: 12.1s\tremaining: 3m 9s\n",
      "60:\tlearn: 0.0167870\ttotal: 12.3s\tremaining: 3m 8s\n",
      "61:\tlearn: 0.0165369\ttotal: 12.5s\tremaining: 3m 8s\n",
      "62:\tlearn: 0.0162244\ttotal: 12.7s\tremaining: 3m 8s\n",
      "63:\tlearn: 0.0160810\ttotal: 12.9s\tremaining: 3m 8s\n",
      "64:\tlearn: 0.0157267\ttotal: 13s\tremaining: 3m 7s\n",
      "65:\tlearn: 0.0152521\ttotal: 13.3s\tremaining: 3m 7s\n",
      "66:\tlearn: 0.0149899\ttotal: 13.5s\tremaining: 3m 7s\n",
      "67:\tlearn: 0.0146559\ttotal: 13.7s\tremaining: 3m 7s\n",
      "68:\tlearn: 0.0143590\ttotal: 13.9s\tremaining: 3m 7s\n",
      "69:\tlearn: 0.0140305\ttotal: 14s\tremaining: 3m 6s\n",
      "70:\tlearn: 0.0138067\ttotal: 14.1s\tremaining: 3m 5s\n",
      "71:\tlearn: 0.0134806\ttotal: 14.3s\tremaining: 3m 3s\n",
      "72:\tlearn: 0.0132369\ttotal: 14.6s\tremaining: 3m 5s\n",
      "73:\tlearn: 0.0129994\ttotal: 14.8s\tremaining: 3m 4s\n",
      "74:\tlearn: 0.0127403\ttotal: 14.9s\tremaining: 3m 3s\n",
      "75:\tlearn: 0.0124237\ttotal: 15.1s\tremaining: 3m 3s\n",
      "76:\tlearn: 0.0122787\ttotal: 15.2s\tremaining: 3m 2s\n",
      "77:\tlearn: 0.0119930\ttotal: 15.4s\tremaining: 3m 1s\n",
      "78:\tlearn: 0.0116613\ttotal: 15.5s\tremaining: 3m\n",
      "79:\tlearn: 0.0115770\ttotal: 15.6s\tremaining: 2m 59s\n",
      "80:\tlearn: 0.0114279\ttotal: 15.8s\tremaining: 2m 59s\n",
      "81:\tlearn: 0.0111520\ttotal: 16.1s\tremaining: 2m 59s\n",
      "82:\tlearn: 0.0110140\ttotal: 16.3s\tremaining: 2m 59s\n",
      "83:\tlearn: 0.0108303\ttotal: 16.4s\tremaining: 2m 59s\n",
      "84:\tlearn: 0.0106599\ttotal: 16.6s\tremaining: 2m 58s\n",
      "85:\tlearn: 0.0104922\ttotal: 16.8s\tremaining: 2m 58s\n",
      "86:\tlearn: 0.0102177\ttotal: 17s\tremaining: 2m 58s\n",
      "87:\tlearn: 0.0100799\ttotal: 17.4s\tremaining: 3m\n",
      "88:\tlearn: 0.0099833\ttotal: 17.6s\tremaining: 3m\n",
      "89:\tlearn: 0.0098010\ttotal: 17.9s\tremaining: 3m 1s\n",
      "90:\tlearn: 0.0094949\ttotal: 18.2s\tremaining: 3m 1s\n",
      "91:\tlearn: 0.0094008\ttotal: 18.4s\tremaining: 3m 1s\n",
      "92:\tlearn: 0.0092404\ttotal: 18.7s\tremaining: 3m 2s\n",
      "93:\tlearn: 0.0092058\ttotal: 18.9s\tremaining: 3m 2s\n",
      "94:\tlearn: 0.0090573\ttotal: 19.1s\tremaining: 3m 1s\n",
      "95:\tlearn: 0.0088985\ttotal: 19.3s\tremaining: 3m 1s\n",
      "96:\tlearn: 0.0088120\ttotal: 19.5s\tremaining: 3m 1s\n",
      "97:\tlearn: 0.0086441\ttotal: 19.6s\tremaining: 3m\n",
      "98:\tlearn: 0.0084876\ttotal: 19.8s\tremaining: 2m 59s\n",
      "99:\tlearn: 0.0083272\ttotal: 19.9s\tremaining: 2m 59s\n",
      "100:\tlearn: 0.0081879\ttotal: 20s\tremaining: 2m 58s\n",
      "101:\tlearn: 0.0079911\ttotal: 20.2s\tremaining: 2m 57s\n",
      "102:\tlearn: 0.0077531\ttotal: 20.3s\tremaining: 2m 56s\n",
      "103:\tlearn: 0.0076882\ttotal: 20.4s\tremaining: 2m 56s\n",
      "104:\tlearn: 0.0076366\ttotal: 20.6s\tremaining: 2m 55s\n",
      "105:\tlearn: 0.0075563\ttotal: 20.8s\tremaining: 2m 55s\n",
      "106:\tlearn: 0.0074557\ttotal: 20.9s\tremaining: 2m 54s\n",
      "107:\tlearn: 0.0073446\ttotal: 21.1s\tremaining: 2m 54s\n",
      "108:\tlearn: 0.0072091\ttotal: 21.2s\tremaining: 2m 53s\n",
      "109:\tlearn: 0.0071574\ttotal: 21.4s\tremaining: 2m 53s\n",
      "110:\tlearn: 0.0069909\ttotal: 21.5s\tremaining: 2m 52s\n",
      "111:\tlearn: 0.0069325\ttotal: 21.8s\tremaining: 2m 52s\n",
      "112:\tlearn: 0.0069004\ttotal: 21.9s\tremaining: 2m 51s\n",
      "113:\tlearn: 0.0067557\ttotal: 22.2s\tremaining: 2m 52s\n",
      "114:\tlearn: 0.0066375\ttotal: 22.4s\tremaining: 2m 52s\n",
      "115:\tlearn: 0.0065234\ttotal: 22.5s\tremaining: 2m 51s\n",
      "116:\tlearn: 0.0064818\ttotal: 22.7s\tremaining: 2m 51s\n",
      "117:\tlearn: 0.0064539\ttotal: 22.8s\tremaining: 2m 50s\n",
      "118:\tlearn: 0.0063114\ttotal: 23s\tremaining: 2m 50s\n",
      "119:\tlearn: 0.0062499\ttotal: 23.1s\tremaining: 2m 49s\n",
      "120:\tlearn: 0.0062333\ttotal: 23.3s\tremaining: 2m 48s\n",
      "121:\tlearn: 0.0061267\ttotal: 23.4s\tremaining: 2m 48s\n",
      "122:\tlearn: 0.0060155\ttotal: 23.5s\tremaining: 2m 47s\n",
      "123:\tlearn: 0.0059740\ttotal: 23.7s\tremaining: 2m 47s\n",
      "124:\tlearn: 0.0058986\ttotal: 23.8s\tremaining: 2m 46s\n",
      "125:\tlearn: 0.0057521\ttotal: 24s\tremaining: 2m 46s\n",
      "126:\tlearn: 0.0057188\ttotal: 24.1s\tremaining: 2m 45s\n",
      "127:\tlearn: 0.0056413\ttotal: 24.2s\tremaining: 2m 45s\n",
      "128:\tlearn: 0.0055992\ttotal: 24.4s\tremaining: 2m 44s\n",
      "129:\tlearn: 0.0055510\ttotal: 24.5s\tremaining: 2m 44s\n",
      "130:\tlearn: 0.0054857\ttotal: 24.6s\tremaining: 2m 43s\n",
      "131:\tlearn: 0.0054635\ttotal: 24.8s\tremaining: 2m 42s\n",
      "132:\tlearn: 0.0053868\ttotal: 24.9s\tremaining: 2m 42s\n",
      "133:\tlearn: 0.0053274\ttotal: 25.1s\tremaining: 2m 42s\n",
      "134:\tlearn: 0.0052871\ttotal: 25.2s\tremaining: 2m 41s\n",
      "135:\tlearn: 0.0052574\ttotal: 25.5s\tremaining: 2m 41s\n",
      "136:\tlearn: 0.0052277\ttotal: 25.6s\tremaining: 2m 41s\n",
      "137:\tlearn: 0.0051870\ttotal: 25.9s\tremaining: 2m 41s\n",
      "138:\tlearn: 0.0051576\ttotal: 26.1s\tremaining: 2m 41s\n",
      "139:\tlearn: 0.0050582\ttotal: 26.2s\tremaining: 2m 41s\n",
      "140:\tlearn: 0.0050481\ttotal: 26.4s\tremaining: 2m 40s\n",
      "141:\tlearn: 0.0049971\ttotal: 26.5s\tremaining: 2m 40s\n",
      "142:\tlearn: 0.0049447\ttotal: 26.7s\tremaining: 2m 39s\n",
      "143:\tlearn: 0.0049259\ttotal: 26.8s\tremaining: 2m 39s\n",
      "144:\tlearn: 0.0048815\ttotal: 27s\tremaining: 2m 38s\n",
      "145:\tlearn: 0.0048493\ttotal: 27.2s\tremaining: 2m 39s\n",
      "146:\tlearn: 0.0047620\ttotal: 27.8s\tremaining: 2m 41s\n",
      "147:\tlearn: 0.0047090\ttotal: 28s\tremaining: 2m 41s\n",
      "148:\tlearn: 0.0046604\ttotal: 28.1s\tremaining: 2m 40s\n",
      "149:\tlearn: 0.0046413\ttotal: 28.3s\tremaining: 2m 40s\n",
      "150:\tlearn: 0.0045903\ttotal: 28.4s\tremaining: 2m 39s\n",
      "151:\tlearn: 0.0045700\ttotal: 28.6s\tremaining: 2m 39s\n",
      "152:\tlearn: 0.0045163\ttotal: 28.7s\tremaining: 2m 39s\n",
      "153:\tlearn: 0.0045163\ttotal: 28.8s\tremaining: 2m 38s\n",
      "154:\tlearn: 0.0045163\ttotal: 28.9s\tremaining: 2m 37s\n",
      "155:\tlearn: 0.0045163\ttotal: 29s\tremaining: 2m 37s\n",
      "156:\tlearn: 0.0045056\ttotal: 29.2s\tremaining: 2m 36s\n",
      "157:\tlearn: 0.0045055\ttotal: 29.3s\tremaining: 2m 36s\n",
      "158:\tlearn: 0.0044721\ttotal: 29.4s\tremaining: 2m 35s\n",
      "159:\tlearn: 0.0044244\ttotal: 29.6s\tremaining: 2m 35s\n",
      "160:\tlearn: 0.0043666\ttotal: 29.7s\tremaining: 2m 34s\n",
      "161:\tlearn: 0.0042877\ttotal: 29.9s\tremaining: 2m 34s\n",
      "162:\tlearn: 0.0042500\ttotal: 30.1s\tremaining: 2m 34s\n",
      "163:\tlearn: 0.0041601\ttotal: 30.3s\tremaining: 2m 34s\n",
      "164:\tlearn: 0.0041311\ttotal: 30.6s\tremaining: 2m 34s\n",
      "165:\tlearn: 0.0040663\ttotal: 30.9s\tremaining: 2m 35s\n",
      "166:\tlearn: 0.0040527\ttotal: 31s\tremaining: 2m 34s\n",
      "167:\tlearn: 0.0039923\ttotal: 31.2s\tremaining: 2m 34s\n",
      "168:\tlearn: 0.0039520\ttotal: 31.3s\tremaining: 2m 33s\n",
      "169:\tlearn: 0.0039363\ttotal: 31.5s\tremaining: 2m 33s\n",
      "170:\tlearn: 0.0039262\ttotal: 31.6s\tremaining: 2m 33s\n",
      "171:\tlearn: 0.0039261\ttotal: 31.7s\tremaining: 2m 32s\n",
      "172:\tlearn: 0.0039061\ttotal: 31.8s\tremaining: 2m 32s\n",
      "173:\tlearn: 0.0038732\ttotal: 32s\tremaining: 2m 31s\n",
      "174:\tlearn: 0.0038732\ttotal: 32.1s\tremaining: 2m 31s\n",
      "175:\tlearn: 0.0038404\ttotal: 32.2s\tremaining: 2m 30s\n",
      "176:\tlearn: 0.0038149\ttotal: 32.4s\tremaining: 2m 30s\n",
      "177:\tlearn: 0.0037897\ttotal: 32.5s\tremaining: 2m 30s\n",
      "178:\tlearn: 0.0037897\ttotal: 32.6s\tremaining: 2m 29s\n",
      "179:\tlearn: 0.0037673\ttotal: 32.8s\tremaining: 2m 29s\n",
      "180:\tlearn: 0.0037408\ttotal: 32.9s\tremaining: 2m 28s\n",
      "181:\tlearn: 0.0036983\ttotal: 33.1s\tremaining: 2m 28s\n",
      "182:\tlearn: 0.0036146\ttotal: 33.2s\tremaining: 2m 28s\n",
      "183:\tlearn: 0.0035883\ttotal: 33.4s\tremaining: 2m 27s\n",
      "184:\tlearn: 0.0035592\ttotal: 33.5s\tremaining: 2m 27s\n",
      "185:\tlearn: 0.0035385\ttotal: 33.6s\tremaining: 2m 27s\n",
      "186:\tlearn: 0.0034996\ttotal: 33.8s\tremaining: 2m 26s\n",
      "187:\tlearn: 0.0034342\ttotal: 33.9s\tremaining: 2m 26s\n",
      "188:\tlearn: 0.0034342\ttotal: 34s\tremaining: 2m 26s\n",
      "189:\tlearn: 0.0033603\ttotal: 34.2s\tremaining: 2m 25s\n",
      "190:\tlearn: 0.0033603\ttotal: 34.3s\tremaining: 2m 25s\n",
      "191:\tlearn: 0.0033603\ttotal: 34.4s\tremaining: 2m 24s\n",
      "192:\tlearn: 0.0033603\ttotal: 34.5s\tremaining: 2m 24s\n",
      "193:\tlearn: 0.0033363\ttotal: 34.6s\tremaining: 2m 23s\n",
      "194:\tlearn: 0.0033363\ttotal: 34.7s\tremaining: 2m 23s\n",
      "195:\tlearn: 0.0033276\ttotal: 34.8s\tremaining: 2m 22s\n",
      "196:\tlearn: 0.0033062\ttotal: 35s\tremaining: 2m 22s\n",
      "197:\tlearn: 0.0032957\ttotal: 35.1s\tremaining: 2m 22s\n",
      "198:\tlearn: 0.0032856\ttotal: 35.3s\tremaining: 2m 22s\n",
      "199:\tlearn: 0.0032856\ttotal: 35.4s\tremaining: 2m 21s\n",
      "200:\tlearn: 0.0032413\ttotal: 35.5s\tremaining: 2m 21s\n",
      "201:\tlearn: 0.0032028\ttotal: 35.7s\tremaining: 2m 20s\n",
      "202:\tlearn: 0.0031609\ttotal: 35.8s\tremaining: 2m 20s\n",
      "203:\tlearn: 0.0031609\ttotal: 35.9s\tremaining: 2m 20s\n",
      "204:\tlearn: 0.0031609\ttotal: 36s\tremaining: 2m 19s\n",
      "205:\tlearn: 0.0031609\ttotal: 36.2s\tremaining: 2m 19s\n",
      "206:\tlearn: 0.0031609\ttotal: 36.3s\tremaining: 2m 19s\n",
      "207:\tlearn: 0.0031609\ttotal: 36.4s\tremaining: 2m 18s\n",
      "208:\tlearn: 0.0031609\ttotal: 36.5s\tremaining: 2m 18s\n",
      "209:\tlearn: 0.0031608\ttotal: 36.6s\tremaining: 2m 17s\n",
      "210:\tlearn: 0.0031608\ttotal: 36.7s\tremaining: 2m 17s\n",
      "211:\tlearn: 0.0031608\ttotal: 36.8s\tremaining: 2m 16s\n",
      "212:\tlearn: 0.0031608\ttotal: 36.9s\tremaining: 2m 16s\n",
      "213:\tlearn: 0.0031608\ttotal: 37s\tremaining: 2m 16s\n",
      "214:\tlearn: 0.0031608\ttotal: 37.2s\tremaining: 2m 15s\n",
      "215:\tlearn: 0.0031608\ttotal: 37.3s\tremaining: 2m 15s\n",
      "216:\tlearn: 0.0031608\ttotal: 37.4s\tremaining: 2m 14s\n",
      "217:\tlearn: 0.0031608\ttotal: 37.6s\tremaining: 2m 14s\n",
      "218:\tlearn: 0.0031608\ttotal: 37.6s\tremaining: 2m 14s\n",
      "219:\tlearn: 0.0031608\ttotal: 37.7s\tremaining: 2m 13s\n",
      "220:\tlearn: 0.0031608\ttotal: 37.9s\tremaining: 2m 13s\n",
      "221:\tlearn: 0.0031608\ttotal: 38s\tremaining: 2m 13s\n",
      "222:\tlearn: 0.0031608\ttotal: 38.1s\tremaining: 2m 12s\n",
      "223:\tlearn: 0.0031608\ttotal: 38.2s\tremaining: 2m 12s\n",
      "224:\tlearn: 0.0031433\ttotal: 38.3s\tremaining: 2m 11s\n",
      "225:\tlearn: 0.0031433\ttotal: 38.4s\tremaining: 2m 11s\n",
      "226:\tlearn: 0.0031433\ttotal: 38.5s\tremaining: 2m 11s\n",
      "227:\tlearn: 0.0031433\ttotal: 38.6s\tremaining: 2m 10s\n",
      "228:\tlearn: 0.0031433\ttotal: 38.7s\tremaining: 2m 10s\n",
      "229:\tlearn: 0.0031433\ttotal: 38.8s\tremaining: 2m 10s\n",
      "230:\tlearn: 0.0031433\ttotal: 38.9s\tremaining: 2m 9s\n",
      "231:\tlearn: 0.0031433\ttotal: 39s\tremaining: 2m 9s\n",
      "232:\tlearn: 0.0031433\ttotal: 39.2s\tremaining: 2m 8s\n",
      "233:\tlearn: 0.0031433\ttotal: 39.3s\tremaining: 2m 8s\n",
      "234:\tlearn: 0.0031433\ttotal: 39.4s\tremaining: 2m 8s\n",
      "235:\tlearn: 0.0031433\ttotal: 39.5s\tremaining: 2m 7s\n",
      "236:\tlearn: 0.0031433\ttotal: 39.6s\tremaining: 2m 7s\n",
      "237:\tlearn: 0.0031433\ttotal: 39.7s\tremaining: 2m 7s\n",
      "238:\tlearn: 0.0031433\ttotal: 39.8s\tremaining: 2m 6s\n",
      "239:\tlearn: 0.0031433\ttotal: 39.9s\tremaining: 2m 6s\n",
      "240:\tlearn: 0.0031433\ttotal: 40s\tremaining: 2m 6s\n",
      "241:\tlearn: 0.0031433\ttotal: 40.1s\tremaining: 2m 5s\n",
      "242:\tlearn: 0.0031433\ttotal: 40.3s\tremaining: 2m 5s\n",
      "243:\tlearn: 0.0031433\ttotal: 40.4s\tremaining: 2m 5s\n",
      "244:\tlearn: 0.0031433\ttotal: 40.5s\tremaining: 2m 4s\n",
      "245:\tlearn: 0.0031433\ttotal: 40.6s\tremaining: 2m 4s\n",
      "246:\tlearn: 0.0031433\ttotal: 40.7s\tremaining: 2m 4s\n",
      "247:\tlearn: 0.0031433\ttotal: 40.9s\tremaining: 2m 4s\n",
      "248:\tlearn: 0.0031433\ttotal: 41s\tremaining: 2m 3s\n",
      "249:\tlearn: 0.0031433\ttotal: 41.2s\tremaining: 2m 3s\n",
      "250:\tlearn: 0.0031433\ttotal: 41.3s\tremaining: 2m 3s\n",
      "251:\tlearn: 0.0031433\ttotal: 41.4s\tremaining: 2m 2s\n",
      "252:\tlearn: 0.0031433\ttotal: 41.5s\tremaining: 2m 2s\n",
      "253:\tlearn: 0.0031433\ttotal: 41.7s\tremaining: 2m 2s\n",
      "254:\tlearn: 0.0031433\ttotal: 41.8s\tremaining: 2m 2s\n",
      "255:\tlearn: 0.0031433\ttotal: 41.9s\tremaining: 2m 1s\n",
      "256:\tlearn: 0.0031433\ttotal: 42s\tremaining: 2m 1s\n",
      "257:\tlearn: 0.0031433\ttotal: 42.1s\tremaining: 2m 1s\n",
      "258:\tlearn: 0.0031433\ttotal: 42.2s\tremaining: 2m\n",
      "259:\tlearn: 0.0031433\ttotal: 42.3s\tremaining: 2m\n",
      "260:\tlearn: 0.0031433\ttotal: 42.5s\tremaining: 2m\n",
      "261:\tlearn: 0.0031433\ttotal: 42.6s\tremaining: 1m 59s\n",
      "262:\tlearn: 0.0031433\ttotal: 42.7s\tremaining: 1m 59s\n",
      "263:\tlearn: 0.0031433\ttotal: 42.8s\tremaining: 1m 59s\n",
      "264:\tlearn: 0.0031433\ttotal: 42.9s\tremaining: 1m 59s\n",
      "265:\tlearn: 0.0031433\ttotal: 43.1s\tremaining: 1m 58s\n",
      "266:\tlearn: 0.0031433\ttotal: 43.2s\tremaining: 1m 58s\n",
      "267:\tlearn: 0.0031433\ttotal: 43.3s\tremaining: 1m 58s\n",
      "268:\tlearn: 0.0031433\ttotal: 43.4s\tremaining: 1m 57s\n",
      "269:\tlearn: 0.0031433\ttotal: 43.5s\tremaining: 1m 57s\n",
      "270:\tlearn: 0.0031433\ttotal: 43.7s\tremaining: 1m 57s\n",
      "271:\tlearn: 0.0031433\ttotal: 43.8s\tremaining: 1m 57s\n",
      "272:\tlearn: 0.0031433\ttotal: 43.9s\tremaining: 1m 56s\n",
      "273:\tlearn: 0.0031433\ttotal: 44s\tremaining: 1m 56s\n",
      "274:\tlearn: 0.0031433\ttotal: 44.1s\tremaining: 1m 56s\n",
      "275:\tlearn: 0.0031433\ttotal: 44.3s\tremaining: 1m 56s\n",
      "276:\tlearn: 0.0031433\ttotal: 44.4s\tremaining: 1m 55s\n",
      "277:\tlearn: 0.0031433\ttotal: 44.5s\tremaining: 1m 55s\n",
      "278:\tlearn: 0.0031433\ttotal: 44.6s\tremaining: 1m 55s\n",
      "279:\tlearn: 0.0031433\ttotal: 44.8s\tremaining: 1m 55s\n",
      "280:\tlearn: 0.0031433\ttotal: 44.9s\tremaining: 1m 54s\n",
      "281:\tlearn: 0.0031433\ttotal: 45s\tremaining: 1m 54s\n",
      "282:\tlearn: 0.0031433\ttotal: 45.1s\tremaining: 1m 54s\n",
      "283:\tlearn: 0.0031433\ttotal: 45.2s\tremaining: 1m 54s\n",
      "284:\tlearn: 0.0031433\ttotal: 45.4s\tremaining: 1m 53s\n",
      "285:\tlearn: 0.0031433\ttotal: 45.5s\tremaining: 1m 53s\n",
      "286:\tlearn: 0.0031433\ttotal: 45.6s\tremaining: 1m 53s\n",
      "287:\tlearn: 0.0031433\ttotal: 45.7s\tremaining: 1m 53s\n",
      "288:\tlearn: 0.0031433\ttotal: 45.9s\tremaining: 1m 52s\n",
      "289:\tlearn: 0.0031433\ttotal: 46s\tremaining: 1m 52s\n",
      "290:\tlearn: 0.0031433\ttotal: 46.1s\tremaining: 1m 52s\n",
      "291:\tlearn: 0.0031433\ttotal: 46.2s\tremaining: 1m 52s\n",
      "292:\tlearn: 0.0031433\ttotal: 46.4s\tremaining: 1m 51s\n",
      "293:\tlearn: 0.0031433\ttotal: 46.5s\tremaining: 1m 51s\n",
      "294:\tlearn: 0.0031433\ttotal: 46.6s\tremaining: 1m 51s\n",
      "295:\tlearn: 0.0031433\ttotal: 46.8s\tremaining: 1m 51s\n",
      "296:\tlearn: 0.0031433\ttotal: 46.9s\tremaining: 1m 50s\n",
      "297:\tlearn: 0.0031433\ttotal: 47.1s\tremaining: 1m 50s\n",
      "298:\tlearn: 0.0031433\ttotal: 47.2s\tremaining: 1m 50s\n",
      "299:\tlearn: 0.0031433\ttotal: 47.4s\tremaining: 1m 50s\n",
      "300:\tlearn: 0.0031433\ttotal: 47.5s\tremaining: 1m 50s\n",
      "301:\tlearn: 0.0031433\ttotal: 47.6s\tremaining: 1m 50s\n",
      "302:\tlearn: 0.0031433\ttotal: 47.7s\tremaining: 1m 49s\n",
      "303:\tlearn: 0.0031433\ttotal: 47.9s\tremaining: 1m 49s\n",
      "304:\tlearn: 0.0031433\ttotal: 48.1s\tremaining: 1m 49s\n",
      "305:\tlearn: 0.0031433\ttotal: 48.3s\tremaining: 1m 49s\n",
      "306:\tlearn: 0.0031433\ttotal: 48.4s\tremaining: 1m 49s\n",
      "307:\tlearn: 0.0031433\ttotal: 48.6s\tremaining: 1m 49s\n",
      "308:\tlearn: 0.0031433\ttotal: 48.8s\tremaining: 1m 49s\n",
      "309:\tlearn: 0.0031433\ttotal: 48.9s\tremaining: 1m 48s\n",
      "310:\tlearn: 0.0031433\ttotal: 49.1s\tremaining: 1m 48s\n",
      "311:\tlearn: 0.0031433\ttotal: 49.2s\tremaining: 1m 48s\n",
      "312:\tlearn: 0.0031433\ttotal: 49.4s\tremaining: 1m 48s\n",
      "313:\tlearn: 0.0031433\ttotal: 49.6s\tremaining: 1m 48s\n",
      "314:\tlearn: 0.0031433\ttotal: 49.7s\tremaining: 1m 48s\n",
      "315:\tlearn: 0.0031433\ttotal: 49.9s\tremaining: 1m 47s\n",
      "316:\tlearn: 0.0031433\ttotal: 50s\tremaining: 1m 47s\n",
      "317:\tlearn: 0.0031433\ttotal: 50.2s\tremaining: 1m 47s\n",
      "318:\tlearn: 0.0031433\ttotal: 50.3s\tremaining: 1m 47s\n",
      "319:\tlearn: 0.0031433\ttotal: 50.4s\tremaining: 1m 47s\n",
      "320:\tlearn: 0.0031433\ttotal: 50.6s\tremaining: 1m 46s\n",
      "321:\tlearn: 0.0031433\ttotal: 50.8s\tremaining: 1m 46s\n",
      "322:\tlearn: 0.0031433\ttotal: 51.2s\tremaining: 1m 47s\n",
      "323:\tlearn: 0.0031433\ttotal: 51.4s\tremaining: 1m 47s\n",
      "324:\tlearn: 0.0031433\ttotal: 51.7s\tremaining: 1m 47s\n",
      "325:\tlearn: 0.0031433\ttotal: 52s\tremaining: 1m 47s\n",
      "326:\tlearn: 0.0031433\ttotal: 52.4s\tremaining: 1m 47s\n",
      "327:\tlearn: 0.0031433\ttotal: 52.6s\tremaining: 1m 47s\n",
      "328:\tlearn: 0.0031433\ttotal: 52.9s\tremaining: 1m 47s\n",
      "329:\tlearn: 0.0031433\ttotal: 53s\tremaining: 1m 47s\n",
      "330:\tlearn: 0.0031433\ttotal: 53.1s\tremaining: 1m 47s\n",
      "331:\tlearn: 0.0031433\ttotal: 53.2s\tremaining: 1m 47s\n",
      "332:\tlearn: 0.0031433\ttotal: 53.4s\tremaining: 1m 47s\n",
      "333:\tlearn: 0.0031433\ttotal: 53.7s\tremaining: 1m 47s\n",
      "334:\tlearn: 0.0031433\ttotal: 53.9s\tremaining: 1m 46s\n",
      "335:\tlearn: 0.0031433\ttotal: 54.1s\tremaining: 1m 46s\n",
      "336:\tlearn: 0.0031433\ttotal: 54.2s\tremaining: 1m 46s\n",
      "337:\tlearn: 0.0031433\ttotal: 54.4s\tremaining: 1m 46s\n",
      "338:\tlearn: 0.0031433\ttotal: 54.5s\tremaining: 1m 46s\n",
      "339:\tlearn: 0.0031433\ttotal: 54.6s\tremaining: 1m 45s\n",
      "340:\tlearn: 0.0031433\ttotal: 54.7s\tremaining: 1m 45s\n",
      "341:\tlearn: 0.0031433\ttotal: 54.8s\tremaining: 1m 45s\n",
      "342:\tlearn: 0.0031433\ttotal: 55s\tremaining: 1m 45s\n",
      "343:\tlearn: 0.0031433\ttotal: 55.1s\tremaining: 1m 45s\n",
      "344:\tlearn: 0.0031433\ttotal: 55.3s\tremaining: 1m 44s\n",
      "345:\tlearn: 0.0031433\ttotal: 55.4s\tremaining: 1m 44s\n",
      "346:\tlearn: 0.0031433\ttotal: 55.5s\tremaining: 1m 44s\n",
      "347:\tlearn: 0.0031433\ttotal: 55.6s\tremaining: 1m 44s\n",
      "348:\tlearn: 0.0031433\ttotal: 55.8s\tremaining: 1m 44s\n",
      "349:\tlearn: 0.0031433\ttotal: 55.9s\tremaining: 1m 43s\n",
      "350:\tlearn: 0.0031433\ttotal: 56.1s\tremaining: 1m 43s\n",
      "351:\tlearn: 0.0031433\ttotal: 56.2s\tremaining: 1m 43s\n",
      "352:\tlearn: 0.0031433\ttotal: 56.3s\tremaining: 1m 43s\n",
      "353:\tlearn: 0.0031433\ttotal: 56.4s\tremaining: 1m 42s\n",
      "354:\tlearn: 0.0031433\ttotal: 56.5s\tremaining: 1m 42s\n",
      "355:\tlearn: 0.0031433\ttotal: 56.7s\tremaining: 1m 42s\n",
      "356:\tlearn: 0.0031433\ttotal: 56.8s\tremaining: 1m 42s\n",
      "357:\tlearn: 0.0031433\ttotal: 56.9s\tremaining: 1m 42s\n",
      "358:\tlearn: 0.0031433\ttotal: 57s\tremaining: 1m 41s\n",
      "359:\tlearn: 0.0031433\ttotal: 57.1s\tremaining: 1m 41s\n",
      "360:\tlearn: 0.0031433\ttotal: 57.2s\tremaining: 1m 41s\n",
      "361:\tlearn: 0.0031433\ttotal: 57.4s\tremaining: 1m 41s\n",
      "362:\tlearn: 0.0031433\ttotal: 57.5s\tremaining: 1m 40s\n",
      "363:\tlearn: 0.0031433\ttotal: 57.8s\tremaining: 1m 40s\n",
      "364:\tlearn: 0.0031433\ttotal: 57.9s\tremaining: 1m 40s\n",
      "365:\tlearn: 0.0031433\ttotal: 58s\tremaining: 1m 40s\n",
      "366:\tlearn: 0.0031433\ttotal: 58.2s\tremaining: 1m 40s\n",
      "367:\tlearn: 0.0031433\ttotal: 58.3s\tremaining: 1m 40s\n",
      "368:\tlearn: 0.0031433\ttotal: 58.6s\tremaining: 1m 40s\n",
      "369:\tlearn: 0.0031433\ttotal: 58.9s\tremaining: 1m 40s\n",
      "370:\tlearn: 0.0031433\ttotal: 59.3s\tremaining: 1m 40s\n",
      "371:\tlearn: 0.0031433\ttotal: 59.5s\tremaining: 1m 40s\n",
      "372:\tlearn: 0.0031433\ttotal: 59.7s\tremaining: 1m 40s\n",
      "373:\tlearn: 0.0031433\ttotal: 59.9s\tremaining: 1m 40s\n",
      "374:\tlearn: 0.0031433\ttotal: 1m\tremaining: 1m 40s\n",
      "375:\tlearn: 0.0031433\ttotal: 1m\tremaining: 1m 39s\n",
      "376:\tlearn: 0.0031433\ttotal: 1m\tremaining: 1m 39s\n",
      "377:\tlearn: 0.0031433\ttotal: 1m\tremaining: 1m 39s\n",
      "378:\tlearn: 0.0031433\ttotal: 1m\tremaining: 1m 39s\n",
      "379:\tlearn: 0.0031433\ttotal: 1m\tremaining: 1m 39s\n",
      "380:\tlearn: 0.0031433\ttotal: 1m 1s\tremaining: 1m 39s\n",
      "381:\tlearn: 0.0031433\ttotal: 1m 1s\tremaining: 1m 39s\n",
      "382:\tlearn: 0.0031433\ttotal: 1m 1s\tremaining: 1m 39s\n",
      "383:\tlearn: 0.0031433\ttotal: 1m 2s\tremaining: 1m 39s\n",
      "384:\tlearn: 0.0031433\ttotal: 1m 2s\tremaining: 1m 39s\n",
      "385:\tlearn: 0.0031433\ttotal: 1m 2s\tremaining: 1m 39s\n",
      "386:\tlearn: 0.0031433\ttotal: 1m 2s\tremaining: 1m 39s\n",
      "387:\tlearn: 0.0031433\ttotal: 1m 2s\tremaining: 1m 38s\n",
      "388:\tlearn: 0.0031433\ttotal: 1m 2s\tremaining: 1m 38s\n",
      "389:\tlearn: 0.0031433\ttotal: 1m 2s\tremaining: 1m 38s\n",
      "390:\tlearn: 0.0031433\ttotal: 1m 3s\tremaining: 1m 38s\n",
      "391:\tlearn: 0.0031433\ttotal: 1m 3s\tremaining: 1m 38s\n",
      "392:\tlearn: 0.0031433\ttotal: 1m 3s\tremaining: 1m 37s\n",
      "393:\tlearn: 0.0031433\ttotal: 1m 3s\tremaining: 1m 37s\n",
      "394:\tlearn: 0.0031433\ttotal: 1m 3s\tremaining: 1m 37s\n",
      "395:\tlearn: 0.0031433\ttotal: 1m 3s\tremaining: 1m 37s\n",
      "396:\tlearn: 0.0031433\ttotal: 1m 3s\tremaining: 1m 36s\n",
      "397:\tlearn: 0.0031433\ttotal: 1m 3s\tremaining: 1m 36s\n",
      "398:\tlearn: 0.0031433\ttotal: 1m 3s\tremaining: 1m 36s\n",
      "399:\tlearn: 0.0031433\ttotal: 1m 4s\tremaining: 1m 36s\n",
      "400:\tlearn: 0.0031433\ttotal: 1m 4s\tremaining: 1m 35s\n",
      "401:\tlearn: 0.0031433\ttotal: 1m 4s\tremaining: 1m 35s\n",
      "402:\tlearn: 0.0031433\ttotal: 1m 4s\tremaining: 1m 35s\n",
      "403:\tlearn: 0.0031433\ttotal: 1m 4s\tremaining: 1m 35s\n",
      "404:\tlearn: 0.0031433\ttotal: 1m 4s\tremaining: 1m 35s\n",
      "405:\tlearn: 0.0031433\ttotal: 1m 4s\tremaining: 1m 34s\n",
      "406:\tlearn: 0.0031433\ttotal: 1m 4s\tremaining: 1m 34s\n",
      "407:\tlearn: 0.0031433\ttotal: 1m 5s\tremaining: 1m 34s\n",
      "408:\tlearn: 0.0031433\ttotal: 1m 5s\tremaining: 1m 34s\n",
      "409:\tlearn: 0.0031433\ttotal: 1m 5s\tremaining: 1m 33s\n",
      "410:\tlearn: 0.0031433\ttotal: 1m 5s\tremaining: 1m 33s\n",
      "411:\tlearn: 0.0031433\ttotal: 1m 5s\tremaining: 1m 33s\n",
      "412:\tlearn: 0.0031433\ttotal: 1m 5s\tremaining: 1m 33s\n",
      "413:\tlearn: 0.0031433\ttotal: 1m 5s\tremaining: 1m 33s\n",
      "414:\tlearn: 0.0031433\ttotal: 1m 5s\tremaining: 1m 32s\n",
      "415:\tlearn: 0.0031433\ttotal: 1m 6s\tremaining: 1m 32s\n",
      "416:\tlearn: 0.0031433\ttotal: 1m 6s\tremaining: 1m 32s\n",
      "417:\tlearn: 0.0031433\ttotal: 1m 6s\tremaining: 1m 32s\n",
      "418:\tlearn: 0.0031433\ttotal: 1m 6s\tremaining: 1m 32s\n",
      "419:\tlearn: 0.0031433\ttotal: 1m 6s\tremaining: 1m 32s\n",
      "420:\tlearn: 0.0031433\ttotal: 1m 7s\tremaining: 1m 32s\n",
      "421:\tlearn: 0.0031433\ttotal: 1m 7s\tremaining: 1m 32s\n",
      "422:\tlearn: 0.0031433\ttotal: 1m 7s\tremaining: 1m 32s\n",
      "423:\tlearn: 0.0031433\ttotal: 1m 7s\tremaining: 1m 32s\n",
      "424:\tlearn: 0.0031433\ttotal: 1m 8s\tremaining: 1m 32s\n",
      "425:\tlearn: 0.0031433\ttotal: 1m 8s\tremaining: 1m 32s\n",
      "426:\tlearn: 0.0031433\ttotal: 1m 8s\tremaining: 1m 31s\n",
      "427:\tlearn: 0.0031433\ttotal: 1m 8s\tremaining: 1m 31s\n",
      "428:\tlearn: 0.0031433\ttotal: 1m 8s\tremaining: 1m 31s\n",
      "429:\tlearn: 0.0031433\ttotal: 1m 9s\tremaining: 1m 31s\n",
      "430:\tlearn: 0.0031433\ttotal: 1m 9s\tremaining: 1m 31s\n",
      "431:\tlearn: 0.0031433\ttotal: 1m 9s\tremaining: 1m 31s\n",
      "432:\tlearn: 0.0031433\ttotal: 1m 9s\tremaining: 1m 30s\n",
      "433:\tlearn: 0.0031433\ttotal: 1m 9s\tremaining: 1m 30s\n",
      "434:\tlearn: 0.0031433\ttotal: 1m 9s\tremaining: 1m 30s\n",
      "435:\tlearn: 0.0031433\ttotal: 1m 9s\tremaining: 1m 30s\n",
      "436:\tlearn: 0.0031433\ttotal: 1m 10s\tremaining: 1m 30s\n",
      "437:\tlearn: 0.0031433\ttotal: 1m 10s\tremaining: 1m 30s\n",
      "438:\tlearn: 0.0031433\ttotal: 1m 10s\tremaining: 1m 30s\n",
      "439:\tlearn: 0.0031433\ttotal: 1m 10s\tremaining: 1m 29s\n",
      "440:\tlearn: 0.0031433\ttotal: 1m 10s\tremaining: 1m 29s\n",
      "441:\tlearn: 0.0031433\ttotal: 1m 10s\tremaining: 1m 29s\n",
      "442:\tlearn: 0.0031433\ttotal: 1m 11s\tremaining: 1m 29s\n",
      "443:\tlearn: 0.0031433\ttotal: 1m 11s\tremaining: 1m 29s\n",
      "444:\tlearn: 0.0031433\ttotal: 1m 11s\tremaining: 1m 28s\n",
      "445:\tlearn: 0.0031433\ttotal: 1m 11s\tremaining: 1m 28s\n",
      "446:\tlearn: 0.0031433\ttotal: 1m 11s\tremaining: 1m 28s\n",
      "447:\tlearn: 0.0031433\ttotal: 1m 11s\tremaining: 1m 28s\n",
      "448:\tlearn: 0.0031433\ttotal: 1m 11s\tremaining: 1m 28s\n",
      "449:\tlearn: 0.0031433\ttotal: 1m 12s\tremaining: 1m 28s\n",
      "450:\tlearn: 0.0031433\ttotal: 1m 12s\tremaining: 1m 27s\n",
      "451:\tlearn: 0.0031433\ttotal: 1m 12s\tremaining: 1m 27s\n",
      "452:\tlearn: 0.0031433\ttotal: 1m 12s\tremaining: 1m 27s\n",
      "453:\tlearn: 0.0031433\ttotal: 1m 12s\tremaining: 1m 27s\n",
      "454:\tlearn: 0.0031433\ttotal: 1m 12s\tremaining: 1m 27s\n",
      "455:\tlearn: 0.0031433\ttotal: 1m 12s\tremaining: 1m 27s\n",
      "456:\tlearn: 0.0031433\ttotal: 1m 13s\tremaining: 1m 26s\n",
      "457:\tlearn: 0.0031433\ttotal: 1m 13s\tremaining: 1m 26s\n",
      "458:\tlearn: 0.0031433\ttotal: 1m 13s\tremaining: 1m 26s\n",
      "459:\tlearn: 0.0031433\ttotal: 1m 13s\tremaining: 1m 26s\n",
      "460:\tlearn: 0.0031433\ttotal: 1m 13s\tremaining: 1m 26s\n",
      "461:\tlearn: 0.0031433\ttotal: 1m 13s\tremaining: 1m 25s\n",
      "462:\tlearn: 0.0031433\ttotal: 1m 13s\tremaining: 1m 25s\n",
      "463:\tlearn: 0.0031433\ttotal: 1m 14s\tremaining: 1m 25s\n",
      "464:\tlearn: 0.0031433\ttotal: 1m 14s\tremaining: 1m 25s\n",
      "465:\tlearn: 0.0031433\ttotal: 1m 14s\tremaining: 1m 25s\n",
      "466:\tlearn: 0.0031433\ttotal: 1m 14s\tremaining: 1m 25s\n",
      "467:\tlearn: 0.0031433\ttotal: 1m 14s\tremaining: 1m 24s\n",
      "468:\tlearn: 0.0031433\ttotal: 1m 14s\tremaining: 1m 24s\n",
      "469:\tlearn: 0.0031433\ttotal: 1m 15s\tremaining: 1m 24s\n",
      "470:\tlearn: 0.0031433\ttotal: 1m 15s\tremaining: 1m 24s\n",
      "471:\tlearn: 0.0031433\ttotal: 1m 15s\tremaining: 1m 24s\n",
      "472:\tlearn: 0.0031433\ttotal: 1m 15s\tremaining: 1m 24s\n",
      "473:\tlearn: 0.0031433\ttotal: 1m 16s\tremaining: 1m 24s\n",
      "474:\tlearn: 0.0031433\ttotal: 1m 16s\tremaining: 1m 24s\n",
      "475:\tlearn: 0.0031433\ttotal: 1m 16s\tremaining: 1m 24s\n",
      "476:\tlearn: 0.0031433\ttotal: 1m 16s\tremaining: 1m 24s\n",
      "477:\tlearn: 0.0031433\ttotal: 1m 17s\tremaining: 1m 24s\n",
      "478:\tlearn: 0.0031433\ttotal: 1m 17s\tremaining: 1m 24s\n",
      "479:\tlearn: 0.0031433\ttotal: 1m 17s\tremaining: 1m 23s\n",
      "480:\tlearn: 0.0031433\ttotal: 1m 17s\tremaining: 1m 23s\n",
      "481:\tlearn: 0.0031433\ttotal: 1m 17s\tremaining: 1m 23s\n",
      "482:\tlearn: 0.0031433\ttotal: 1m 17s\tremaining: 1m 23s\n",
      "483:\tlearn: 0.0031433\ttotal: 1m 18s\tremaining: 1m 23s\n",
      "484:\tlearn: 0.0031433\ttotal: 1m 18s\tremaining: 1m 23s\n",
      "485:\tlearn: 0.0031433\ttotal: 1m 18s\tremaining: 1m 23s\n",
      "486:\tlearn: 0.0031433\ttotal: 1m 18s\tremaining: 1m 23s\n",
      "487:\tlearn: 0.0031433\ttotal: 1m 19s\tremaining: 1m 23s\n",
      "488:\tlearn: 0.0031433\ttotal: 1m 19s\tremaining: 1m 22s\n",
      "489:\tlearn: 0.0031433\ttotal: 1m 19s\tremaining: 1m 22s\n",
      "490:\tlearn: 0.0031433\ttotal: 1m 19s\tremaining: 1m 22s\n",
      "491:\tlearn: 0.0031433\ttotal: 1m 19s\tremaining: 1m 22s\n",
      "492:\tlearn: 0.0031433\ttotal: 1m 20s\tremaining: 1m 22s\n",
      "493:\tlearn: 0.0031433\ttotal: 1m 20s\tremaining: 1m 22s\n",
      "494:\tlearn: 0.0031433\ttotal: 1m 20s\tremaining: 1m 22s\n",
      "495:\tlearn: 0.0031433\ttotal: 1m 21s\tremaining: 1m 22s\n",
      "496:\tlearn: 0.0031433\ttotal: 1m 21s\tremaining: 1m 22s\n",
      "497:\tlearn: 0.0031433\ttotal: 1m 22s\tremaining: 1m 22s\n",
      "498:\tlearn: 0.0031433\ttotal: 1m 22s\tremaining: 1m 22s\n",
      "499:\tlearn: 0.0031433\ttotal: 1m 22s\tremaining: 1m 22s\n",
      "500:\tlearn: 0.0031433\ttotal: 1m 22s\tremaining: 1m 22s\n",
      "501:\tlearn: 0.0031433\ttotal: 1m 23s\tremaining: 1m 22s\n",
      "502:\tlearn: 0.0031433\ttotal: 1m 23s\tremaining: 1m 22s\n",
      "503:\tlearn: 0.0031433\ttotal: 1m 23s\tremaining: 1m 22s\n",
      "504:\tlearn: 0.0031433\ttotal: 1m 23s\tremaining: 1m 22s\n",
      "505:\tlearn: 0.0031433\ttotal: 1m 24s\tremaining: 1m 22s\n",
      "506:\tlearn: 0.0031433\ttotal: 1m 24s\tremaining: 1m 21s\n",
      "507:\tlearn: 0.0031433\ttotal: 1m 24s\tremaining: 1m 21s\n",
      "508:\tlearn: 0.0031433\ttotal: 1m 24s\tremaining: 1m 21s\n",
      "509:\tlearn: 0.0031433\ttotal: 1m 24s\tremaining: 1m 21s\n",
      "510:\tlearn: 0.0031432\ttotal: 1m 24s\tremaining: 1m 21s\n",
      "511:\tlearn: 0.0031432\ttotal: 1m 24s\tremaining: 1m 20s\n",
      "512:\tlearn: 0.0031432\ttotal: 1m 25s\tremaining: 1m 20s\n",
      "513:\tlearn: 0.0031433\ttotal: 1m 25s\tremaining: 1m 20s\n",
      "514:\tlearn: 0.0031433\ttotal: 1m 25s\tremaining: 1m 20s\n",
      "515:\tlearn: 0.0031433\ttotal: 1m 25s\tremaining: 1m 20s\n",
      "516:\tlearn: 0.0031433\ttotal: 1m 25s\tremaining: 1m 20s\n",
      "517:\tlearn: 0.0031433\ttotal: 1m 25s\tremaining: 1m 19s\n",
      "518:\tlearn: 0.0031433\ttotal: 1m 26s\tremaining: 1m 19s\n",
      "519:\tlearn: 0.0031433\ttotal: 1m 26s\tremaining: 1m 19s\n",
      "520:\tlearn: 0.0031433\ttotal: 1m 26s\tremaining: 1m 19s\n",
      "521:\tlearn: 0.0031433\ttotal: 1m 26s\tremaining: 1m 19s\n",
      "522:\tlearn: 0.0031433\ttotal: 1m 26s\tremaining: 1m 19s\n",
      "523:\tlearn: 0.0031433\ttotal: 1m 26s\tremaining: 1m 18s\n",
      "524:\tlearn: 0.0031433\ttotal: 1m 26s\tremaining: 1m 18s\n",
      "525:\tlearn: 0.0031433\ttotal: 1m 27s\tremaining: 1m 18s\n",
      "526:\tlearn: 0.0031433\ttotal: 1m 27s\tremaining: 1m 18s\n",
      "527:\tlearn: 0.0031433\ttotal: 1m 27s\tremaining: 1m 18s\n",
      "528:\tlearn: 0.0031433\ttotal: 1m 27s\tremaining: 1m 18s\n",
      "529:\tlearn: 0.0031433\ttotal: 1m 28s\tremaining: 1m 18s\n",
      "530:\tlearn: 0.0031433\ttotal: 1m 28s\tremaining: 1m 17s\n",
      "531:\tlearn: 0.0031433\ttotal: 1m 28s\tremaining: 1m 17s\n",
      "532:\tlearn: 0.0031433\ttotal: 1m 28s\tremaining: 1m 17s\n",
      "533:\tlearn: 0.0031433\ttotal: 1m 28s\tremaining: 1m 17s\n",
      "534:\tlearn: 0.0031433\ttotal: 1m 28s\tremaining: 1m 17s\n",
      "535:\tlearn: 0.0031433\ttotal: 1m 28s\tremaining: 1m 16s\n",
      "536:\tlearn: 0.0031433\ttotal: 1m 29s\tremaining: 1m 16s\n",
      "537:\tlearn: 0.0031433\ttotal: 1m 29s\tremaining: 1m 16s\n",
      "538:\tlearn: 0.0031433\ttotal: 1m 29s\tremaining: 1m 16s\n",
      "539:\tlearn: 0.0031433\ttotal: 1m 29s\tremaining: 1m 16s\n",
      "540:\tlearn: 0.0031433\ttotal: 1m 29s\tremaining: 1m 15s\n",
      "541:\tlearn: 0.0031433\ttotal: 1m 29s\tremaining: 1m 15s\n",
      "542:\tlearn: 0.0031433\ttotal: 1m 29s\tremaining: 1m 15s\n",
      "543:\tlearn: 0.0031433\ttotal: 1m 29s\tremaining: 1m 15s\n",
      "544:\tlearn: 0.0031433\ttotal: 1m 30s\tremaining: 1m 15s\n",
      "545:\tlearn: 0.0031433\ttotal: 1m 30s\tremaining: 1m 14s\n",
      "546:\tlearn: 0.0031433\ttotal: 1m 30s\tremaining: 1m 14s\n",
      "547:\tlearn: 0.0031433\ttotal: 1m 30s\tremaining: 1m 14s\n",
      "548:\tlearn: 0.0031433\ttotal: 1m 30s\tremaining: 1m 14s\n",
      "549:\tlearn: 0.0031433\ttotal: 1m 30s\tremaining: 1m 14s\n",
      "550:\tlearn: 0.0031433\ttotal: 1m 30s\tremaining: 1m 13s\n",
      "551:\tlearn: 0.0031433\ttotal: 1m 30s\tremaining: 1m 13s\n",
      "552:\tlearn: 0.0031433\ttotal: 1m 30s\tremaining: 1m 13s\n",
      "553:\tlearn: 0.0031433\ttotal: 1m 31s\tremaining: 1m 13s\n",
      "554:\tlearn: 0.0031433\ttotal: 1m 31s\tremaining: 1m 13s\n",
      "555:\tlearn: 0.0031433\ttotal: 1m 31s\tremaining: 1m 12s\n",
      "556:\tlearn: 0.0031433\ttotal: 1m 31s\tremaining: 1m 12s\n",
      "557:\tlearn: 0.0031433\ttotal: 1m 31s\tremaining: 1m 12s\n",
      "558:\tlearn: 0.0031433\ttotal: 1m 31s\tremaining: 1m 12s\n",
      "559:\tlearn: 0.0031433\ttotal: 1m 31s\tremaining: 1m 12s\n",
      "560:\tlearn: 0.0031433\ttotal: 1m 32s\tremaining: 1m 12s\n",
      "561:\tlearn: 0.0031433\ttotal: 1m 32s\tremaining: 1m 11s\n",
      "562:\tlearn: 0.0031433\ttotal: 1m 32s\tremaining: 1m 11s\n",
      "563:\tlearn: 0.0031433\ttotal: 1m 32s\tremaining: 1m 11s\n",
      "564:\tlearn: 0.0031433\ttotal: 1m 32s\tremaining: 1m 11s\n",
      "565:\tlearn: 0.0031433\ttotal: 1m 32s\tremaining: 1m 11s\n",
      "566:\tlearn: 0.0031433\ttotal: 1m 32s\tremaining: 1m 10s\n",
      "567:\tlearn: 0.0031433\ttotal: 1m 32s\tremaining: 1m 10s\n",
      "568:\tlearn: 0.0031433\ttotal: 1m 32s\tremaining: 1m 10s\n",
      "569:\tlearn: 0.0031433\ttotal: 1m 33s\tremaining: 1m 10s\n",
      "570:\tlearn: 0.0031433\ttotal: 1m 33s\tremaining: 1m 10s\n",
      "571:\tlearn: 0.0031433\ttotal: 1m 33s\tremaining: 1m 9s\n",
      "572:\tlearn: 0.0031433\ttotal: 1m 33s\tremaining: 1m 9s\n",
      "573:\tlearn: 0.0031433\ttotal: 1m 33s\tremaining: 1m 9s\n",
      "574:\tlearn: 0.0031433\ttotal: 1m 33s\tremaining: 1m 9s\n",
      "575:\tlearn: 0.0031433\ttotal: 1m 33s\tremaining: 1m 9s\n",
      "576:\tlearn: 0.0031433\ttotal: 1m 33s\tremaining: 1m 8s\n",
      "577:\tlearn: 0.0031433\ttotal: 1m 33s\tremaining: 1m 8s\n",
      "578:\tlearn: 0.0031433\ttotal: 1m 34s\tremaining: 1m 8s\n",
      "579:\tlearn: 0.0031433\ttotal: 1m 34s\tremaining: 1m 8s\n",
      "580:\tlearn: 0.0031433\ttotal: 1m 34s\tremaining: 1m 8s\n",
      "581:\tlearn: 0.0031433\ttotal: 1m 34s\tremaining: 1m 7s\n",
      "582:\tlearn: 0.0031433\ttotal: 1m 34s\tremaining: 1m 7s\n",
      "583:\tlearn: 0.0031433\ttotal: 1m 34s\tremaining: 1m 7s\n",
      "584:\tlearn: 0.0031433\ttotal: 1m 34s\tremaining: 1m 7s\n",
      "585:\tlearn: 0.0031433\ttotal: 1m 34s\tremaining: 1m 7s\n",
      "586:\tlearn: 0.0031433\ttotal: 1m 35s\tremaining: 1m 6s\n",
      "587:\tlearn: 0.0031433\ttotal: 1m 35s\tremaining: 1m 6s\n",
      "588:\tlearn: 0.0031433\ttotal: 1m 35s\tremaining: 1m 6s\n",
      "589:\tlearn: 0.0031433\ttotal: 1m 35s\tremaining: 1m 6s\n",
      "590:\tlearn: 0.0031433\ttotal: 1m 35s\tremaining: 1m 6s\n",
      "591:\tlearn: 0.0031433\ttotal: 1m 35s\tremaining: 1m 5s\n",
      "592:\tlearn: 0.0031433\ttotal: 1m 35s\tremaining: 1m 5s\n",
      "593:\tlearn: 0.0031433\ttotal: 1m 36s\tremaining: 1m 5s\n",
      "594:\tlearn: 0.0031433\ttotal: 1m 36s\tremaining: 1m 5s\n",
      "595:\tlearn: 0.0031433\ttotal: 1m 36s\tremaining: 1m 5s\n",
      "596:\tlearn: 0.0031433\ttotal: 1m 36s\tremaining: 1m 5s\n",
      "597:\tlearn: 0.0031433\ttotal: 1m 36s\tremaining: 1m 5s\n",
      "598:\tlearn: 0.0031433\ttotal: 1m 36s\tremaining: 1m 4s\n",
      "599:\tlearn: 0.0031433\ttotal: 1m 36s\tremaining: 1m 4s\n",
      "600:\tlearn: 0.0031433\ttotal: 1m 37s\tremaining: 1m 4s\n",
      "601:\tlearn: 0.0031433\ttotal: 1m 37s\tremaining: 1m 4s\n",
      "602:\tlearn: 0.0031433\ttotal: 1m 37s\tremaining: 1m 4s\n",
      "603:\tlearn: 0.0031433\ttotal: 1m 37s\tremaining: 1m 3s\n",
      "604:\tlearn: 0.0031433\ttotal: 1m 37s\tremaining: 1m 3s\n",
      "605:\tlearn: 0.0031433\ttotal: 1m 37s\tremaining: 1m 3s\n",
      "606:\tlearn: 0.0031433\ttotal: 1m 38s\tremaining: 1m 3s\n",
      "607:\tlearn: 0.0031433\ttotal: 1m 38s\tremaining: 1m 3s\n",
      "608:\tlearn: 0.0031433\ttotal: 1m 38s\tremaining: 1m 3s\n",
      "609:\tlearn: 0.0031433\ttotal: 1m 38s\tremaining: 1m 2s\n",
      "610:\tlearn: 0.0031433\ttotal: 1m 38s\tremaining: 1m 2s\n",
      "611:\tlearn: 0.0031433\ttotal: 1m 38s\tremaining: 1m 2s\n",
      "612:\tlearn: 0.0031433\ttotal: 1m 38s\tremaining: 1m 2s\n",
      "613:\tlearn: 0.0031433\ttotal: 1m 38s\tremaining: 1m 2s\n",
      "614:\tlearn: 0.0031433\ttotal: 1m 39s\tremaining: 1m 2s\n",
      "615:\tlearn: 0.0031433\ttotal: 1m 39s\tremaining: 1m 1s\n",
      "616:\tlearn: 0.0031433\ttotal: 1m 39s\tremaining: 1m 1s\n",
      "617:\tlearn: 0.0031433\ttotal: 1m 39s\tremaining: 1m 1s\n",
      "618:\tlearn: 0.0031433\ttotal: 1m 39s\tremaining: 1m 1s\n",
      "619:\tlearn: 0.0031433\ttotal: 1m 39s\tremaining: 1m 1s\n",
      "620:\tlearn: 0.0031433\ttotal: 1m 39s\tremaining: 1m\n",
      "621:\tlearn: 0.0031433\ttotal: 1m 39s\tremaining: 1m\n",
      "622:\tlearn: 0.0031433\ttotal: 1m 40s\tremaining: 1m\n",
      "623:\tlearn: 0.0031433\ttotal: 1m 40s\tremaining: 1m\n",
      "624:\tlearn: 0.0031433\ttotal: 1m 40s\tremaining: 1m\n",
      "625:\tlearn: 0.0031433\ttotal: 1m 40s\tremaining: 1m\n",
      "626:\tlearn: 0.0031433\ttotal: 1m 40s\tremaining: 59.9s\n",
      "627:\tlearn: 0.0031433\ttotal: 1m 40s\tremaining: 59.7s\n",
      "628:\tlearn: 0.0031433\ttotal: 1m 40s\tremaining: 59.5s\n",
      "629:\tlearn: 0.0031433\ttotal: 1m 41s\tremaining: 59.4s\n",
      "630:\tlearn: 0.0031433\ttotal: 1m 41s\tremaining: 59.2s\n",
      "631:\tlearn: 0.0031433\ttotal: 1m 41s\tremaining: 59s\n",
      "632:\tlearn: 0.0031433\ttotal: 1m 41s\tremaining: 58.9s\n",
      "633:\tlearn: 0.0031433\ttotal: 1m 41s\tremaining: 58.7s\n",
      "634:\tlearn: 0.0031433\ttotal: 1m 41s\tremaining: 58.5s\n",
      "635:\tlearn: 0.0031433\ttotal: 1m 41s\tremaining: 58.3s\n",
      "636:\tlearn: 0.0031433\ttotal: 1m 42s\tremaining: 58.2s\n",
      "637:\tlearn: 0.0031433\ttotal: 1m 42s\tremaining: 58s\n",
      "638:\tlearn: 0.0031433\ttotal: 1m 42s\tremaining: 57.8s\n",
      "639:\tlearn: 0.0031433\ttotal: 1m 42s\tremaining: 57.6s\n",
      "640:\tlearn: 0.0031433\ttotal: 1m 42s\tremaining: 57.5s\n",
      "641:\tlearn: 0.0031433\ttotal: 1m 42s\tremaining: 57.3s\n",
      "642:\tlearn: 0.0031433\ttotal: 1m 42s\tremaining: 57.1s\n",
      "643:\tlearn: 0.0031433\ttotal: 1m 42s\tremaining: 56.9s\n",
      "644:\tlearn: 0.0031433\ttotal: 1m 43s\tremaining: 56.7s\n",
      "645:\tlearn: 0.0031433\ttotal: 1m 43s\tremaining: 56.5s\n",
      "646:\tlearn: 0.0031433\ttotal: 1m 43s\tremaining: 56.3s\n",
      "647:\tlearn: 0.0031433\ttotal: 1m 43s\tremaining: 56.1s\n",
      "648:\tlearn: 0.0031433\ttotal: 1m 43s\tremaining: 56s\n",
      "649:\tlearn: 0.0031433\ttotal: 1m 43s\tremaining: 55.8s\n",
      "650:\tlearn: 0.0031433\ttotal: 1m 43s\tremaining: 55.6s\n",
      "651:\tlearn: 0.0031433\ttotal: 1m 43s\tremaining: 55.4s\n",
      "652:\tlearn: 0.0031433\ttotal: 1m 43s\tremaining: 55.2s\n",
      "653:\tlearn: 0.0031433\ttotal: 1m 44s\tremaining: 55.1s\n",
      "654:\tlearn: 0.0031433\ttotal: 1m 44s\tremaining: 54.9s\n",
      "655:\tlearn: 0.0031433\ttotal: 1m 44s\tremaining: 54.8s\n",
      "656:\tlearn: 0.0031433\ttotal: 1m 44s\tremaining: 54.6s\n",
      "657:\tlearn: 0.0031433\ttotal: 1m 44s\tremaining: 54.4s\n",
      "658:\tlearn: 0.0031433\ttotal: 1m 44s\tremaining: 54.2s\n",
      "659:\tlearn: 0.0031433\ttotal: 1m 44s\tremaining: 54.1s\n",
      "660:\tlearn: 0.0031433\ttotal: 1m 45s\tremaining: 53.9s\n",
      "661:\tlearn: 0.0031433\ttotal: 1m 45s\tremaining: 53.7s\n",
      "662:\tlearn: 0.0031433\ttotal: 1m 45s\tremaining: 53.6s\n",
      "663:\tlearn: 0.0031433\ttotal: 1m 45s\tremaining: 53.4s\n",
      "664:\tlearn: 0.0031433\ttotal: 1m 45s\tremaining: 53.2s\n",
      "665:\tlearn: 0.0031433\ttotal: 1m 45s\tremaining: 53s\n",
      "666:\tlearn: 0.0031433\ttotal: 1m 45s\tremaining: 52.8s\n",
      "667:\tlearn: 0.0031433\ttotal: 1m 45s\tremaining: 52.7s\n",
      "668:\tlearn: 0.0031433\ttotal: 1m 46s\tremaining: 52.5s\n",
      "669:\tlearn: 0.0031433\ttotal: 1m 46s\tremaining: 52.3s\n",
      "670:\tlearn: 0.0031433\ttotal: 1m 46s\tremaining: 52.1s\n",
      "671:\tlearn: 0.0031433\ttotal: 1m 46s\tremaining: 52s\n",
      "672:\tlearn: 0.0031433\ttotal: 1m 46s\tremaining: 51.8s\n",
      "673:\tlearn: 0.0031433\ttotal: 1m 46s\tremaining: 51.6s\n",
      "674:\tlearn: 0.0031433\ttotal: 1m 46s\tremaining: 51.4s\n",
      "675:\tlearn: 0.0031433\ttotal: 1m 46s\tremaining: 51.3s\n",
      "676:\tlearn: 0.0031433\ttotal: 1m 47s\tremaining: 51.1s\n",
      "677:\tlearn: 0.0031433\ttotal: 1m 47s\tremaining: 50.9s\n",
      "678:\tlearn: 0.0031433\ttotal: 1m 47s\tremaining: 50.8s\n",
      "679:\tlearn: 0.0031433\ttotal: 1m 47s\tremaining: 50.6s\n",
      "680:\tlearn: 0.0031433\ttotal: 1m 47s\tremaining: 50.4s\n",
      "681:\tlearn: 0.0031433\ttotal: 1m 47s\tremaining: 50.3s\n",
      "682:\tlearn: 0.0031433\ttotal: 1m 48s\tremaining: 50.1s\n",
      "683:\tlearn: 0.0031433\ttotal: 1m 48s\tremaining: 50s\n",
      "684:\tlearn: 0.0031433\ttotal: 1m 48s\tremaining: 49.8s\n",
      "685:\tlearn: 0.0031433\ttotal: 1m 48s\tremaining: 49.7s\n",
      "686:\tlearn: 0.0031433\ttotal: 1m 48s\tremaining: 49.5s\n",
      "687:\tlearn: 0.0031433\ttotal: 1m 48s\tremaining: 49.4s\n",
      "688:\tlearn: 0.0031433\ttotal: 1m 48s\tremaining: 49.2s\n",
      "689:\tlearn: 0.0031433\ttotal: 1m 49s\tremaining: 49s\n",
      "690:\tlearn: 0.0031433\ttotal: 1m 49s\tremaining: 48.9s\n",
      "691:\tlearn: 0.0031433\ttotal: 1m 49s\tremaining: 48.7s\n",
      "692:\tlearn: 0.0031433\ttotal: 1m 49s\tremaining: 48.6s\n",
      "693:\tlearn: 0.0031433\ttotal: 1m 49s\tremaining: 48.4s\n",
      "694:\tlearn: 0.0031433\ttotal: 1m 49s\tremaining: 48.2s\n",
      "695:\tlearn: 0.0031433\ttotal: 1m 50s\tremaining: 48.1s\n",
      "696:\tlearn: 0.0031433\ttotal: 1m 50s\tremaining: 47.9s\n",
      "697:\tlearn: 0.0031433\ttotal: 1m 50s\tremaining: 47.7s\n",
      "698:\tlearn: 0.0031433\ttotal: 1m 50s\tremaining: 47.7s\n",
      "699:\tlearn: 0.0031433\ttotal: 1m 50s\tremaining: 47.5s\n",
      "700:\tlearn: 0.0031433\ttotal: 1m 50s\tremaining: 47.3s\n",
      "701:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 47.1s\n",
      "702:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 47s\n",
      "703:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 46.8s\n",
      "704:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 46.6s\n",
      "705:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 46.4s\n",
      "706:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 46.3s\n",
      "707:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 46.1s\n",
      "708:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 45.9s\n",
      "709:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 45.7s\n",
      "710:\tlearn: 0.0031433\ttotal: 1m 51s\tremaining: 45.5s\n",
      "711:\tlearn: 0.0031433\ttotal: 1m 52s\tremaining: 45.3s\n",
      "712:\tlearn: 0.0031433\ttotal: 1m 52s\tremaining: 45.2s\n",
      "713:\tlearn: 0.0031433\ttotal: 1m 52s\tremaining: 45s\n",
      "714:\tlearn: 0.0031433\ttotal: 1m 52s\tremaining: 44.8s\n",
      "715:\tlearn: 0.0031433\ttotal: 1m 52s\tremaining: 44.7s\n",
      "716:\tlearn: 0.0031433\ttotal: 1m 52s\tremaining: 44.5s\n",
      "717:\tlearn: 0.0031433\ttotal: 1m 52s\tremaining: 44.3s\n",
      "718:\tlearn: 0.0031433\ttotal: 1m 52s\tremaining: 44.1s\n",
      "719:\tlearn: 0.0031433\ttotal: 1m 52s\tremaining: 43.9s\n",
      "720:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 43.8s\n",
      "721:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 43.6s\n",
      "722:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 43.4s\n",
      "723:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 43.2s\n",
      "724:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 43s\n",
      "725:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 42.9s\n",
      "726:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 42.7s\n",
      "727:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 42.5s\n",
      "728:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 42.3s\n",
      "729:\tlearn: 0.0031433\ttotal: 1m 53s\tremaining: 42.2s\n",
      "730:\tlearn: 0.0031433\ttotal: 1m 54s\tremaining: 42s\n",
      "731:\tlearn: 0.0031433\ttotal: 1m 54s\tremaining: 41.8s\n",
      "732:\tlearn: 0.0031433\ttotal: 1m 54s\tremaining: 41.6s\n",
      "733:\tlearn: 0.0031433\ttotal: 1m 54s\tremaining: 41.5s\n",
      "734:\tlearn: 0.0031433\ttotal: 1m 54s\tremaining: 41.3s\n",
      "735:\tlearn: 0.0031433\ttotal: 1m 54s\tremaining: 41.2s\n",
      "736:\tlearn: 0.0031433\ttotal: 1m 54s\tremaining: 41s\n",
      "737:\tlearn: 0.0031433\ttotal: 1m 55s\tremaining: 40.8s\n",
      "738:\tlearn: 0.0031433\ttotal: 1m 55s\tremaining: 40.7s\n",
      "739:\tlearn: 0.0031433\ttotal: 1m 55s\tremaining: 40.6s\n",
      "740:\tlearn: 0.0031433\ttotal: 1m 55s\tremaining: 40.4s\n",
      "741:\tlearn: 0.0031433\ttotal: 1m 55s\tremaining: 40.2s\n",
      "742:\tlearn: 0.0031433\ttotal: 1m 55s\tremaining: 40.1s\n",
      "743:\tlearn: 0.0031433\ttotal: 1m 56s\tremaining: 39.9s\n",
      "744:\tlearn: 0.0031433\ttotal: 1m 56s\tremaining: 39.8s\n",
      "745:\tlearn: 0.0031433\ttotal: 1m 56s\tremaining: 39.6s\n",
      "746:\tlearn: 0.0031433\ttotal: 1m 56s\tremaining: 39.4s\n",
      "747:\tlearn: 0.0031433\ttotal: 1m 56s\tremaining: 39.3s\n",
      "748:\tlearn: 0.0031433\ttotal: 1m 56s\tremaining: 39.1s\n",
      "749:\tlearn: 0.0031433\ttotal: 1m 56s\tremaining: 39s\n",
      "750:\tlearn: 0.0031433\ttotal: 1m 57s\tremaining: 38.8s\n",
      "751:\tlearn: 0.0031433\ttotal: 1m 57s\tremaining: 38.7s\n",
      "752:\tlearn: 0.0031433\ttotal: 1m 57s\tremaining: 38.5s\n",
      "753:\tlearn: 0.0031433\ttotal: 1m 57s\tremaining: 38.3s\n",
      "754:\tlearn: 0.0031433\ttotal: 1m 57s\tremaining: 38.2s\n",
      "755:\tlearn: 0.0031433\ttotal: 1m 57s\tremaining: 38s\n",
      "756:\tlearn: 0.0031433\ttotal: 1m 58s\tremaining: 37.9s\n",
      "757:\tlearn: 0.0031433\ttotal: 1m 58s\tremaining: 37.7s\n",
      "758:\tlearn: 0.0031433\ttotal: 1m 58s\tremaining: 37.6s\n",
      "759:\tlearn: 0.0031433\ttotal: 1m 58s\tremaining: 37.4s\n",
      "760:\tlearn: 0.0031433\ttotal: 1m 58s\tremaining: 37.3s\n",
      "761:\tlearn: 0.0031433\ttotal: 1m 58s\tremaining: 37.1s\n",
      "762:\tlearn: 0.0031433\ttotal: 1m 58s\tremaining: 36.9s\n",
      "763:\tlearn: 0.0031433\ttotal: 1m 59s\tremaining: 36.8s\n",
      "764:\tlearn: 0.0031433\ttotal: 1m 59s\tremaining: 36.6s\n",
      "765:\tlearn: 0.0031433\ttotal: 1m 59s\tremaining: 36.5s\n",
      "766:\tlearn: 0.0031433\ttotal: 1m 59s\tremaining: 36.3s\n",
      "767:\tlearn: 0.0031433\ttotal: 1m 59s\tremaining: 36.1s\n",
      "768:\tlearn: 0.0031433\ttotal: 1m 59s\tremaining: 36s\n",
      "769:\tlearn: 0.0031433\ttotal: 1m 59s\tremaining: 35.8s\n",
      "770:\tlearn: 0.0031433\ttotal: 1m 59s\tremaining: 35.6s\n",
      "771:\tlearn: 0.0031433\ttotal: 2m\tremaining: 35.5s\n",
      "772:\tlearn: 0.0031433\ttotal: 2m\tremaining: 35.3s\n",
      "773:\tlearn: 0.0031433\ttotal: 2m\tremaining: 35.1s\n",
      "774:\tlearn: 0.0031433\ttotal: 2m\tremaining: 35s\n",
      "775:\tlearn: 0.0031433\ttotal: 2m\tremaining: 34.8s\n",
      "776:\tlearn: 0.0031433\ttotal: 2m\tremaining: 34.6s\n",
      "777:\tlearn: 0.0031433\ttotal: 2m\tremaining: 34.5s\n",
      "778:\tlearn: 0.0031433\ttotal: 2m 1s\tremaining: 34.3s\n",
      "779:\tlearn: 0.0031433\ttotal: 2m 1s\tremaining: 34.2s\n",
      "780:\tlearn: 0.0031433\ttotal: 2m 1s\tremaining: 34s\n",
      "781:\tlearn: 0.0031433\ttotal: 2m 1s\tremaining: 33.8s\n",
      "782:\tlearn: 0.0031433\ttotal: 2m 1s\tremaining: 33.7s\n",
      "783:\tlearn: 0.0031433\ttotal: 2m 1s\tremaining: 33.5s\n",
      "784:\tlearn: 0.0031433\ttotal: 2m 1s\tremaining: 33.3s\n",
      "785:\tlearn: 0.0031433\ttotal: 2m 1s\tremaining: 33.2s\n",
      "786:\tlearn: 0.0031433\ttotal: 2m 2s\tremaining: 33s\n",
      "787:\tlearn: 0.0031433\ttotal: 2m 2s\tremaining: 32.9s\n",
      "788:\tlearn: 0.0031433\ttotal: 2m 2s\tremaining: 32.7s\n",
      "789:\tlearn: 0.0031433\ttotal: 2m 2s\tremaining: 32.6s\n",
      "790:\tlearn: 0.0031433\ttotal: 2m 2s\tremaining: 32.4s\n",
      "791:\tlearn: 0.0031433\ttotal: 2m 2s\tremaining: 32.3s\n",
      "792:\tlearn: 0.0031433\ttotal: 2m 3s\tremaining: 32.2s\n",
      "793:\tlearn: 0.0031433\ttotal: 2m 3s\tremaining: 32s\n",
      "794:\tlearn: 0.0031433\ttotal: 2m 3s\tremaining: 31.9s\n",
      "795:\tlearn: 0.0031433\ttotal: 2m 3s\tremaining: 31.7s\n",
      "796:\tlearn: 0.0031433\ttotal: 2m 3s\tremaining: 31.6s\n",
      "797:\tlearn: 0.0031433\ttotal: 2m 4s\tremaining: 31.4s\n",
      "798:\tlearn: 0.0031433\ttotal: 2m 4s\tremaining: 31.3s\n",
      "799:\tlearn: 0.0031433\ttotal: 2m 4s\tremaining: 31.1s\n",
      "800:\tlearn: 0.0031433\ttotal: 2m 4s\tremaining: 31s\n",
      "801:\tlearn: 0.0031433\ttotal: 2m 4s\tremaining: 30.8s\n",
      "802:\tlearn: 0.0031433\ttotal: 2m 4s\tremaining: 30.6s\n",
      "803:\tlearn: 0.0031433\ttotal: 2m 5s\tremaining: 30.5s\n",
      "804:\tlearn: 0.0031433\ttotal: 2m 5s\tremaining: 30.3s\n",
      "805:\tlearn: 0.0031433\ttotal: 2m 5s\tremaining: 30.1s\n",
      "806:\tlearn: 0.0031433\ttotal: 2m 5s\tremaining: 30s\n",
      "807:\tlearn: 0.0031433\ttotal: 2m 5s\tremaining: 29.8s\n",
      "808:\tlearn: 0.0031433\ttotal: 2m 5s\tremaining: 29.7s\n",
      "809:\tlearn: 0.0031433\ttotal: 2m 5s\tremaining: 29.5s\n",
      "810:\tlearn: 0.0031433\ttotal: 2m 5s\tremaining: 29.3s\n",
      "811:\tlearn: 0.0031433\ttotal: 2m 6s\tremaining: 29.2s\n",
      "812:\tlearn: 0.0031433\ttotal: 2m 6s\tremaining: 29s\n",
      "813:\tlearn: 0.0031433\ttotal: 2m 6s\tremaining: 28.9s\n",
      "814:\tlearn: 0.0031433\ttotal: 2m 6s\tremaining: 28.7s\n",
      "815:\tlearn: 0.0031433\ttotal: 2m 6s\tremaining: 28.6s\n",
      "816:\tlearn: 0.0031433\ttotal: 2m 6s\tremaining: 28.4s\n",
      "817:\tlearn: 0.0031433\ttotal: 2m 6s\tremaining: 28.3s\n",
      "818:\tlearn: 0.0031433\ttotal: 2m 7s\tremaining: 28.1s\n",
      "819:\tlearn: 0.0031433\ttotal: 2m 7s\tremaining: 27.9s\n",
      "820:\tlearn: 0.0031433\ttotal: 2m 7s\tremaining: 27.8s\n",
      "821:\tlearn: 0.0031433\ttotal: 2m 7s\tremaining: 27.6s\n",
      "822:\tlearn: 0.0031433\ttotal: 2m 7s\tremaining: 27.4s\n",
      "823:\tlearn: 0.0031433\ttotal: 2m 7s\tremaining: 27.3s\n",
      "824:\tlearn: 0.0031433\ttotal: 2m 7s\tremaining: 27.1s\n",
      "825:\tlearn: 0.0031433\ttotal: 2m 8s\tremaining: 27s\n",
      "826:\tlearn: 0.0031433\ttotal: 2m 8s\tremaining: 26.8s\n",
      "827:\tlearn: 0.0031433\ttotal: 2m 8s\tremaining: 26.7s\n",
      "828:\tlearn: 0.0031433\ttotal: 2m 8s\tremaining: 26.5s\n",
      "829:\tlearn: 0.0031433\ttotal: 2m 8s\tremaining: 26.3s\n",
      "830:\tlearn: 0.0031433\ttotal: 2m 8s\tremaining: 26.2s\n",
      "831:\tlearn: 0.0031433\ttotal: 2m 8s\tremaining: 26s\n",
      "832:\tlearn: 0.0031433\ttotal: 2m 8s\tremaining: 25.9s\n",
      "833:\tlearn: 0.0031433\ttotal: 2m 9s\tremaining: 25.7s\n",
      "834:\tlearn: 0.0031433\ttotal: 2m 9s\tremaining: 25.5s\n",
      "835:\tlearn: 0.0031433\ttotal: 2m 9s\tremaining: 25.4s\n",
      "836:\tlearn: 0.0031433\ttotal: 2m 9s\tremaining: 25.2s\n",
      "837:\tlearn: 0.0031433\ttotal: 2m 9s\tremaining: 25.1s\n",
      "838:\tlearn: 0.0031433\ttotal: 2m 9s\tremaining: 24.9s\n",
      "839:\tlearn: 0.0031433\ttotal: 2m 9s\tremaining: 24.7s\n",
      "840:\tlearn: 0.0031433\ttotal: 2m 9s\tremaining: 24.6s\n",
      "841:\tlearn: 0.0031433\ttotal: 2m 10s\tremaining: 24.4s\n",
      "842:\tlearn: 0.0031433\ttotal: 2m 10s\tremaining: 24.3s\n",
      "843:\tlearn: 0.0031433\ttotal: 2m 10s\tremaining: 24.1s\n",
      "844:\tlearn: 0.0031433\ttotal: 2m 10s\tremaining: 24s\n",
      "845:\tlearn: 0.0031433\ttotal: 2m 10s\tremaining: 23.8s\n",
      "846:\tlearn: 0.0031433\ttotal: 2m 10s\tremaining: 23.6s\n",
      "847:\tlearn: 0.0031433\ttotal: 2m 11s\tremaining: 23.5s\n",
      "848:\tlearn: 0.0031433\ttotal: 2m 11s\tremaining: 23.3s\n",
      "849:\tlearn: 0.0031433\ttotal: 2m 11s\tremaining: 23.2s\n",
      "850:\tlearn: 0.0031433\ttotal: 2m 11s\tremaining: 23s\n",
      "851:\tlearn: 0.0031433\ttotal: 2m 11s\tremaining: 22.8s\n",
      "852:\tlearn: 0.0031433\ttotal: 2m 11s\tremaining: 22.7s\n",
      "853:\tlearn: 0.0031433\ttotal: 2m 11s\tremaining: 22.5s\n",
      "854:\tlearn: 0.0031433\ttotal: 2m 11s\tremaining: 22.4s\n",
      "855:\tlearn: 0.0031433\ttotal: 2m 12s\tremaining: 22.2s\n",
      "856:\tlearn: 0.0031433\ttotal: 2m 12s\tremaining: 22.1s\n",
      "857:\tlearn: 0.0031433\ttotal: 2m 12s\tremaining: 21.9s\n",
      "858:\tlearn: 0.0031433\ttotal: 2m 12s\tremaining: 21.7s\n",
      "859:\tlearn: 0.0031433\ttotal: 2m 12s\tremaining: 21.6s\n",
      "860:\tlearn: 0.0031433\ttotal: 2m 12s\tremaining: 21.5s\n",
      "861:\tlearn: 0.0031433\ttotal: 2m 13s\tremaining: 21.3s\n",
      "862:\tlearn: 0.0031433\ttotal: 2m 13s\tremaining: 21.1s\n",
      "863:\tlearn: 0.0031433\ttotal: 2m 13s\tremaining: 21s\n",
      "864:\tlearn: 0.0031433\ttotal: 2m 13s\tremaining: 20.9s\n",
      "865:\tlearn: 0.0031433\ttotal: 2m 13s\tremaining: 20.7s\n",
      "866:\tlearn: 0.0031433\ttotal: 2m 13s\tremaining: 20.5s\n",
      "867:\tlearn: 0.0031433\ttotal: 2m 13s\tremaining: 20.4s\n",
      "868:\tlearn: 0.0031433\ttotal: 2m 14s\tremaining: 20.2s\n",
      "869:\tlearn: 0.0031433\ttotal: 2m 14s\tremaining: 20.1s\n",
      "870:\tlearn: 0.0031433\ttotal: 2m 14s\tremaining: 19.9s\n",
      "871:\tlearn: 0.0031433\ttotal: 2m 14s\tremaining: 19.8s\n",
      "872:\tlearn: 0.0031433\ttotal: 2m 14s\tremaining: 19.6s\n",
      "873:\tlearn: 0.0031433\ttotal: 2m 14s\tremaining: 19.4s\n",
      "874:\tlearn: 0.0031433\ttotal: 2m 15s\tremaining: 19.3s\n",
      "875:\tlearn: 0.0031433\ttotal: 2m 15s\tremaining: 19.1s\n",
      "876:\tlearn: 0.0031433\ttotal: 2m 15s\tremaining: 19s\n",
      "877:\tlearn: 0.0031433\ttotal: 2m 15s\tremaining: 18.8s\n",
      "878:\tlearn: 0.0031433\ttotal: 2m 15s\tremaining: 18.7s\n",
      "879:\tlearn: 0.0031433\ttotal: 2m 15s\tremaining: 18.5s\n",
      "880:\tlearn: 0.0031433\ttotal: 2m 15s\tremaining: 18.4s\n",
      "881:\tlearn: 0.0031433\ttotal: 2m 16s\tremaining: 18.2s\n",
      "882:\tlearn: 0.0031433\ttotal: 2m 16s\tremaining: 18s\n",
      "883:\tlearn: 0.0031433\ttotal: 2m 16s\tremaining: 17.9s\n",
      "884:\tlearn: 0.0031433\ttotal: 2m 16s\tremaining: 17.7s\n",
      "885:\tlearn: 0.0031433\ttotal: 2m 16s\tremaining: 17.6s\n",
      "886:\tlearn: 0.0031433\ttotal: 2m 16s\tremaining: 17.4s\n",
      "887:\tlearn: 0.0031433\ttotal: 2m 16s\tremaining: 17.3s\n",
      "888:\tlearn: 0.0031433\ttotal: 2m 17s\tremaining: 17.1s\n",
      "889:\tlearn: 0.0031433\ttotal: 2m 17s\tremaining: 16.9s\n",
      "890:\tlearn: 0.0031433\ttotal: 2m 17s\tremaining: 16.8s\n",
      "891:\tlearn: 0.0031433\ttotal: 2m 17s\tremaining: 16.6s\n",
      "892:\tlearn: 0.0031433\ttotal: 2m 17s\tremaining: 16.5s\n",
      "893:\tlearn: 0.0031433\ttotal: 2m 17s\tremaining: 16.3s\n",
      "894:\tlearn: 0.0031433\ttotal: 2m 17s\tremaining: 16.2s\n",
      "895:\tlearn: 0.0031433\ttotal: 2m 17s\tremaining: 16s\n",
      "896:\tlearn: 0.0031433\ttotal: 2m 18s\tremaining: 15.9s\n",
      "897:\tlearn: 0.0031433\ttotal: 2m 18s\tremaining: 15.7s\n",
      "898:\tlearn: 0.0031433\ttotal: 2m 18s\tremaining: 15.5s\n",
      "899:\tlearn: 0.0031433\ttotal: 2m 18s\tremaining: 15.4s\n",
      "900:\tlearn: 0.0031433\ttotal: 2m 18s\tremaining: 15.2s\n",
      "901:\tlearn: 0.0031433\ttotal: 2m 18s\tremaining: 15.1s\n",
      "902:\tlearn: 0.0031433\ttotal: 2m 18s\tremaining: 14.9s\n",
      "903:\tlearn: 0.0031433\ttotal: 2m 18s\tremaining: 14.8s\n",
      "904:\tlearn: 0.0031433\ttotal: 2m 19s\tremaining: 14.6s\n",
      "905:\tlearn: 0.0031433\ttotal: 2m 19s\tremaining: 14.4s\n",
      "906:\tlearn: 0.0031433\ttotal: 2m 19s\tremaining: 14.3s\n",
      "907:\tlearn: 0.0031433\ttotal: 2m 19s\tremaining: 14.1s\n",
      "908:\tlearn: 0.0031433\ttotal: 2m 19s\tremaining: 14s\n",
      "909:\tlearn: 0.0031433\ttotal: 2m 19s\tremaining: 13.8s\n",
      "910:\tlearn: 0.0031433\ttotal: 2m 19s\tremaining: 13.7s\n",
      "911:\tlearn: 0.0031433\ttotal: 2m 19s\tremaining: 13.5s\n",
      "912:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 13.3s\n",
      "913:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 13.2s\n",
      "914:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 13s\n",
      "915:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 12.9s\n",
      "916:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 12.7s\n",
      "917:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 12.6s\n",
      "918:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 12.4s\n",
      "919:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 12.2s\n",
      "920:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 12.1s\n",
      "921:\tlearn: 0.0031433\ttotal: 2m 20s\tremaining: 11.9s\n",
      "922:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 11.8s\n",
      "923:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 11.6s\n",
      "924:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 11.5s\n",
      "925:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 11.3s\n",
      "926:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 11.1s\n",
      "927:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 11s\n",
      "928:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 10.8s\n",
      "929:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 10.7s\n",
      "930:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 10.5s\n",
      "931:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 10.3s\n",
      "932:\tlearn: 0.0031433\ttotal: 2m 21s\tremaining: 10.2s\n",
      "933:\tlearn: 0.0031433\ttotal: 2m 22s\tremaining: 10s\n",
      "934:\tlearn: 0.0031433\ttotal: 2m 22s\tremaining: 9.89s\n",
      "935:\tlearn: 0.0031433\ttotal: 2m 22s\tremaining: 9.73s\n",
      "936:\tlearn: 0.0031433\ttotal: 2m 22s\tremaining: 9.58s\n",
      "937:\tlearn: 0.0031433\ttotal: 2m 22s\tremaining: 9.42s\n",
      "938:\tlearn: 0.0031433\ttotal: 2m 22s\tremaining: 9.27s\n",
      "939:\tlearn: 0.0031433\ttotal: 2m 22s\tremaining: 9.11s\n",
      "940:\tlearn: 0.0031433\ttotal: 2m 22s\tremaining: 8.96s\n",
      "941:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 8.81s\n",
      "942:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 8.66s\n",
      "943:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 8.5s\n",
      "944:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 8.35s\n",
      "945:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 8.19s\n",
      "946:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 8.04s\n",
      "947:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 7.88s\n",
      "948:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 7.73s\n",
      "949:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 7.57s\n",
      "950:\tlearn: 0.0031433\ttotal: 2m 23s\tremaining: 7.42s\n",
      "951:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 7.26s\n",
      "952:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 7.11s\n",
      "953:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 6.95s\n",
      "954:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 6.8s\n",
      "955:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 6.65s\n",
      "956:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 6.49s\n",
      "957:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 6.34s\n",
      "958:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 6.19s\n",
      "959:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 6.03s\n",
      "960:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 5.88s\n",
      "961:\tlearn: 0.0031433\ttotal: 2m 24s\tremaining: 5.73s\n",
      "962:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 5.57s\n",
      "963:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 5.42s\n",
      "964:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 5.27s\n",
      "965:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 5.12s\n",
      "966:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 4.96s\n",
      "967:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 4.81s\n",
      "968:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 4.66s\n",
      "969:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 4.51s\n",
      "970:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 4.35s\n",
      "971:\tlearn: 0.0031433\ttotal: 2m 25s\tremaining: 4.2s\n",
      "972:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 4.05s\n",
      "973:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 3.9s\n",
      "974:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 3.75s\n",
      "975:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 3.6s\n",
      "976:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 3.44s\n",
      "977:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 3.29s\n",
      "978:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 3.14s\n",
      "979:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 2.99s\n",
      "980:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 2.84s\n",
      "981:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 2.69s\n",
      "982:\tlearn: 0.0031433\ttotal: 2m 26s\tremaining: 2.54s\n",
      "983:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 2.39s\n",
      "984:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 2.24s\n",
      "985:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 2.09s\n",
      "986:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 1.94s\n",
      "987:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 1.79s\n",
      "988:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 1.64s\n",
      "989:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 1.49s\n",
      "990:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 1.34s\n",
      "991:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 1.19s\n",
      "992:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 1.04s\n",
      "993:\tlearn: 0.0031433\ttotal: 2m 27s\tremaining: 893ms\n",
      "994:\tlearn: 0.0031433\ttotal: 2m 28s\tremaining: 744ms\n",
      "995:\tlearn: 0.0031433\ttotal: 2m 28s\tremaining: 595ms\n",
      "996:\tlearn: 0.0031433\ttotal: 2m 28s\tremaining: 446ms\n",
      "997:\tlearn: 0.0031433\ttotal: 2m 28s\tremaining: 298ms\n",
      "998:\tlearn: 0.0031433\ttotal: 2m 28s\tremaining: 149ms\n",
      "999:\tlearn: 0.0031433\ttotal: 2m 28s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "def apply_catboost(X_train, y_train, X_test):\n",
    "    return apply_fit_predict_model(lambda: CatBoostClassifier(), X_train, y_train, X_test)\n",
    "\n",
    "catboost_y_pred = apply_catboost(X_train, y_train, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-based model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== accuracy ======\n",
      "score: 0.999903276295658 (rf)\t0.9998593109755025 (xgb)\t0.9995515537344143 (catb)\t0.9994987953502278 (lgbm)\t0.9986106958830874 (dt)\t0.9870741958742943 (gb)\t\n",
      "improvement(from next/worst): 0.004397%, 1.299708% (rf)\t0.03079%, 1.295254% (xgb)\t0.005278%, 1.264075% (catb)\t0.088934%, 1.25873% (lgbm)\t1.168757%, 1.168757% (dt)\t\n",
      "====== precision_0 ======\n",
      "score: 1.0 (rf)\t1.0 (xgb)\t0.9999647284976103 (catb)\t0.9997179374173645 (lgbm)\t0.9994175579794556 (dt)\t0.9812145269682092 (gb)\t\n",
      "improvement(from next/worst): 0.0%, 1.914512% (rf)\t0.003527%, 1.914512% (xgb)\t0.024686%, 1.910918% (catb)\t0.030055%, 1.885766% (lgbm)\t1.855153%, 1.855153% (dt)\t\n",
      "====== precision_1 ======\n",
      "score: 0.9998069735202766 (rf)\t0.9997192588433464 (xgb)\t0.999280714373432 (lgbm)\t0.9991406976132438 (catb)\t0.9978096306161071 (dt)\t0.9930535815804434 (gb)\t\n",
      "improvement(from next/worst): 0.008774%, 0.680063% (rf)\t0.043886%, 0.67123% (xgb)\t0.014014%, 0.627069% (lgbm)\t0.133399%, 0.61297% (catb)\t0.478932%, 0.478932% (dt)\t\n",
      "====== recall_0 ======\n",
      "score: 0.999806167400881 (rf)\t0.9997180616740088 (xgb)\t0.9992775330396476 (lgbm)\t0.999136563876652 (catb)\t0.9977973568281938 (dt)\t0.9931101321585903 (gb)\t\n",
      "improvement(from next/worst): 0.008813%, 0.674249% (rf)\t0.044085%, 0.665377% (xgb)\t0.014109%, 0.621019% (lgbm)\t0.134216%, 0.606824% (catb)\t0.471974%, 0.471974% (dt)\t\n",
      "====== recall_1 ======\n",
      "score: 1.0 (rf)\t1.0 (xgb)\t0.999964897500702 (catb)\t0.9997191800056164 (lgbm)\t0.9994208087615838 (dt)\t0.981062201628756 (gb)\t\n",
      "improvement(from next/worst): 0.0%, 1.930336% (rf)\t0.00351%, 1.930336% (xgb)\t0.024579%, 1.926758% (catb)\t0.029854%, 1.901712% (lgbm)\t1.871299%, 1.871299% (dt)\t\n",
      "====== f1_0 ======\n",
      "score: 0.999903074306761 (rf)\t0.9998590109618978 (xgb)\t0.9995504746458886 (catb)\t0.9994976867151355 (lgbm)\t0.9986068002257337 (dt)\t0.9871264931516447 (gb)\t\n",
      "improvement(from next/worst): 0.004407%, 1.294321% (rf)\t0.030868%, 1.289857% (xgb)\t0.005281%, 1.258601% (catb)\t0.089213%, 1.253253% (lgbm)\t1.163003%, 1.163003% (dt)\t\n",
      "====== f1_1 ======\n",
      "score: 0.9999034774444338 (rf)\t0.9998596097150076 (xgb)\t0.9995526276546286 (catb)\t0.9994998991024504 (lgbm)\t0.9986145698151591 (dt)\t0.9870214719593162 (gb)\t\n",
      "improvement(from next/worst): 0.004387%, 1.305139% (rf)\t0.030712%, 1.300695% (xgb)\t0.005275%, 1.269593% (catb)\t0.088656%, 1.264251% (lgbm)\t1.174554%, 1.174554% (dt)\t\n",
      "====== roc_auc ======\n",
      "score: 0.9999030837004405 (rf)\t0.9998590308370043 (xgb)\t0.999550730688677 (catb)\t0.9994983565226321 (lgbm)\t0.9986090827948888 (dt)\t0.9870861668936732 (gb)\t\n",
      "improvement(from next/worst): 0.004406%, 1.29846% (rf)\t0.030844%, 1.293997% (xgb)\t0.00524%, 1.262763% (catb)\t0.089051%, 1.257458% (lgbm)\t1.167367%, 1.167367% (dt)\t\n",
      "====== cohen_kappa ======\n",
      "score: 0.9998065517530046 (rf)\t0.999718620682475 (xgb)\t0.999103102453704 (catb)\t0.9989975858660266 (lgbm)\t0.9972213718592949 (dt)\t0.9741489112247109 (gb)\t\n",
      "improvement(from next/worst): 0.008796%, 2.633852% (rf)\t0.061607%, 2.624826% (xgb)\t0.010562%, 2.56164% (catb)\t0.178116%, 2.550809% (lgbm)\t2.368474%, 2.368474% (dt)\t\n",
      "====== matthews_corr ======\n",
      "score: 0.9998065704604976 (rf)\t0.9997186602584983 (xgb)\t0.9991034437421373 (catb)\t0.99899768241756 (lgbm)\t0.9972226770824649 (dt)\t0.9742202199910576 (gb)\t\n",
      "improvement(from next/worst): 0.008793%, 2.626342% (rf)\t0.061577%, 2.617318% (xgb)\t0.010587%, 2.554168% (catb)\t0.177995%, 2.543312% (lgbm)\t2.361115%, 2.361115% (dt)\t\n",
      "====== balanced_accuracy ======\n",
      "score: 0.9999030837004406 (rf)\t0.9998590308370043 (xgb)\t0.9995507306886771 (catb)\t0.9994983565226321 (lgbm)\t0.9986090827948888 (dt)\t0.9870861668936732 (gb)\t\n",
      "improvement(from next/worst): 0.004406%, 1.29846% (rf)\t0.030844%, 1.293997% (xgb)\t0.00524%, 1.262763% (catb)\t0.089051%, 1.257458% (lgbm)\t1.167367%, 1.167367% (dt)\t\n",
      "====== g_mean ======\n",
      "score: 0.9999030790036008 (rf)\t0.999859020899451 (xgb)\t0.9995506448830495 (catb)\t0.9994983321288894 (lgbm)\t0.9986087528864366 (dt)\t0.987067785268738 (gb)\t\n",
      "improvement(from next/worst): 0.004406%, 1.300346% (rf)\t0.030851%, 1.295882% (xgb)\t0.005234%, 1.264641% (catb)\t0.089082%, 1.259341% (lgbm)\t1.169217%, 1.169217% (dt)\t\n",
      "====== classification_error ======\n",
      "score: 0.012925804125705653 (gb)\t0.0013893041169126263 (dt)\t0.0005012046497722134 (lgbm)\t0.0004484462655857113 (catb)\t0.00014068902449748677 (xgb)\t9.67237043419944e-05 (rf)\t\n",
      "improvement(from next/worst): 830.379747%, 13263.636364% (gb)\t177.192982%, 1336.363636% (dt)\t11.764706%, 418.181818% (lgbm)\t218.75%, 363.636364% (catb)\t45.454545%, 45.454545% (xgb)\t\n",
      "====== sensitivity_0 ======\n",
      "score: 0.999806167400881 (rf)\t0.9997180616740088 (xgb)\t0.9992775330396476 (lgbm)\t0.999136563876652 (catb)\t0.9977973568281938 (dt)\t0.9931101321585903 (gb)\t\n",
      "improvement(from next/worst): 0.008813%, 0.674249% (rf)\t0.044085%, 0.665377% (xgb)\t0.014109%, 0.621019% (lgbm)\t0.134216%, 0.606824% (catb)\t0.471974%, 0.471974% (dt)\t\n",
      "====== sensitivity_1 ======\n",
      "score: 1.0 (rf)\t1.0 (xgb)\t0.999964897500702 (catb)\t0.9997191800056164 (lgbm)\t0.9994208087615838 (dt)\t0.981062201628756 (gb)\t\n",
      "improvement(from next/worst): 0.0%, 1.930336% (rf)\t0.00351%, 1.930336% (xgb)\t0.024579%, 1.926758% (catb)\t0.029854%, 1.901712% (lgbm)\t1.871299%, 1.871299% (dt)\t\n",
      "====== specificity_0 ======\n",
      "score: 0.006889867841409658 (gb)\t0.0022026431718061845 (dt)\t0.0008634361233480492 (catb)\t0.0007224669603523859 (lgbm)\t0.0002819383259912156 (xgb)\t0.00019383259911898154 (rf)\t\n",
      "improvement(from next/worst): 212.8%, 3454.545455% (gb)\t155.102041%, 1036.363636% (dt)\t19.512195%, 345.454545% (catb)\t156.25%, 272.727273% (lgbm)\t45.454545%, 45.454545% (xgb)\t\n",
      "====== specificity_1 ======\n",
      "score: 0.018937798371243986 (gb)\t0.0005791912384162101 (dt)\t0.0002808199943835632 (lgbm)\t3.51024992979454e-05 (catb)\t0.0 (xgb)\t0.0 (rf)\t\n",
      "improvement(from next/worst): 3169.69697%, inf% (gb)\t106.25%, inf% (dt)\t700.0%, inf% (lgbm)\tinf%, inf% (catb)\tnan%, nan% (xgb)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/4b_rm7gs2fj181km0x332skh0000gn/T/ipykernel_7382/1027943219.py:14: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  text = f\"{round(100*((ans[i][score_name]/ans[i+1][score_name])-1),6)}%, {round(100*((ans[i][score_name]/ans[n-1][score_name])-1),6)}% ({ans[i][name_str]})\"\n",
      "/var/folders/xs/4b_rm7gs2fj181km0x332skh0000gn/T/ipykernel_7382/1027943219.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  text = f\"{round(100*((ans[i][score_name]/ans[i+1][score_name])-1),6)}%, {round(100*((ans[i][score_name]/ans[n-1][score_name])-1),6)}% ({ans[i][name_str]})\"\n"
     ]
    }
   ],
   "source": [
    "ans = {'dt':dt_y_pred, 'rf':random_forest_y_pred, 'gb':gb_y_pred, 'xgb':xgb_y_pred, 'lgbm': lgbm_y_pred,'catb': catboost_y_pred }\n",
    "\n",
    "lst = []\n",
    "for model_name in ans:\n",
    "    lst += [getMetrics(ans[model_name],y_test,method_name=model_name)]\n",
    "\n",
    "compareModels(lst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest had the best performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks:\n",
    "- multi-layer perceptron, convolutional neural networks, recurrent neural networks, autoencoders, generative adversarial networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def apply_mlp(X_train, y_train, X_test):\n",
    "    return apply_fit_predict_model(lambda: MLPClassifier(), X_train, y_train, X_test)\n",
    "mlp_y_pred = apply_mlp(X_train, y_train, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def apply_cnn(X_train, y_train, X_test):\n",
    "    # Reshape data for CNN\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "    # Define CNN model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile and fit the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Predict using the model\n",
    "    return model.predict_classes(X_test)\n",
    "\n",
    "cnn_y_pred = apply_cnn(X_train, y_train, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative adversarial networks (GANs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supposert Vector Machine:\n",
    "- Linear SVM, Non-Linear SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusteering models:\n",
    "- K-Means, Hierarchical, DBSCAN, Gaussian Mixture models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Models:\n",
    "- Naive Bayes, Bayesian Networks, Gaussian Processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models:\n",
    "- Bagging, Boosting, Stacking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary models:\n",
    "- genetic algorithms, particle swarm optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
